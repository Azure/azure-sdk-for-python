# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for license information.
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------

import datetime
from typing import Any, Dict, List, Optional, Union

import msrest.serialization

from ._spark_client_enums import *


class SparkBatchJob(msrest.serialization.Model):
    """SparkBatchJob.

    All required parameters must be populated in order to send to Azure.

    :ivar livy_info:
    :vartype livy_info: ~azure.synapse.spark.models.SparkBatchJobState
    :ivar name: The batch name.
    :vartype name: str
    :ivar workspace_name: The workspace name.
    :vartype workspace_name: str
    :ivar spark_pool_name: The Spark pool name.
    :vartype spark_pool_name: str
    :ivar submitter_name: The submitter name.
    :vartype submitter_name: str
    :ivar submitter_id: The submitter identifier.
    :vartype submitter_id: str
    :ivar artifact_id: The artifact identifier.
    :vartype artifact_id: str
    :ivar job_type: The job type. Possible values include: "SparkBatch", "SparkSession".
    :vartype job_type: str or ~azure.synapse.spark.models.SparkJobType
    :ivar result: The Spark batch job result. Possible values include: "Uncertain", "Succeeded",
     "Failed", "Cancelled".
    :vartype result: str or ~azure.synapse.spark.models.SparkBatchJobResultType
    :ivar scheduler: The scheduler information.
    :vartype scheduler: ~azure.synapse.spark.models.SparkScheduler
    :ivar plugin: The plugin information.
    :vartype plugin: ~azure.synapse.spark.models.SparkServicePlugin
    :ivar errors: The error information.
    :vartype errors: list[~azure.synapse.spark.models.SparkServiceError]
    :ivar tags: A set of tags. The tags.
    :vartype tags: dict[str, str]
    :ivar id: Required. The session Id.
    :vartype id: int
    :ivar app_id: The application id of this session.
    :vartype app_id: str
    :ivar app_info: The detailed application info.
    :vartype app_info: dict[str, str]
    :ivar state: The batch state. Possible values include: "not_started", "starting", "idle",
     "busy", "shutting_down", "error", "dead", "killed", "success", "running", "recovering".
    :vartype state: str or ~azure.synapse.spark.models.LivyStates
    :ivar log_lines: The log lines.
    :vartype log_lines: list[str]
    """

    _validation = {
        'id': {'required': True},
    }

    _attribute_map = {
        'livy_info': {'key': 'livyInfo', 'type': 'SparkBatchJobState'},
        'name': {'key': 'name', 'type': 'str'},
        'workspace_name': {'key': 'workspaceName', 'type': 'str'},
        'spark_pool_name': {'key': 'sparkPoolName', 'type': 'str'},
        'submitter_name': {'key': 'submitterName', 'type': 'str'},
        'submitter_id': {'key': 'submitterId', 'type': 'str'},
        'artifact_id': {'key': 'artifactId', 'type': 'str'},
        'job_type': {'key': 'jobType', 'type': 'str'},
        'result': {'key': 'result', 'type': 'str'},
        'scheduler': {'key': 'schedulerInfo', 'type': 'SparkScheduler'},
        'plugin': {'key': 'pluginInfo', 'type': 'SparkServicePlugin'},
        'errors': {'key': 'errorInfo', 'type': '[SparkServiceError]'},
        'tags': {'key': 'tags', 'type': '{str}'},
        'id': {'key': 'id', 'type': 'int'},
        'app_id': {'key': 'appId', 'type': 'str'},
        'app_info': {'key': 'appInfo', 'type': '{str}'},
        'state': {'key': 'state', 'type': 'str'},
        'log_lines': {'key': 'log', 'type': '[str]'},
    }

    def __init__(
        self,
        *,
        id: int,
        livy_info: Optional["SparkBatchJobState"] = None,
        name: Optional[str] = None,
        workspace_name: Optional[str] = None,
        spark_pool_name: Optional[str] = None,
        submitter_name: Optional[str] = None,
        submitter_id: Optional[str] = None,
        artifact_id: Optional[str] = None,
        job_type: Optional[Union[str, "SparkJobType"]] = None,
        result: Optional[Union[str, "SparkBatchJobResultType"]] = None,
        scheduler: Optional["SparkScheduler"] = None,
        plugin: Optional["SparkServicePlugin"] = None,
        errors: Optional[List["SparkServiceError"]] = None,
        tags: Optional[Dict[str, str]] = None,
        app_id: Optional[str] = None,
        app_info: Optional[Dict[str, str]] = None,
        state: Optional[Union[str, "LivyStates"]] = None,
        log_lines: Optional[List[str]] = None,
        **kwargs
    ):
        """
        :keyword livy_info:
        :paramtype livy_info: ~azure.synapse.spark.models.SparkBatchJobState
        :keyword name: The batch name.
        :paramtype name: str
        :keyword workspace_name: The workspace name.
        :paramtype workspace_name: str
        :keyword spark_pool_name: The Spark pool name.
        :paramtype spark_pool_name: str
        :keyword submitter_name: The submitter name.
        :paramtype submitter_name: str
        :keyword submitter_id: The submitter identifier.
        :paramtype submitter_id: str
        :keyword artifact_id: The artifact identifier.
        :paramtype artifact_id: str
        :keyword job_type: The job type. Possible values include: "SparkBatch", "SparkSession".
        :paramtype job_type: str or ~azure.synapse.spark.models.SparkJobType
        :keyword result: The Spark batch job result. Possible values include: "Uncertain", "Succeeded",
         "Failed", "Cancelled".
        :paramtype result: str or ~azure.synapse.spark.models.SparkBatchJobResultType
        :keyword scheduler: The scheduler information.
        :paramtype scheduler: ~azure.synapse.spark.models.SparkScheduler
        :keyword plugin: The plugin information.
        :paramtype plugin: ~azure.synapse.spark.models.SparkServicePlugin
        :keyword errors: The error information.
        :paramtype errors: list[~azure.synapse.spark.models.SparkServiceError]
        :keyword tags: A set of tags. The tags.
        :paramtype tags: dict[str, str]
        :keyword id: Required. The session Id.
        :paramtype id: int
        :keyword app_id: The application id of this session.
        :paramtype app_id: str
        :keyword app_info: The detailed application info.
        :paramtype app_info: dict[str, str]
        :keyword state: The batch state. Possible values include: "not_started", "starting", "idle",
         "busy", "shutting_down", "error", "dead", "killed", "success", "running", "recovering".
        :paramtype state: str or ~azure.synapse.spark.models.LivyStates
        :keyword log_lines: The log lines.
        :paramtype log_lines: list[str]
        """
        super(SparkBatchJob, self).__init__(**kwargs)
        self.livy_info = livy_info
        self.name = name
        self.workspace_name = workspace_name
        self.spark_pool_name = spark_pool_name
        self.submitter_name = submitter_name
        self.submitter_id = submitter_id
        self.artifact_id = artifact_id
        self.job_type = job_type
        self.result = result
        self.scheduler = scheduler
        self.plugin = plugin
        self.errors = errors
        self.tags = tags
        self.id = id
        self.app_id = app_id
        self.app_info = app_info
        self.state = state
        self.log_lines = log_lines


class SparkBatchJobCollection(msrest.serialization.Model):
    """Response for batch list operation.

    All required parameters must be populated in order to send to Azure.

    :ivar from_property: Required. The start index of fetched sessions.
    :vartype from_property: int
    :ivar total: Required. Number of sessions fetched.
    :vartype total: int
    :ivar sessions: Batch list.
    :vartype sessions: list[~azure.synapse.spark.models.SparkBatchJob]
    """

    _validation = {
        'from_property': {'required': True},
        'total': {'required': True},
    }

    _attribute_map = {
        'from_property': {'key': 'from', 'type': 'int'},
        'total': {'key': 'total', 'type': 'int'},
        'sessions': {'key': 'sessions', 'type': '[SparkBatchJob]'},
    }

    def __init__(
        self,
        *,
        from_property: int,
        total: int,
        sessions: Optional[List["SparkBatchJob"]] = None,
        **kwargs
    ):
        """
        :keyword from_property: Required. The start index of fetched sessions.
        :paramtype from_property: int
        :keyword total: Required. Number of sessions fetched.
        :paramtype total: int
        :keyword sessions: Batch list.
        :paramtype sessions: list[~azure.synapse.spark.models.SparkBatchJob]
        """
        super(SparkBatchJobCollection, self).__init__(**kwargs)
        self.from_property = from_property
        self.total = total
        self.sessions = sessions


class SparkBatchJobOptions(msrest.serialization.Model):
    """SparkBatchJobOptions.

    All required parameters must be populated in order to send to Azure.

    :ivar tags: A set of tags. Dictionary of :code:`<string>`.
    :vartype tags: dict[str, str]
    :ivar artifact_id:
    :vartype artifact_id: str
    :ivar name: Required.
    :vartype name: str
    :ivar file: Required.
    :vartype file: str
    :ivar class_name:
    :vartype class_name: str
    :ivar arguments:
    :vartype arguments: list[str]
    :ivar jars:
    :vartype jars: list[str]
    :ivar python_files:
    :vartype python_files: list[str]
    :ivar files:
    :vartype files: list[str]
    :ivar archives:
    :vartype archives: list[str]
    :ivar configuration: Dictionary of :code:`<string>`.
    :vartype configuration: dict[str, str]
    :ivar driver_memory:
    :vartype driver_memory: str
    :ivar driver_cores:
    :vartype driver_cores: int
    :ivar executor_memory:
    :vartype executor_memory: str
    :ivar executor_cores:
    :vartype executor_cores: int
    :ivar executor_count:
    :vartype executor_count: int
    """

    _validation = {
        'name': {'required': True},
        'file': {'required': True},
    }

    _attribute_map = {
        'tags': {'key': 'tags', 'type': '{str}'},
        'artifact_id': {'key': 'artifactId', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'file': {'key': 'file', 'type': 'str'},
        'class_name': {'key': 'className', 'type': 'str'},
        'arguments': {'key': 'args', 'type': '[str]'},
        'jars': {'key': 'jars', 'type': '[str]'},
        'python_files': {'key': 'pyFiles', 'type': '[str]'},
        'files': {'key': 'files', 'type': '[str]'},
        'archives': {'key': 'archives', 'type': '[str]'},
        'configuration': {'key': 'conf', 'type': '{str}'},
        'driver_memory': {'key': 'driverMemory', 'type': 'str'},
        'driver_cores': {'key': 'driverCores', 'type': 'int'},
        'executor_memory': {'key': 'executorMemory', 'type': 'str'},
        'executor_cores': {'key': 'executorCores', 'type': 'int'},
        'executor_count': {'key': 'numExecutors', 'type': 'int'},
    }

    def __init__(
        self,
        *,
        name: str,
        file: str,
        tags: Optional[Dict[str, str]] = None,
        artifact_id: Optional[str] = None,
        class_name: Optional[str] = None,
        arguments: Optional[List[str]] = None,
        jars: Optional[List[str]] = None,
        python_files: Optional[List[str]] = None,
        files: Optional[List[str]] = None,
        archives: Optional[List[str]] = None,
        configuration: Optional[Dict[str, str]] = None,
        driver_memory: Optional[str] = None,
        driver_cores: Optional[int] = None,
        executor_memory: Optional[str] = None,
        executor_cores: Optional[int] = None,
        executor_count: Optional[int] = None,
        **kwargs
    ):
        """
        :keyword tags: A set of tags. Dictionary of :code:`<string>`.
        :paramtype tags: dict[str, str]
        :keyword artifact_id:
        :paramtype artifact_id: str
        :keyword name: Required.
        :paramtype name: str
        :keyword file: Required.
        :paramtype file: str
        :keyword class_name:
        :paramtype class_name: str
        :keyword arguments:
        :paramtype arguments: list[str]
        :keyword jars:
        :paramtype jars: list[str]
        :keyword python_files:
        :paramtype python_files: list[str]
        :keyword files:
        :paramtype files: list[str]
        :keyword archives:
        :paramtype archives: list[str]
        :keyword configuration: Dictionary of :code:`<string>`.
        :paramtype configuration: dict[str, str]
        :keyword driver_memory:
        :paramtype driver_memory: str
        :keyword driver_cores:
        :paramtype driver_cores: int
        :keyword executor_memory:
        :paramtype executor_memory: str
        :keyword executor_cores:
        :paramtype executor_cores: int
        :keyword executor_count:
        :paramtype executor_count: int
        """
        super(SparkBatchJobOptions, self).__init__(**kwargs)
        self.tags = tags
        self.artifact_id = artifact_id
        self.name = name
        self.file = file
        self.class_name = class_name
        self.arguments = arguments
        self.jars = jars
        self.python_files = python_files
        self.files = files
        self.archives = archives
        self.configuration = configuration
        self.driver_memory = driver_memory
        self.driver_cores = driver_cores
        self.executor_memory = executor_memory
        self.executor_cores = executor_cores
        self.executor_count = executor_count


class SparkBatchJobState(msrest.serialization.Model):
    """SparkBatchJobState.

    :ivar not_started_at: the time that at which "not_started" livy state was first seen.
    :vartype not_started_at: ~datetime.datetime
    :ivar starting_at: the time that at which "starting" livy state was first seen.
    :vartype starting_at: ~datetime.datetime
    :ivar running_at: the time that at which "running" livy state was first seen.
    :vartype running_at: ~datetime.datetime
    :ivar dead_at: time that at which "dead" livy state was first seen.
    :vartype dead_at: ~datetime.datetime
    :ivar success_at: the time that at which "success" livy state was first seen.
    :vartype success_at: ~datetime.datetime
    :ivar terminated_at: the time that at which "killed" livy state was first seen.
    :vartype terminated_at: ~datetime.datetime
    :ivar recovering_at: the time that at which "recovering" livy state was first seen.
    :vartype recovering_at: ~datetime.datetime
    :ivar current_state: the Spark job state.
    :vartype current_state: str
    :ivar job_creation_request:
    :vartype job_creation_request: ~azure.synapse.spark.models.SparkRequest
    """

    _attribute_map = {
        'not_started_at': {'key': 'notStartedAt', 'type': 'iso-8601'},
        'starting_at': {'key': 'startingAt', 'type': 'iso-8601'},
        'running_at': {'key': 'runningAt', 'type': 'iso-8601'},
        'dead_at': {'key': 'deadAt', 'type': 'iso-8601'},
        'success_at': {'key': 'successAt', 'type': 'iso-8601'},
        'terminated_at': {'key': 'killedAt', 'type': 'iso-8601'},
        'recovering_at': {'key': 'recoveringAt', 'type': 'iso-8601'},
        'current_state': {'key': 'currentState', 'type': 'str'},
        'job_creation_request': {'key': 'jobCreationRequest', 'type': 'SparkRequest'},
    }

    def __init__(
        self,
        *,
        not_started_at: Optional[datetime.datetime] = None,
        starting_at: Optional[datetime.datetime] = None,
        running_at: Optional[datetime.datetime] = None,
        dead_at: Optional[datetime.datetime] = None,
        success_at: Optional[datetime.datetime] = None,
        terminated_at: Optional[datetime.datetime] = None,
        recovering_at: Optional[datetime.datetime] = None,
        current_state: Optional[str] = None,
        job_creation_request: Optional["SparkRequest"] = None,
        **kwargs
    ):
        """
        :keyword not_started_at: the time that at which "not_started" livy state was first seen.
        :paramtype not_started_at: ~datetime.datetime
        :keyword starting_at: the time that at which "starting" livy state was first seen.
        :paramtype starting_at: ~datetime.datetime
        :keyword running_at: the time that at which "running" livy state was first seen.
        :paramtype running_at: ~datetime.datetime
        :keyword dead_at: time that at which "dead" livy state was first seen.
        :paramtype dead_at: ~datetime.datetime
        :keyword success_at: the time that at which "success" livy state was first seen.
        :paramtype success_at: ~datetime.datetime
        :keyword terminated_at: the time that at which "killed" livy state was first seen.
        :paramtype terminated_at: ~datetime.datetime
        :keyword recovering_at: the time that at which "recovering" livy state was first seen.
        :paramtype recovering_at: ~datetime.datetime
        :keyword current_state: the Spark job state.
        :paramtype current_state: str
        :keyword job_creation_request:
        :paramtype job_creation_request: ~azure.synapse.spark.models.SparkRequest
        """
        super(SparkBatchJobState, self).__init__(**kwargs)
        self.not_started_at = not_started_at
        self.starting_at = starting_at
        self.running_at = running_at
        self.dead_at = dead_at
        self.success_at = success_at
        self.terminated_at = terminated_at
        self.recovering_at = recovering_at
        self.current_state = current_state
        self.job_creation_request = job_creation_request


class SparkRequest(msrest.serialization.Model):
    """SparkRequest.

    :ivar name:
    :vartype name: str
    :ivar file:
    :vartype file: str
    :ivar class_name:
    :vartype class_name: str
    :ivar arguments:
    :vartype arguments: list[str]
    :ivar jars:
    :vartype jars: list[str]
    :ivar python_files:
    :vartype python_files: list[str]
    :ivar files:
    :vartype files: list[str]
    :ivar archives:
    :vartype archives: list[str]
    :ivar configuration: Dictionary of :code:`<string>`.
    :vartype configuration: dict[str, str]
    :ivar driver_memory:
    :vartype driver_memory: str
    :ivar driver_cores:
    :vartype driver_cores: int
    :ivar executor_memory:
    :vartype executor_memory: str
    :ivar executor_cores:
    :vartype executor_cores: int
    :ivar executor_count:
    :vartype executor_count: int
    """

    _attribute_map = {
        'name': {'key': 'name', 'type': 'str'},
        'file': {'key': 'file', 'type': 'str'},
        'class_name': {'key': 'className', 'type': 'str'},
        'arguments': {'key': 'args', 'type': '[str]'},
        'jars': {'key': 'jars', 'type': '[str]'},
        'python_files': {'key': 'pyFiles', 'type': '[str]'},
        'files': {'key': 'files', 'type': '[str]'},
        'archives': {'key': 'archives', 'type': '[str]'},
        'configuration': {'key': 'conf', 'type': '{str}'},
        'driver_memory': {'key': 'driverMemory', 'type': 'str'},
        'driver_cores': {'key': 'driverCores', 'type': 'int'},
        'executor_memory': {'key': 'executorMemory', 'type': 'str'},
        'executor_cores': {'key': 'executorCores', 'type': 'int'},
        'executor_count': {'key': 'numExecutors', 'type': 'int'},
    }

    def __init__(
        self,
        *,
        name: Optional[str] = None,
        file: Optional[str] = None,
        class_name: Optional[str] = None,
        arguments: Optional[List[str]] = None,
        jars: Optional[List[str]] = None,
        python_files: Optional[List[str]] = None,
        files: Optional[List[str]] = None,
        archives: Optional[List[str]] = None,
        configuration: Optional[Dict[str, str]] = None,
        driver_memory: Optional[str] = None,
        driver_cores: Optional[int] = None,
        executor_memory: Optional[str] = None,
        executor_cores: Optional[int] = None,
        executor_count: Optional[int] = None,
        **kwargs
    ):
        """
        :keyword name:
        :paramtype name: str
        :keyword file:
        :paramtype file: str
        :keyword class_name:
        :paramtype class_name: str
        :keyword arguments:
        :paramtype arguments: list[str]
        :keyword jars:
        :paramtype jars: list[str]
        :keyword python_files:
        :paramtype python_files: list[str]
        :keyword files:
        :paramtype files: list[str]
        :keyword archives:
        :paramtype archives: list[str]
        :keyword configuration: Dictionary of :code:`<string>`.
        :paramtype configuration: dict[str, str]
        :keyword driver_memory:
        :paramtype driver_memory: str
        :keyword driver_cores:
        :paramtype driver_cores: int
        :keyword executor_memory:
        :paramtype executor_memory: str
        :keyword executor_cores:
        :paramtype executor_cores: int
        :keyword executor_count:
        :paramtype executor_count: int
        """
        super(SparkRequest, self).__init__(**kwargs)
        self.name = name
        self.file = file
        self.class_name = class_name
        self.arguments = arguments
        self.jars = jars
        self.python_files = python_files
        self.files = files
        self.archives = archives
        self.configuration = configuration
        self.driver_memory = driver_memory
        self.driver_cores = driver_cores
        self.executor_memory = executor_memory
        self.executor_cores = executor_cores
        self.executor_count = executor_count


class SparkScheduler(msrest.serialization.Model):
    """SparkScheduler.

    :ivar submitted_at:
    :vartype submitted_at: ~datetime.datetime
    :ivar scheduled_at:
    :vartype scheduled_at: ~datetime.datetime
    :ivar ended_at:
    :vartype ended_at: ~datetime.datetime
    :ivar cancellation_requested_at:
    :vartype cancellation_requested_at: ~datetime.datetime
    :ivar current_state: Possible values include: "Queued", "Scheduled", "Ended".
    :vartype current_state: str or ~azure.synapse.spark.models.SchedulerCurrentState
    """

    _attribute_map = {
        'submitted_at': {'key': 'submittedAt', 'type': 'iso-8601'},
        'scheduled_at': {'key': 'scheduledAt', 'type': 'iso-8601'},
        'ended_at': {'key': 'endedAt', 'type': 'iso-8601'},
        'cancellation_requested_at': {'key': 'cancellationRequestedAt', 'type': 'iso-8601'},
        'current_state': {'key': 'currentState', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        submitted_at: Optional[datetime.datetime] = None,
        scheduled_at: Optional[datetime.datetime] = None,
        ended_at: Optional[datetime.datetime] = None,
        cancellation_requested_at: Optional[datetime.datetime] = None,
        current_state: Optional[Union[str, "SchedulerCurrentState"]] = None,
        **kwargs
    ):
        """
        :keyword submitted_at:
        :paramtype submitted_at: ~datetime.datetime
        :keyword scheduled_at:
        :paramtype scheduled_at: ~datetime.datetime
        :keyword ended_at:
        :paramtype ended_at: ~datetime.datetime
        :keyword cancellation_requested_at:
        :paramtype cancellation_requested_at: ~datetime.datetime
        :keyword current_state: Possible values include: "Queued", "Scheduled", "Ended".
        :paramtype current_state: str or ~azure.synapse.spark.models.SchedulerCurrentState
        """
        super(SparkScheduler, self).__init__(**kwargs)
        self.submitted_at = submitted_at
        self.scheduled_at = scheduled_at
        self.ended_at = ended_at
        self.cancellation_requested_at = cancellation_requested_at
        self.current_state = current_state


class SparkServiceError(msrest.serialization.Model):
    """SparkServiceError.

    :ivar message:
    :vartype message: str
    :ivar error_code:
    :vartype error_code: str
    :ivar source: Possible values include: "System", "User", "Unknown", "Dependency".
    :vartype source: str or ~azure.synapse.spark.models.SparkErrorSource
    """

    _attribute_map = {
        'message': {'key': 'message', 'type': 'str'},
        'error_code': {'key': 'errorCode', 'type': 'str'},
        'source': {'key': 'source', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        message: Optional[str] = None,
        error_code: Optional[str] = None,
        source: Optional[Union[str, "SparkErrorSource"]] = None,
        **kwargs
    ):
        """
        :keyword message:
        :paramtype message: str
        :keyword error_code:
        :paramtype error_code: str
        :keyword source: Possible values include: "System", "User", "Unknown", "Dependency".
        :paramtype source: str or ~azure.synapse.spark.models.SparkErrorSource
        """
        super(SparkServiceError, self).__init__(**kwargs)
        self.message = message
        self.error_code = error_code
        self.source = source


class SparkServicePlugin(msrest.serialization.Model):
    """SparkServicePlugin.

    :ivar preparation_started_at:
    :vartype preparation_started_at: ~datetime.datetime
    :ivar resource_acquisition_started_at:
    :vartype resource_acquisition_started_at: ~datetime.datetime
    :ivar submission_started_at:
    :vartype submission_started_at: ~datetime.datetime
    :ivar monitoring_started_at:
    :vartype monitoring_started_at: ~datetime.datetime
    :ivar cleanup_started_at:
    :vartype cleanup_started_at: ~datetime.datetime
    :ivar current_state: Possible values include: "Preparation", "ResourceAcquisition", "Queued",
     "Submission", "Monitoring", "Cleanup", "Ended".
    :vartype current_state: str or ~azure.synapse.spark.models.PluginCurrentState
    """

    _attribute_map = {
        'preparation_started_at': {'key': 'preparationStartedAt', 'type': 'iso-8601'},
        'resource_acquisition_started_at': {'key': 'resourceAcquisitionStartedAt', 'type': 'iso-8601'},
        'submission_started_at': {'key': 'submissionStartedAt', 'type': 'iso-8601'},
        'monitoring_started_at': {'key': 'monitoringStartedAt', 'type': 'iso-8601'},
        'cleanup_started_at': {'key': 'cleanupStartedAt', 'type': 'iso-8601'},
        'current_state': {'key': 'currentState', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        preparation_started_at: Optional[datetime.datetime] = None,
        resource_acquisition_started_at: Optional[datetime.datetime] = None,
        submission_started_at: Optional[datetime.datetime] = None,
        monitoring_started_at: Optional[datetime.datetime] = None,
        cleanup_started_at: Optional[datetime.datetime] = None,
        current_state: Optional[Union[str, "PluginCurrentState"]] = None,
        **kwargs
    ):
        """
        :keyword preparation_started_at:
        :paramtype preparation_started_at: ~datetime.datetime
        :keyword resource_acquisition_started_at:
        :paramtype resource_acquisition_started_at: ~datetime.datetime
        :keyword submission_started_at:
        :paramtype submission_started_at: ~datetime.datetime
        :keyword monitoring_started_at:
        :paramtype monitoring_started_at: ~datetime.datetime
        :keyword cleanup_started_at:
        :paramtype cleanup_started_at: ~datetime.datetime
        :keyword current_state: Possible values include: "Preparation", "ResourceAcquisition",
         "Queued", "Submission", "Monitoring", "Cleanup", "Ended".
        :paramtype current_state: str or ~azure.synapse.spark.models.PluginCurrentState
        """
        super(SparkServicePlugin, self).__init__(**kwargs)
        self.preparation_started_at = preparation_started_at
        self.resource_acquisition_started_at = resource_acquisition_started_at
        self.submission_started_at = submission_started_at
        self.monitoring_started_at = monitoring_started_at
        self.cleanup_started_at = cleanup_started_at
        self.current_state = current_state


class SparkSession(msrest.serialization.Model):
    """SparkSession.

    All required parameters must be populated in order to send to Azure.

    :ivar livy_info:
    :vartype livy_info: ~azure.synapse.spark.models.SparkSessionState
    :ivar name:
    :vartype name: str
    :ivar workspace_name:
    :vartype workspace_name: str
    :ivar spark_pool_name:
    :vartype spark_pool_name: str
    :ivar submitter_name:
    :vartype submitter_name: str
    :ivar submitter_id:
    :vartype submitter_id: str
    :ivar artifact_id:
    :vartype artifact_id: str
    :ivar job_type: The job type. Possible values include: "SparkBatch", "SparkSession".
    :vartype job_type: str or ~azure.synapse.spark.models.SparkJobType
    :ivar result: Possible values include: "Uncertain", "Succeeded", "Failed", "Cancelled".
    :vartype result: str or ~azure.synapse.spark.models.SparkSessionResultType
    :ivar scheduler:
    :vartype scheduler: ~azure.synapse.spark.models.SparkScheduler
    :ivar plugin:
    :vartype plugin: ~azure.synapse.spark.models.SparkServicePlugin
    :ivar errors:
    :vartype errors: list[~azure.synapse.spark.models.SparkServiceError]
    :ivar tags: A set of tags. Dictionary of :code:`<string>`.
    :vartype tags: dict[str, str]
    :ivar id: Required.
    :vartype id: int
    :ivar app_id:
    :vartype app_id: str
    :ivar app_info: Dictionary of :code:`<string>`.
    :vartype app_info: dict[str, str]
    :ivar state: The session state. Possible values include: "not_started", "starting", "idle",
     "busy", "shutting_down", "error", "dead", "killed", "success", "running", "recovering".
    :vartype state: str or ~azure.synapse.spark.models.LivyStates
    :ivar log_lines:
    :vartype log_lines: list[str]
    """

    _validation = {
        'id': {'required': True},
    }

    _attribute_map = {
        'livy_info': {'key': 'livyInfo', 'type': 'SparkSessionState'},
        'name': {'key': 'name', 'type': 'str'},
        'workspace_name': {'key': 'workspaceName', 'type': 'str'},
        'spark_pool_name': {'key': 'sparkPoolName', 'type': 'str'},
        'submitter_name': {'key': 'submitterName', 'type': 'str'},
        'submitter_id': {'key': 'submitterId', 'type': 'str'},
        'artifact_id': {'key': 'artifactId', 'type': 'str'},
        'job_type': {'key': 'jobType', 'type': 'str'},
        'result': {'key': 'result', 'type': 'str'},
        'scheduler': {'key': 'schedulerInfo', 'type': 'SparkScheduler'},
        'plugin': {'key': 'pluginInfo', 'type': 'SparkServicePlugin'},
        'errors': {'key': 'errorInfo', 'type': '[SparkServiceError]'},
        'tags': {'key': 'tags', 'type': '{str}'},
        'id': {'key': 'id', 'type': 'int'},
        'app_id': {'key': 'appId', 'type': 'str'},
        'app_info': {'key': 'appInfo', 'type': '{str}'},
        'state': {'key': 'state', 'type': 'str'},
        'log_lines': {'key': 'log', 'type': '[str]'},
    }

    def __init__(
        self,
        *,
        id: int,
        livy_info: Optional["SparkSessionState"] = None,
        name: Optional[str] = None,
        workspace_name: Optional[str] = None,
        spark_pool_name: Optional[str] = None,
        submitter_name: Optional[str] = None,
        submitter_id: Optional[str] = None,
        artifact_id: Optional[str] = None,
        job_type: Optional[Union[str, "SparkJobType"]] = None,
        result: Optional[Union[str, "SparkSessionResultType"]] = None,
        scheduler: Optional["SparkScheduler"] = None,
        plugin: Optional["SparkServicePlugin"] = None,
        errors: Optional[List["SparkServiceError"]] = None,
        tags: Optional[Dict[str, str]] = None,
        app_id: Optional[str] = None,
        app_info: Optional[Dict[str, str]] = None,
        state: Optional[Union[str, "LivyStates"]] = None,
        log_lines: Optional[List[str]] = None,
        **kwargs
    ):
        """
        :keyword livy_info:
        :paramtype livy_info: ~azure.synapse.spark.models.SparkSessionState
        :keyword name:
        :paramtype name: str
        :keyword workspace_name:
        :paramtype workspace_name: str
        :keyword spark_pool_name:
        :paramtype spark_pool_name: str
        :keyword submitter_name:
        :paramtype submitter_name: str
        :keyword submitter_id:
        :paramtype submitter_id: str
        :keyword artifact_id:
        :paramtype artifact_id: str
        :keyword job_type: The job type. Possible values include: "SparkBatch", "SparkSession".
        :paramtype job_type: str or ~azure.synapse.spark.models.SparkJobType
        :keyword result: Possible values include: "Uncertain", "Succeeded", "Failed", "Cancelled".
        :paramtype result: str or ~azure.synapse.spark.models.SparkSessionResultType
        :keyword scheduler:
        :paramtype scheduler: ~azure.synapse.spark.models.SparkScheduler
        :keyword plugin:
        :paramtype plugin: ~azure.synapse.spark.models.SparkServicePlugin
        :keyword errors:
        :paramtype errors: list[~azure.synapse.spark.models.SparkServiceError]
        :keyword tags: A set of tags. Dictionary of :code:`<string>`.
        :paramtype tags: dict[str, str]
        :keyword id: Required.
        :paramtype id: int
        :keyword app_id:
        :paramtype app_id: str
        :keyword app_info: Dictionary of :code:`<string>`.
        :paramtype app_info: dict[str, str]
        :keyword state: The session state. Possible values include: "not_started", "starting", "idle",
         "busy", "shutting_down", "error", "dead", "killed", "success", "running", "recovering".
        :paramtype state: str or ~azure.synapse.spark.models.LivyStates
        :keyword log_lines:
        :paramtype log_lines: list[str]
        """
        super(SparkSession, self).__init__(**kwargs)
        self.livy_info = livy_info
        self.name = name
        self.workspace_name = workspace_name
        self.spark_pool_name = spark_pool_name
        self.submitter_name = submitter_name
        self.submitter_id = submitter_id
        self.artifact_id = artifact_id
        self.job_type = job_type
        self.result = result
        self.scheduler = scheduler
        self.plugin = plugin
        self.errors = errors
        self.tags = tags
        self.id = id
        self.app_id = app_id
        self.app_info = app_info
        self.state = state
        self.log_lines = log_lines


class SparkSessionCollection(msrest.serialization.Model):
    """SparkSessionCollection.

    All required parameters must be populated in order to send to Azure.

    :ivar from_property: Required.
    :vartype from_property: int
    :ivar total: Required.
    :vartype total: int
    :ivar sessions:
    :vartype sessions: list[~azure.synapse.spark.models.SparkSession]
    """

    _validation = {
        'from_property': {'required': True},
        'total': {'required': True},
    }

    _attribute_map = {
        'from_property': {'key': 'from', 'type': 'int'},
        'total': {'key': 'total', 'type': 'int'},
        'sessions': {'key': 'sessions', 'type': '[SparkSession]'},
    }

    def __init__(
        self,
        *,
        from_property: int,
        total: int,
        sessions: Optional[List["SparkSession"]] = None,
        **kwargs
    ):
        """
        :keyword from_property: Required.
        :paramtype from_property: int
        :keyword total: Required.
        :paramtype total: int
        :keyword sessions:
        :paramtype sessions: list[~azure.synapse.spark.models.SparkSession]
        """
        super(SparkSessionCollection, self).__init__(**kwargs)
        self.from_property = from_property
        self.total = total
        self.sessions = sessions


class SparkSessionOptions(msrest.serialization.Model):
    """SparkSessionOptions.

    All required parameters must be populated in order to send to Azure.

    :ivar tags: A set of tags. Dictionary of :code:`<string>`.
    :vartype tags: dict[str, str]
    :ivar artifact_id:
    :vartype artifact_id: str
    :ivar name: Required.
    :vartype name: str
    :ivar file:
    :vartype file: str
    :ivar class_name:
    :vartype class_name: str
    :ivar arguments:
    :vartype arguments: list[str]
    :ivar jars:
    :vartype jars: list[str]
    :ivar python_files:
    :vartype python_files: list[str]
    :ivar files:
    :vartype files: list[str]
    :ivar archives:
    :vartype archives: list[str]
    :ivar configuration: Dictionary of :code:`<string>`.
    :vartype configuration: dict[str, str]
    :ivar driver_memory:
    :vartype driver_memory: str
    :ivar driver_cores:
    :vartype driver_cores: int
    :ivar executor_memory:
    :vartype executor_memory: str
    :ivar executor_cores:
    :vartype executor_cores: int
    :ivar executor_count:
    :vartype executor_count: int
    """

    _validation = {
        'name': {'required': True},
    }

    _attribute_map = {
        'tags': {'key': 'tags', 'type': '{str}'},
        'artifact_id': {'key': 'artifactId', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'file': {'key': 'file', 'type': 'str'},
        'class_name': {'key': 'className', 'type': 'str'},
        'arguments': {'key': 'args', 'type': '[str]'},
        'jars': {'key': 'jars', 'type': '[str]'},
        'python_files': {'key': 'pyFiles', 'type': '[str]'},
        'files': {'key': 'files', 'type': '[str]'},
        'archives': {'key': 'archives', 'type': '[str]'},
        'configuration': {'key': 'conf', 'type': '{str}'},
        'driver_memory': {'key': 'driverMemory', 'type': 'str'},
        'driver_cores': {'key': 'driverCores', 'type': 'int'},
        'executor_memory': {'key': 'executorMemory', 'type': 'str'},
        'executor_cores': {'key': 'executorCores', 'type': 'int'},
        'executor_count': {'key': 'numExecutors', 'type': 'int'},
    }

    def __init__(
        self,
        *,
        name: str,
        tags: Optional[Dict[str, str]] = None,
        artifact_id: Optional[str] = None,
        file: Optional[str] = None,
        class_name: Optional[str] = None,
        arguments: Optional[List[str]] = None,
        jars: Optional[List[str]] = None,
        python_files: Optional[List[str]] = None,
        files: Optional[List[str]] = None,
        archives: Optional[List[str]] = None,
        configuration: Optional[Dict[str, str]] = None,
        driver_memory: Optional[str] = None,
        driver_cores: Optional[int] = None,
        executor_memory: Optional[str] = None,
        executor_cores: Optional[int] = None,
        executor_count: Optional[int] = None,
        **kwargs
    ):
        """
        :keyword tags: A set of tags. Dictionary of :code:`<string>`.
        :paramtype tags: dict[str, str]
        :keyword artifact_id:
        :paramtype artifact_id: str
        :keyword name: Required.
        :paramtype name: str
        :keyword file:
        :paramtype file: str
        :keyword class_name:
        :paramtype class_name: str
        :keyword arguments:
        :paramtype arguments: list[str]
        :keyword jars:
        :paramtype jars: list[str]
        :keyword python_files:
        :paramtype python_files: list[str]
        :keyword files:
        :paramtype files: list[str]
        :keyword archives:
        :paramtype archives: list[str]
        :keyword configuration: Dictionary of :code:`<string>`.
        :paramtype configuration: dict[str, str]
        :keyword driver_memory:
        :paramtype driver_memory: str
        :keyword driver_cores:
        :paramtype driver_cores: int
        :keyword executor_memory:
        :paramtype executor_memory: str
        :keyword executor_cores:
        :paramtype executor_cores: int
        :keyword executor_count:
        :paramtype executor_count: int
        """
        super(SparkSessionOptions, self).__init__(**kwargs)
        self.tags = tags
        self.artifact_id = artifact_id
        self.name = name
        self.file = file
        self.class_name = class_name
        self.arguments = arguments
        self.jars = jars
        self.python_files = python_files
        self.files = files
        self.archives = archives
        self.configuration = configuration
        self.driver_memory = driver_memory
        self.driver_cores = driver_cores
        self.executor_memory = executor_memory
        self.executor_cores = executor_cores
        self.executor_count = executor_count


class SparkSessionState(msrest.serialization.Model):
    """SparkSessionState.

    :ivar not_started_at:
    :vartype not_started_at: ~datetime.datetime
    :ivar starting_at:
    :vartype starting_at: ~datetime.datetime
    :ivar idle_at:
    :vartype idle_at: ~datetime.datetime
    :ivar dead_at:
    :vartype dead_at: ~datetime.datetime
    :ivar shutting_down_at:
    :vartype shutting_down_at: ~datetime.datetime
    :ivar terminated_at:
    :vartype terminated_at: ~datetime.datetime
    :ivar recovering_at:
    :vartype recovering_at: ~datetime.datetime
    :ivar busy_at:
    :vartype busy_at: ~datetime.datetime
    :ivar error_at:
    :vartype error_at: ~datetime.datetime
    :ivar current_state:
    :vartype current_state: str
    :ivar job_creation_request:
    :vartype job_creation_request: ~azure.synapse.spark.models.SparkRequest
    """

    _attribute_map = {
        'not_started_at': {'key': 'notStartedAt', 'type': 'iso-8601'},
        'starting_at': {'key': 'startingAt', 'type': 'iso-8601'},
        'idle_at': {'key': 'idleAt', 'type': 'iso-8601'},
        'dead_at': {'key': 'deadAt', 'type': 'iso-8601'},
        'shutting_down_at': {'key': 'shuttingDownAt', 'type': 'iso-8601'},
        'terminated_at': {'key': 'killedAt', 'type': 'iso-8601'},
        'recovering_at': {'key': 'recoveringAt', 'type': 'iso-8601'},
        'busy_at': {'key': 'busyAt', 'type': 'iso-8601'},
        'error_at': {'key': 'errorAt', 'type': 'iso-8601'},
        'current_state': {'key': 'currentState', 'type': 'str'},
        'job_creation_request': {'key': 'jobCreationRequest', 'type': 'SparkRequest'},
    }

    def __init__(
        self,
        *,
        not_started_at: Optional[datetime.datetime] = None,
        starting_at: Optional[datetime.datetime] = None,
        idle_at: Optional[datetime.datetime] = None,
        dead_at: Optional[datetime.datetime] = None,
        shutting_down_at: Optional[datetime.datetime] = None,
        terminated_at: Optional[datetime.datetime] = None,
        recovering_at: Optional[datetime.datetime] = None,
        busy_at: Optional[datetime.datetime] = None,
        error_at: Optional[datetime.datetime] = None,
        current_state: Optional[str] = None,
        job_creation_request: Optional["SparkRequest"] = None,
        **kwargs
    ):
        """
        :keyword not_started_at:
        :paramtype not_started_at: ~datetime.datetime
        :keyword starting_at:
        :paramtype starting_at: ~datetime.datetime
        :keyword idle_at:
        :paramtype idle_at: ~datetime.datetime
        :keyword dead_at:
        :paramtype dead_at: ~datetime.datetime
        :keyword shutting_down_at:
        :paramtype shutting_down_at: ~datetime.datetime
        :keyword terminated_at:
        :paramtype terminated_at: ~datetime.datetime
        :keyword recovering_at:
        :paramtype recovering_at: ~datetime.datetime
        :keyword busy_at:
        :paramtype busy_at: ~datetime.datetime
        :keyword error_at:
        :paramtype error_at: ~datetime.datetime
        :keyword current_state:
        :paramtype current_state: str
        :keyword job_creation_request:
        :paramtype job_creation_request: ~azure.synapse.spark.models.SparkRequest
        """
        super(SparkSessionState, self).__init__(**kwargs)
        self.not_started_at = not_started_at
        self.starting_at = starting_at
        self.idle_at = idle_at
        self.dead_at = dead_at
        self.shutting_down_at = shutting_down_at
        self.terminated_at = terminated_at
        self.recovering_at = recovering_at
        self.busy_at = busy_at
        self.error_at = error_at
        self.current_state = current_state
        self.job_creation_request = job_creation_request


class SparkStatement(msrest.serialization.Model):
    """SparkStatement.

    All required parameters must be populated in order to send to Azure.

    :ivar id: Required.
    :vartype id: int
    :ivar code:
    :vartype code: str
    :ivar state: Possible values include: "waiting", "running", "available", "error", "cancelling",
     "cancelled".
    :vartype state: str or ~azure.synapse.spark.models.LivyStatementStates
    :ivar output:
    :vartype output: ~azure.synapse.spark.models.SparkStatementOutput
    """

    _validation = {
        'id': {'required': True},
    }

    _attribute_map = {
        'id': {'key': 'id', 'type': 'int'},
        'code': {'key': 'code', 'type': 'str'},
        'state': {'key': 'state', 'type': 'str'},
        'output': {'key': 'output', 'type': 'SparkStatementOutput'},
    }

    def __init__(
        self,
        *,
        id: int,
        code: Optional[str] = None,
        state: Optional[Union[str, "LivyStatementStates"]] = None,
        output: Optional["SparkStatementOutput"] = None,
        **kwargs
    ):
        """
        :keyword id: Required.
        :paramtype id: int
        :keyword code:
        :paramtype code: str
        :keyword state: Possible values include: "waiting", "running", "available", "error",
         "cancelling", "cancelled".
        :paramtype state: str or ~azure.synapse.spark.models.LivyStatementStates
        :keyword output:
        :paramtype output: ~azure.synapse.spark.models.SparkStatementOutput
        """
        super(SparkStatement, self).__init__(**kwargs)
        self.id = id
        self.code = code
        self.state = state
        self.output = output


class SparkStatementCancellationResult(msrest.serialization.Model):
    """SparkStatementCancellationResult.

    :ivar message: The msg property from the Livy API. The value is always "canceled".
    :vartype message: str
    """

    _attribute_map = {
        'message': {'key': 'msg', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        message: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword message: The msg property from the Livy API. The value is always "canceled".
        :paramtype message: str
        """
        super(SparkStatementCancellationResult, self).__init__(**kwargs)
        self.message = message


class SparkStatementCollection(msrest.serialization.Model):
    """SparkStatementCollection.

    All required parameters must be populated in order to send to Azure.

    :ivar total: Required.
    :vartype total: int
    :ivar statements:
    :vartype statements: list[~azure.synapse.spark.models.SparkStatement]
    """

    _validation = {
        'total': {'required': True},
    }

    _attribute_map = {
        'total': {'key': 'total_statements', 'type': 'int'},
        'statements': {'key': 'statements', 'type': '[SparkStatement]'},
    }

    def __init__(
        self,
        *,
        total: int,
        statements: Optional[List["SparkStatement"]] = None,
        **kwargs
    ):
        """
        :keyword total: Required.
        :paramtype total: int
        :keyword statements:
        :paramtype statements: list[~azure.synapse.spark.models.SparkStatement]
        """
        super(SparkStatementCollection, self).__init__(**kwargs)
        self.total = total
        self.statements = statements


class SparkStatementOptions(msrest.serialization.Model):
    """SparkStatementOptions.

    :ivar code:
    :vartype code: str
    :ivar kind: Possible values include: "spark", "pyspark", "dotnetspark", "sql".
    :vartype kind: str or ~azure.synapse.spark.models.SparkStatementLanguageType
    """

    _attribute_map = {
        'code': {'key': 'code', 'type': 'str'},
        'kind': {'key': 'kind', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        code: Optional[str] = None,
        kind: Optional[Union[str, "SparkStatementLanguageType"]] = None,
        **kwargs
    ):
        """
        :keyword code:
        :paramtype code: str
        :keyword kind: Possible values include: "spark", "pyspark", "dotnetspark", "sql".
        :paramtype kind: str or ~azure.synapse.spark.models.SparkStatementLanguageType
        """
        super(SparkStatementOptions, self).__init__(**kwargs)
        self.code = code
        self.kind = kind


class SparkStatementOutput(msrest.serialization.Model):
    """SparkStatementOutput.

    All required parameters must be populated in order to send to Azure.

    :ivar status:
    :vartype status: str
    :ivar execution_count: Required.
    :vartype execution_count: int
    :ivar data: Any object.
    :vartype data: any
    :ivar error_name:
    :vartype error_name: str
    :ivar error_value:
    :vartype error_value: str
    :ivar traceback:
    :vartype traceback: list[str]
    """

    _validation = {
        'execution_count': {'required': True},
    }

    _attribute_map = {
        'status': {'key': 'status', 'type': 'str'},
        'execution_count': {'key': 'execution_count', 'type': 'int'},
        'data': {'key': 'data', 'type': 'object'},
        'error_name': {'key': 'ename', 'type': 'str'},
        'error_value': {'key': 'evalue', 'type': 'str'},
        'traceback': {'key': 'traceback', 'type': '[str]'},
    }

    def __init__(
        self,
        *,
        execution_count: int,
        status: Optional[str] = None,
        data: Optional[Any] = None,
        error_name: Optional[str] = None,
        error_value: Optional[str] = None,
        traceback: Optional[List[str]] = None,
        **kwargs
    ):
        """
        :keyword status:
        :paramtype status: str
        :keyword execution_count: Required.
        :paramtype execution_count: int
        :keyword data: Any object.
        :paramtype data: any
        :keyword error_name:
        :paramtype error_name: str
        :keyword error_value:
        :paramtype error_value: str
        :keyword traceback:
        :paramtype traceback: list[str]
        """
        super(SparkStatementOutput, self).__init__(**kwargs)
        self.status = status
        self.execution_count = execution_count
        self.data = data
        self.error_name = error_name
        self.error_value = error_value
        self.traceback = traceback
