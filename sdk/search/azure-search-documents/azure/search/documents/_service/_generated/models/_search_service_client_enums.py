# coding=utf-8
# --------------------------------------------------------------------------
# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.0.6257, generator: {generator})
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------

from enum import Enum

class DataSourceType(str, Enum):
    """Defines the type of a datasource.
    """

    azure_sql = "azuresql"
    cosmos_db = "cosmosdb"
    azure_blob = "azureblob"
    azure_table = "azuretable"
    my_sql = "mysql"

class IndexerExecutionStatus(str, Enum):
    """Represents the status of an individual indexer execution.
    """

    transient_failure = "transientFailure"
    success = "success"
    in_progress = "inProgress"
    reset = "reset"

class DataType(str, Enum):
    """Defines the data type of a field in a search index.
    """

    edm_string = "Edm.String"
    edm_int32 = "Edm.Int32"
    edm_int64 = "Edm.Int64"
    edm_double = "Edm.Double"
    edm_boolean = "Edm.Boolean"
    edm_date_time_offset = "Edm.DateTimeOffset"
    edm_geography_point = "Edm.GeographyPoint"
    edm_complex_type = "Edm.ComplexType"

class AnalyzerName(str, Enum):
    """Defines the names of all text analyzers supported by Azure Cognitive Search.
    """

    ar_microsoft = "ar.microsoft"  #: Microsoft analyzer for Arabic.
    ar_lucene = "ar.lucene"  #: Lucene analyzer for Arabic.
    hy_lucene = "hy.lucene"  #: Lucene analyzer for Armenian.
    bn_microsoft = "bn.microsoft"  #: Microsoft analyzer for Bangla.
    eu_lucene = "eu.lucene"  #: Lucene analyzer for Basque.
    bg_microsoft = "bg.microsoft"  #: Microsoft analyzer for Bulgarian.
    bg_lucene = "bg.lucene"  #: Lucene analyzer for Bulgarian.
    ca_microsoft = "ca.microsoft"  #: Microsoft analyzer for Catalan.
    ca_lucene = "ca.lucene"  #: Lucene analyzer for Catalan.
    zh_hans_microsoft = "zh-Hans.microsoft"  #: Microsoft analyzer for Chinese (Simplified).
    zh_hans_lucene = "zh-Hans.lucene"  #: Lucene analyzer for Chinese (Simplified).
    zh_hant_microsoft = "zh-Hant.microsoft"  #: Microsoft analyzer for Chinese (Traditional).
    zh_hant_lucene = "zh-Hant.lucene"  #: Lucene analyzer for Chinese (Traditional).
    hr_microsoft = "hr.microsoft"  #: Microsoft analyzer for Croatian.
    cs_microsoft = "cs.microsoft"  #: Microsoft analyzer for Czech.
    cs_lucene = "cs.lucene"  #: Lucene analyzer for Czech.
    da_microsoft = "da.microsoft"  #: Microsoft analyzer for Danish.
    da_lucene = "da.lucene"  #: Lucene analyzer for Danish.
    nl_microsoft = "nl.microsoft"  #: Microsoft analyzer for Dutch.
    nl_lucene = "nl.lucene"  #: Lucene analyzer for Dutch.
    en_microsoft = "en.microsoft"  #: Microsoft analyzer for English.
    en_lucene = "en.lucene"  #: Lucene analyzer for English.
    et_microsoft = "et.microsoft"  #: Microsoft analyzer for Estonian.
    fi_microsoft = "fi.microsoft"  #: Microsoft analyzer for Finnish.
    fi_lucene = "fi.lucene"  #: Lucene analyzer for Finnish.
    fr_microsoft = "fr.microsoft"  #: Microsoft analyzer for French.
    fr_lucene = "fr.lucene"  #: Lucene analyzer for French.
    gl_lucene = "gl.lucene"  #: Lucene analyzer for Galician.
    de_microsoft = "de.microsoft"  #: Microsoft analyzer for German.
    de_lucene = "de.lucene"  #: Lucene analyzer for German.
    el_microsoft = "el.microsoft"  #: Microsoft analyzer for Greek.
    el_lucene = "el.lucene"  #: Lucene analyzer for Greek.
    gu_microsoft = "gu.microsoft"  #: Microsoft analyzer for Gujarati.
    he_microsoft = "he.microsoft"  #: Microsoft analyzer for Hebrew.
    hi_microsoft = "hi.microsoft"  #: Microsoft analyzer for Hindi.
    hi_lucene = "hi.lucene"  #: Lucene analyzer for Hindi.
    hu_microsoft = "hu.microsoft"  #: Microsoft analyzer for Hungarian.
    hu_lucene = "hu.lucene"  #: Lucene analyzer for Hungarian.
    is_microsoft = "is.microsoft"  #: Microsoft analyzer for Icelandic.
    id_microsoft = "id.microsoft"  #: Microsoft analyzer for Indonesian (Bahasa).
    id_lucene = "id.lucene"  #: Lucene analyzer for Indonesian.
    ga_lucene = "ga.lucene"  #: Lucene analyzer for Irish.
    it_microsoft = "it.microsoft"  #: Microsoft analyzer for Italian.
    it_lucene = "it.lucene"  #: Lucene analyzer for Italian.
    ja_microsoft = "ja.microsoft"  #: Microsoft analyzer for Japanese.
    ja_lucene = "ja.lucene"  #: Lucene analyzer for Japanese.
    kn_microsoft = "kn.microsoft"  #: Microsoft analyzer for Kannada.
    ko_microsoft = "ko.microsoft"  #: Microsoft analyzer for Korean.
    ko_lucene = "ko.lucene"  #: Lucene analyzer for Korean.
    lv_microsoft = "lv.microsoft"  #: Microsoft analyzer for Latvian.
    lv_lucene = "lv.lucene"  #: Lucene analyzer for Latvian.
    lt_microsoft = "lt.microsoft"  #: Microsoft analyzer for Lithuanian.
    ml_microsoft = "ml.microsoft"  #: Microsoft analyzer for Malayalam.
    ms_microsoft = "ms.microsoft"  #: Microsoft analyzer for Malay (Latin).
    mr_microsoft = "mr.microsoft"  #: Microsoft analyzer for Marathi.
    nb_microsoft = "nb.microsoft"  #: Microsoft analyzer for Norwegian (Bokm√•l).
    no_lucene = "no.lucene"  #: Lucene analyzer for Norwegian.
    fa_lucene = "fa.lucene"  #: Lucene analyzer for Persian.
    pl_microsoft = "pl.microsoft"  #: Microsoft analyzer for Polish.
    pl_lucene = "pl.lucene"  #: Lucene analyzer for Polish.
    pt_br_microsoft = "pt-BR.microsoft"  #: Microsoft analyzer for Portuguese (Brazil).
    pt_br_lucene = "pt-BR.lucene"  #: Lucene analyzer for Portuguese (Brazil).
    pt_microsoft = "pt-PT.microsoft"  #: Microsoft analyzer for Portuguese (Portugal).
    pt_lucene = "pt-PT.lucene"  #: Lucene analyzer for Portuguese (Portugal).
    pa_microsoft = "pa.microsoft"  #: Microsoft analyzer for Punjabi.
    ro_microsoft = "ro.microsoft"  #: Microsoft analyzer for Romanian.
    ro_lucene = "ro.lucene"  #: Lucene analyzer for Romanian.
    ru_microsoft = "ru.microsoft"  #: Microsoft analyzer for Russian.
    ru_lucene = "ru.lucene"  #: Lucene analyzer for Russian.
    sr_cyrillic_microsoft = "sr-cyrillic.microsoft"  #: Microsoft analyzer for Serbian (Cyrillic).
    sr_latin_microsoft = "sr-latin.microsoft"  #: Microsoft analyzer for Serbian (Latin).
    sk_microsoft = "sk.microsoft"  #: Microsoft analyzer for Slovak.
    sl_microsoft = "sl.microsoft"  #: Microsoft analyzer for Slovenian.
    es_microsoft = "es.microsoft"  #: Microsoft analyzer for Spanish.
    es_lucene = "es.lucene"  #: Lucene analyzer for Spanish.
    sv_microsoft = "sv.microsoft"  #: Microsoft analyzer for Swedish.
    sv_lucene = "sv.lucene"  #: Lucene analyzer for Swedish.
    ta_microsoft = "ta.microsoft"  #: Microsoft analyzer for Tamil.
    te_microsoft = "te.microsoft"  #: Microsoft analyzer for Telugu.
    th_microsoft = "th.microsoft"  #: Microsoft analyzer for Thai.
    th_lucene = "th.lucene"  #: Lucene analyzer for Thai.
    tr_microsoft = "tr.microsoft"  #: Microsoft analyzer for Turkish.
    tr_lucene = "tr.lucene"  #: Lucene analyzer for Turkish.
    uk_microsoft = "uk.microsoft"  #: Microsoft analyzer for Ukrainian.
    ur_microsoft = "ur.microsoft"  #: Microsoft analyzer for Urdu.
    vi_microsoft = "vi.microsoft"  #: Microsoft analyzer for Vietnamese.
    standard_lucene = "standard.lucene"  #: Standard Lucene analyzer.
    standard_ascii_folding_lucene = "standardasciifolding.lucene"  #: Standard ASCII Folding Lucene analyzer. See https://docs.microsoft.com/rest/api/searchservice/Custom-analyzers-in-Azure-Search#Analyzers.
    keyword = "keyword"  #: Treats the entire content of a field as a single token. This is useful for data like zip codes, ids, and some product names. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/KeywordAnalyzer.html.
    pattern = "pattern"  #: Flexibly separates text into terms via a regular expression pattern. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/PatternAnalyzer.html.
    simple = "simple"  #: Divides text at non-letters and converts them to lower case. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/SimpleAnalyzer.html.
    stop = "stop"  #: Divides text at non-letters; Applies the lowercase and stopword token filters. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/StopAnalyzer.html.
    whitespace = "whitespace"  #: An analyzer that uses the whitespace tokenizer. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/WhitespaceAnalyzer.html.

class ScoringFunctionInterpolation(str, Enum):
    """Defines the function used to interpolate score boosting across a range of documents.
    """

    linear = "linear"
    constant = "constant"
    quadratic = "quadratic"
    logarithmic = "logarithmic"

class ScoringFunctionAggregation(str, Enum):
    """Defines the aggregation function used to combine the results of all the scoring functions in a
    scoring profile.
    """

    sum = "sum"
    average = "average"
    minimum = "minimum"
    maximum = "maximum"
    first_matching = "firstMatching"

class TokenFilterName(str, Enum):
    """Defines the names of all token filters supported by Azure Cognitive Search.
    """

    arabic_normalization = "arabic_normalization"  #: A token filter that applies the Arabic normalizer to normalize the orthography. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ar/ArabicNormalizationFilter.html.
    apostrophe = "apostrophe"  #: Strips all characters after an apostrophe (including the apostrophe itself). See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/tr/ApostropheFilter.html.
    ascii_folding = "asciifolding"  #: Converts alphabetic, numeric, and symbolic Unicode characters which are not in the first 127 ASCII characters (the "Basic Latin" Unicode block) into their ASCII equivalents, if such equivalents exist. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/ASCIIFoldingFilter.html.
    cjk_bigram = "cjk_bigram"  #: Forms bigrams of CJK terms that are generated from StandardTokenizer. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/cjk/CJKBigramFilter.html.
    cjk_width = "cjk_width"  #: Normalizes CJK width differences. Folds fullwidth ASCII variants into the equivalent basic Latin, and half-width Katakana variants into the equivalent Kana. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/cjk/CJKWidthFilter.html.
    classic = "classic"  #: Removes English possessives, and dots from acronyms. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/standard/ClassicFilter.html.
    common_gram = "common_grams"  #: Construct bigrams for frequently occurring terms while indexing. Single terms are still indexed too, with bigrams overlaid. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/commongrams/CommonGramsFilter.html.
    edge_n_gram = "edgeNGram_v2"  #: Generates n-grams of the given size(s) starting from the front or the back of an input token. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.html.
    elision = "elision"  #: Removes elisions. For example, "l'avion" (the plane) will be converted to "avion" (plane). See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/util/ElisionFilter.html.
    german_normalization = "german_normalization"  #: Normalizes German characters according to the heuristics of the German2 snowball algorithm. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/de/GermanNormalizationFilter.html.
    hindi_normalization = "hindi_normalization"  #: Normalizes text in Hindi to remove some differences in spelling variations. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/hi/HindiNormalizationFilter.html.
    indic_normalization = "indic_normalization"  #: Normalizes the Unicode representation of text in Indian languages. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/in/IndicNormalizationFilter.html.
    keyword_repeat = "keyword_repeat"  #: Emits each incoming token twice, once as keyword and once as non-keyword. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/KeywordRepeatFilter.html.
    k_stem = "kstem"  #: A high-performance kstem filter for English. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/en/KStemFilter.html.
    length = "length"  #: Removes words that are too long or too short. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/LengthFilter.html.
    limit = "limit"  #: Limits the number of tokens while indexing. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/LimitTokenCountFilter.html.
    lowercase = "lowercase"  #: Normalizes token text to lower case. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/LowerCaseFilter.htm.
    n_gram = "nGram_v2"  #: Generates n-grams of the given size(s). See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ngram/NGramTokenFilter.html.
    persian_normalization = "persian_normalization"  #: Applies normalization for Persian. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/fa/PersianNormalizationFilter.html.
    phonetic = "phonetic"  #: Create tokens for phonetic matches. See https://lucene.apache.org/core/4_10_3/analyzers-phonetic/org/apache/lucene/analysis/phonetic/package-tree.html.
    porter_stem = "porter_stem"  #: Uses the Porter stemming algorithm to transform the token stream. See http://tartarus.org/~martin/PorterStemmer.
    reverse = "reverse"  #: Reverses the token string. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/reverse/ReverseStringFilter.html.
    scandinavian_normalization = "scandinavian_normalization"  #: Normalizes use of the interchangeable Scandinavian characters. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/ScandinavianNormalizationFilter.html.
    scandinavian_folding_normalization = "scandinavian_folding"  #: Folds Scandinavian characters √•√Ö√§√¶√Ñ√Ü-&gt;a and √∂√ñ√∏√ò-&gt;o. It also discriminates against use of double vowels aa, ae, ao, oe and oo, leaving just the first one. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/ScandinavianFoldingFilter.html.
    shingle = "shingle"  #: Creates combinations of tokens as a single token. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/shingle/ShingleFilter.html.
    snowball = "snowball"  #: A filter that stems words using a Snowball-generated stemmer. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/snowball/SnowballFilter.html.
    sorani_normalization = "sorani_normalization"  #: Normalizes the Unicode representation of Sorani text. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ckb/SoraniNormalizationFilter.html.
    stemmer = "stemmer"  #: Language specific stemming filter. See https://docs.microsoft.com/rest/api/searchservice/Custom-analyzers-in-Azure-Search#TokenFilters.
    stopwords = "stopwords"  #: Removes stop words from a token stream. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/StopFilter.html.
    trim = "trim"  #: Trims leading and trailing whitespace from tokens. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/TrimFilter.html.
    truncate = "truncate"  #: Truncates the terms to a specific length. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/TruncateTokenFilter.html.
    unique = "unique"  #: Filters out tokens with same text as the previous token. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/RemoveDuplicatesTokenFilter.html.
    uppercase = "uppercase"  #: Normalizes token text to upper case. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/UpperCaseFilter.html.
    word_delimiter = "word_delimiter"  #: Splits words into subwords and performs optional transformations on subword groups.

class TokenCharacterKind(str, Enum):
    """Represents classes of characters on which a token filter can operate.
    """

    letter = "letter"
    digit = "digit"
    whitespace = "whitespace"
    punctuation = "punctuation"
    symbol = "symbol"

class CjkBigramTokenFilterScripts(str, Enum):
    """Scripts that can be ignored by CjkBigramTokenFilter.
    """

    han = "han"
    hiragana = "hiragana"
    katakana = "katakana"
    hangul = "hangul"

class VisualFeature(str, Enum):
    """The strings indicating what visual feature types to return.
    """

    adult = "adult"
    brands = "brands"
    categories = "categories"
    description = "description"
    faces = "faces"
    objects = "objects"
    tags = "tags"

class ImageDetail(str, Enum):
    """A string indicating which domain-specific details to return.
    """

    celebrities = "celebrities"
    landmarks = "landmarks"

class EntityCategory(str, Enum):
    """A string indicating what entity categories to return.
    """

    location = "location"
    organization = "organization"
    person = "person"
    quantity = "quantity"
    datetime = "datetime"
    url = "url"
    email = "email"

class TokenizerName(str, Enum):
    """Defines the names of all tokenizers supported by Azure Cognitive Search.
    """

    classic = "classic"  #: Grammar-based tokenizer that is suitable for processing most European-language documents. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/standard/ClassicTokenizer.html.
    edge_n_gram = "edgeNGram"  #: Tokenizes the input from an edge into n-grams of the given size(s). See https://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ngram/EdgeNGramTokenizer.html.
    keyword = "keyword_v2"  #: Emits the entire input as a single token. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/KeywordTokenizer.html.
    letter = "letter"  #: Divides text at non-letters. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/LetterTokenizer.html.
    lowercase = "lowercase"  #: Divides text at non-letters and converts them to lower case. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/LowerCaseTokenizer.html.
    microsoft_language_tokenizer = "microsoft_language_tokenizer"  #: Divides text using language-specific rules.
    microsoft_language_stemming_tokenizer = "microsoft_language_stemming_tokenizer"  #: Divides text using language-specific rules and reduces words to their base forms.
    n_gram = "nGram"  #: Tokenizes the input into n-grams of the given size(s). See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ngram/NGramTokenizer.html.
    path_hierarchy = "path_hierarchy_v2"  #: Tokenizer for path-like hierarchies. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/path/PathHierarchyTokenizer.html.
    pattern = "pattern"  #: Tokenizer that uses regex pattern matching to construct distinct tokens. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/pattern/PatternTokenizer.html.
    standard = "standard_v2"  #: Standard Lucene analyzer; Composed of the standard tokenizer, lowercase filter and stop filter. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/standard/StandardTokenizer.html.
    uax_url_email = "uax_url_email"  #: Tokenizes urls and emails as one token. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizer.html.
    whitespace = "whitespace"  #: Divides text at whitespace. See http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/WhitespaceTokenizer.html.

class RegexFlags(str, Enum):
    """Defines flags that can be combined to control how regular expressions are used in the pattern
    analyzer and pattern tokenizer.
    """

    canon_eq = "CANON_EQ"
    case_insensitive = "CASE_INSENSITIVE"
    comments = "COMMENTS"
    dotall = "DOTALL"
    literal = "LITERAL"
    multiline = "MULTILINE"
    unicode_case = "UNICODE_CASE"
    unix_lines = "UNIX_LINES"

class KeyPhraseExtractionSkillLanguage(str, Enum):
    """The language codes supported for input text by KeyPhraseExtractionSkill.
    """

    da = "da"  #: Danish.
    nl = "nl"  #: Dutch.
    en = "en"  #: English.
    fi = "fi"  #: Finnish.
    fr = "fr"  #: French.
    de = "de"  #: German.
    it = "it"  #: Italian.
    ja = "ja"  #: Japanese.
    ko = "ko"  #: Korean.
    no = "no"  #: Norwegian (Bokmaal).
    pl = "pl"  #: Polish.
    pt = "pt-PT"  #: Portuguese (Portugal).
    pt_br = "pt-BR"  #: Portuguese (Brazil).
    ru = "ru"  #: Russian.
    es = "es"  #: Spanish.
    sv = "sv"  #: Swedish.

class OcrSkillLanguage(str, Enum):
    """The language codes supported for input by OcrSkill.
    """

    zh_hans = "zh-Hans"  #: Chinese-Simplified.
    zh_hant = "zh-Hant"  #: Chinese-Traditional.
    cs = "cs"  #: Czech.
    da = "da"  #: Danish.
    nl = "nl"  #: Dutch.
    en = "en"  #: English.
    fi = "fi"  #: Finnish.
    fr = "fr"  #: French.
    de = "de"  #: German.
    el = "el"  #: Greek.
    hu = "hu"  #: Hungarian.
    it = "it"  #: Italian.
    ja = "ja"  #: Japanese.
    ko = "ko"  #: Korean.
    nb = "nb"  #: Norwegian (Bokmaal).
    pl = "pl"  #: Polish.
    pt = "pt"  #: Portuguese.
    ru = "ru"  #: Russian.
    es = "es"  #: Spanish.
    sv = "sv"  #: Swedish.
    tr = "tr"  #: Turkish.
    ar = "ar"  #: Arabic.
    ro = "ro"  #: Romanian.
    sr_cyrl = "sr-Cyrl"  #: Serbian (Cyrillic, Serbia).
    sr_latn = "sr-Latn"  #: Serbian (Latin, Serbia).
    sk = "sk"  #: Slovak.

class ImageAnalysisSkillLanguage(str, Enum):
    """The language codes supported for input by ImageAnalysisSkill.
    """

    en = "en"  #: English.
    es = "es"  #: Spanish.
    ja = "ja"  #: Japanese.
    pt = "pt"  #: Portuguese.
    zh = "zh"  #: Chinese.

class EntityRecognitionSkillLanguage(str, Enum):
    """The language codes supported for input text by EntityRecognitionSkill.
    """

    ar = "ar"  #: Arabic.
    cs = "cs"  #: Czech.
    zh_hans = "zh-Hans"  #: Chinese-Simplified.
    zh_hant = "zh-Hant"  #: Chinese-Traditional.
    da = "da"  #: Danish.
    nl = "nl"  #: Dutch.
    en = "en"  #: English.
    fi = "fi"  #: Finnish.
    fr = "fr"  #: French.
    de = "de"  #: German.
    el = "el"  #: Greek.
    hu = "hu"  #: Hungarian.
    it = "it"  #: Italian.
    ja = "ja"  #: Japanese.
    ko = "ko"  #: Korean.
    no = "no"  #: Norwegian (Bokmaal).
    pl = "pl"  #: Polish.
    pt = "pt-PT"  #: Portuguese (Portugal).
    pt_br = "pt-BR"  #: Portuguese (Brazil).
    ru = "ru"  #: Russian.
    es = "es"  #: Spanish.
    sv = "sv"  #: Swedish.
    tr = "tr"  #: Turkish.

class SentimentSkillLanguage(str, Enum):
    """The language codes supported for input text by SentimentSkill.
    """

    da = "da"  #: Danish.
    nl = "nl"  #: Dutch.
    en = "en"  #: English.
    fi = "fi"  #: Finnish.
    fr = "fr"  #: French.
    de = "de"  #: German.
    el = "el"  #: Greek.
    it = "it"  #: Italian.
    no = "no"  #: Norwegian (Bokmaal).
    pl = "pl"  #: Polish.
    pt = "pt-PT"  #: Portuguese (Portugal).
    ru = "ru"  #: Russian.
    es = "es"  #: Spanish.
    sv = "sv"  #: Swedish.
    tr = "tr"  #: Turkish.

class SplitSkillLanguage(str, Enum):
    """The language codes supported for input text by SplitSkill.
    """

    da = "da"  #: Danish.
    de = "de"  #: German.
    en = "en"  #: English.
    es = "es"  #: Spanish.
    fi = "fi"  #: Finnish.
    fr = "fr"  #: French.
    it = "it"  #: Italian.
    ko = "ko"  #: Korean.
    pt = "pt"  #: Portuguese.

class TextTranslationSkillLanguage(str, Enum):
    """The language codes supported for input text by TextTranslationSkill.
    """

    af = "af"  #: Afrikaans.
    ar = "ar"  #: Arabic.
    bn = "bn"  #: Bangla.
    bs = "bs"  #: Bosnian (Latin).
    bg = "bg"  #: Bulgarian.
    yue = "yue"  #: Cantonese (Traditional).
    ca = "ca"  #: Catalan.
    zh_hans = "zh-Hans"  #: Chinese Simplified.
    zh_hant = "zh-Hant"  #: Chinese Traditional.
    hr = "hr"  #: Croatian.
    cs = "cs"  #: Czech.
    da = "da"  #: Danish.
    nl = "nl"  #: Dutch.
    en = "en"  #: English.
    et = "et"  #: Estonian.
    fj = "fj"  #: Fijian.
    fil = "fil"  #: Filipino.
    fi = "fi"  #: Finnish.
    fr = "fr"  #: French.
    de = "de"  #: German.
    el = "el"  #: Greek.
    ht = "ht"  #: Haitian Creole.
    he = "he"  #: Hebrew.
    hi = "hi"  #: Hindi.
    mww = "mww"  #: Hmong Daw.
    hu = "hu"  #: Hungarian.
    is_enum = "is"  #: Icelandic.
    id = "id"  #: Indonesian.
    it = "it"  #: Italian.
    ja = "ja"  #: Japanese.
    sw = "sw"  #: Kiswahili.
    tlh = "tlh"  #: Klingon.
    ko = "ko"  #: Korean.
    lv = "lv"  #: Latvian.
    lt = "lt"  #: Lithuanian.
    mg = "mg"  #: Malagasy.
    ms = "ms"  #: Malay.
    mt = "mt"  #: Maltese.
    nb = "nb"  #: Norwegian.
    fa = "fa"  #: Persian.
    pl = "pl"  #: Polish.
    pt = "pt"  #: Portuguese.
    otq = "otq"  #: Queretaro Otomi.
    ro = "ro"  #: Romanian.
    ru = "ru"  #: Russian.
    sm = "sm"  #: Samoan.
    sr_cyrl = "sr-Cyrl"  #: Serbian (Cyrillic).
    sr_latn = "sr-Latn"  #: Serbian (Latin).
    sk = "sk"  #: Slovak.
    sl = "sl"  #: Slovenian.
    es = "es"  #: Spanish.
    sv = "sv"  #: Swedish.
    ty = "ty"  #: Tahitian.
    ta = "ta"  #: Tamil.
    te = "te"  #: Telugu.
    th = "th"  #: Thai.
    to = "to"  #: Tongan.
    tr = "tr"  #: Turkish.
    uk = "uk"  #: Ukrainian.
    ur = "ur"  #: Urdu.
    vi = "vi"  #: Vietnamese.
    cy = "cy"  #: Welsh.
    yua = "yua"  #: Yucatec Maya.

class IndexerStatus(str, Enum):
    """Represents the overall indexer status.
    """

    unknown = "unknown"
    error = "error"
    running = "running"

class MicrosoftTokenizerLanguage(str, Enum):
    """Lists the languages supported by the Microsoft language tokenizer.
    """

    bangla = "bangla"
    bulgarian = "bulgarian"
    catalan = "catalan"
    chinese_simplified = "chineseSimplified"
    chinese_traditional = "chineseTraditional"
    croatian = "croatian"
    czech = "czech"
    danish = "danish"
    dutch = "dutch"
    english = "english"
    french = "french"
    german = "german"
    greek = "greek"
    gujarati = "gujarati"
    hindi = "hindi"
    icelandic = "icelandic"
    indonesian = "indonesian"
    italian = "italian"
    japanese = "japanese"
    kannada = "kannada"
    korean = "korean"
    malay = "malay"
    malayalam = "malayalam"
    marathi = "marathi"
    norwegian_bokmaal = "norwegianBokmaal"
    polish = "polish"
    portuguese = "portuguese"
    portuguese_brazilian = "portugueseBrazilian"
    punjabi = "punjabi"
    romanian = "romanian"
    russian = "russian"
    serbian_cyrillic = "serbianCyrillic"
    serbian_latin = "serbianLatin"
    slovenian = "slovenian"
    spanish = "spanish"
    swedish = "swedish"
    tamil = "tamil"
    telugu = "telugu"
    thai = "thai"
    ukrainian = "ukrainian"
    urdu = "urdu"
    vietnamese = "vietnamese"

class MicrosoftStemmingTokenizerLanguage(str, Enum):
    """Lists the languages supported by the Microsoft language stemming tokenizer.
    """

    arabic = "arabic"
    bangla = "bangla"
    bulgarian = "bulgarian"
    catalan = "catalan"
    croatian = "croatian"
    czech = "czech"
    danish = "danish"
    dutch = "dutch"
    english = "english"
    estonian = "estonian"
    finnish = "finnish"
    french = "french"
    german = "german"
    greek = "greek"
    gujarati = "gujarati"
    hebrew = "hebrew"
    hindi = "hindi"
    hungarian = "hungarian"
    icelandic = "icelandic"
    indonesian = "indonesian"
    italian = "italian"
    kannada = "kannada"
    latvian = "latvian"
    lithuanian = "lithuanian"
    malay = "malay"
    malayalam = "malayalam"
    marathi = "marathi"
    norwegian_bokmaal = "norwegianBokmaal"
    polish = "polish"
    portuguese = "portuguese"
    portuguese_brazilian = "portugueseBrazilian"
    punjabi = "punjabi"
    romanian = "romanian"
    russian = "russian"
    serbian_cyrillic = "serbianCyrillic"
    serbian_latin = "serbianLatin"
    slovak = "slovak"
    slovenian = "slovenian"
    spanish = "spanish"
    swedish = "swedish"
    tamil = "tamil"
    telugu = "telugu"
    turkish = "turkish"
    ukrainian = "ukrainian"
    urdu = "urdu"

class EdgeNGramTokenFilterSide(str, Enum):
    """Specifies which side of the input an n-gram should be generated from.
    """

    front = "front"
    back = "back"

class PhoneticEncoder(str, Enum):
    """Identifies the type of phonetic encoder to use with a PhoneticTokenFilter.
    """

    metaphone = "metaphone"
    double_metaphone = "doubleMetaphone"
    soundex = "soundex"
    refined_soundex = "refinedSoundex"
    caverphone1 = "caverphone1"
    caverphone2 = "caverphone2"
    cologne = "cologne"
    nysiis = "nysiis"
    koelner_phonetik = "koelnerPhonetik"
    haase_phonetik = "haasePhonetik"
    beider_morse = "beiderMorse"

class SnowballTokenFilterLanguage(str, Enum):
    """The language to use for a Snowball token filter.
    """

    armenian = "armenian"
    basque = "basque"
    catalan = "catalan"
    danish = "danish"
    dutch = "dutch"
    english = "english"
    finnish = "finnish"
    french = "french"
    german = "german"
    german2 = "german2"
    hungarian = "hungarian"
    italian = "italian"
    kp = "kp"
    lovins = "lovins"
    norwegian = "norwegian"
    porter = "porter"
    portuguese = "portuguese"
    romanian = "romanian"
    russian = "russian"
    spanish = "spanish"
    swedish = "swedish"
    turkish = "turkish"

class StemmerTokenFilterLanguage(str, Enum):
    """The language to use for a stemmer token filter.
    """

    arabic = "arabic"
    armenian = "armenian"
    basque = "basque"
    brazilian = "brazilian"
    bulgarian = "bulgarian"
    catalan = "catalan"
    czech = "czech"
    danish = "danish"
    dutch = "dutch"
    dutch_kp = "dutchKp"
    english = "english"
    light_english = "lightEnglish"
    minimal_english = "minimalEnglish"
    possessive_english = "possessiveEnglish"
    porter2 = "porter2"
    lovins = "lovins"
    finnish = "finnish"
    light_finnish = "lightFinnish"
    french = "french"
    light_french = "lightFrench"
    minimal_french = "minimalFrench"
    galician = "galician"
    minimal_galician = "minimalGalician"
    german = "german"
    german2 = "german2"
    light_german = "lightGerman"
    minimal_german = "minimalGerman"
    greek = "greek"
    hindi = "hindi"
    hungarian = "hungarian"
    light_hungarian = "lightHungarian"
    indonesian = "indonesian"
    irish = "irish"
    italian = "italian"
    light_italian = "lightItalian"
    sorani = "sorani"
    latvian = "latvian"
    norwegian = "norwegian"
    light_norwegian = "lightNorwegian"
    minimal_norwegian = "minimalNorwegian"
    light_nynorsk = "lightNynorsk"
    minimal_nynorsk = "minimalNynorsk"
    portuguese = "portuguese"
    light_portuguese = "lightPortuguese"
    minimal_portuguese = "minimalPortuguese"
    portuguese_rslp = "portugueseRslp"
    romanian = "romanian"
    russian = "russian"
    light_russian = "lightRussian"
    spanish = "spanish"
    light_spanish = "lightSpanish"
    swedish = "swedish"
    light_swedish = "lightSwedish"
    turkish = "turkish"

class StopwordsList(str, Enum):
    """Identifies a predefined list of language-specific stopwords.
    """

    arabic = "arabic"
    armenian = "armenian"
    basque = "basque"
    brazilian = "brazilian"
    bulgarian = "bulgarian"
    catalan = "catalan"
    czech = "czech"
    danish = "danish"
    dutch = "dutch"
    english = "english"
    finnish = "finnish"
    french = "french"
    galician = "galician"
    german = "german"
    greek = "greek"
    hindi = "hindi"
    hungarian = "hungarian"
    indonesian = "indonesian"
    irish = "irish"
    italian = "italian"
    latvian = "latvian"
    norwegian = "norwegian"
    persian = "persian"
    portuguese = "portuguese"
    romanian = "romanian"
    russian = "russian"
    sorani = "sorani"
    spanish = "spanish"
    swedish = "swedish"
    thai = "thai"
    turkish = "turkish"

class TextExtractionAlgorithm(str, Enum):
    """A value indicating which algorithm to use. Default is printed.
    """

    printed = "printed"
    handwritten = "handwritten"

class TextSplitMode(str, Enum):
    """A value indicating which split mode to perform.
    """

    pages = "pages"
    sentences = "sentences"
