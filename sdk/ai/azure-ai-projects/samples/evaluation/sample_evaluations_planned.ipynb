{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Evaluations\n",
    "\n",
    "This notebook demonstrates how to use methods to create, get, and list cloud evaluations using the Azure AI Project SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have the following:\n",
    "- Python 3.8 or later.\n",
    "- The Azure AI Project SDK installed. You can install it using the following command:\n",
    "  ```bash\n",
    "  pip install azure-ai-projects azure-identity\n",
    "  ```\n",
    "- Set the following environment variables with your own values:\n",
    "  - `PROJECT_ENDPOINT`: The Azure AI Project endpoint, as found in the overview page of your Azure AI Foundry project.\n",
    "  - `DATASET_NAME`: The name of the dataset to create and use in this sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import (\n",
    "    Evaluation,\n",
    "    InputDataset,\n",
    "    EvaluatorConfiguration,\n",
    "    EvaluatorIds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Environment Variables\n",
    "\n",
    "Ensure the following environment variables are set before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "endpoint = os.environ[\"PROJECT_ENDPOINT\"]  # Example: https://<account_name>.services.ai.azure.com/api/projects/<project_name>\n",
    "model_endpoint = os.environ[\"MODEL_ENDPOINT\"]  # Example: https://<account_name>.services.ai.azure.com\n",
    "model_api_key = os.environ[\"MODEL_API_KEY\"]\n",
    "model_deployment_name = os.environ[\"MODEL_DEPLOYMENT_NAME\"]  # Example: gpt-4o-mini\n",
    "dataset_name = os.environ[\"DATASET_NAME\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticate and Initialize the Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate and initialize the client\n",
    "credential = DefaultAzureCredential(exclude_interactive_browser_credential=False)\n",
    "project_client = AIProjectClient(endpoint=endpoint, credential=credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create evaluation with dataset (batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload a Dataset\n",
    "\n",
    "Upload a single file and create a new dataset to reference the file. Here, we explicitly specify the dataset version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Uploading dataset...\")\n",
    "dataset = project_client.datasets.upload_file(\n",
    "    name=dataset_name,\n",
    "    version=\"1\",\n",
    "    file=\"./samples_folder/sample_data_evaluation.jsonl\",\n",
    ")\n",
    "print(\"Dataset uploaded:\", dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Evaluation(basic)\n",
    "\n",
    "Create an evaluation with default options(project, data mapping etc.) and built-in evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define deployment configuration for all evaluators.\n",
    "common_init_params = {\n",
    "    \"deployment_name\": model_deployment_name,\n",
    "}\n",
    "\n",
    "# Create an evaluation\n",
    "evaluation = Evaluation(\n",
    "    display_name=\"Sample Cloud Evaluation\",\n",
    "    description=\"Sample cloud evaluation with built-in evaluators\",\n",
    "    data=InputDataset(id=\"<>\"),    \n",
    "    init_params=common_init_params,\n",
    "    evaluators={\n",
    "        \"relevance\": EvaluatorConfiguration(\n",
    "            id=EvaluatorIds.RELEVANCE.value,\n",
    "        ),\n",
    "        \"violence\": EvaluatorConfiguration(\n",
    "            id=EvaluatorIds.VIOLENCE.value,\n",
    "        ),\n",
    "        \"bleu_score\": EvaluatorConfiguration(\n",
    "            id=EvaluatorIds.BLEU_SCORE.value,\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "\n",
    "evaluation_response = project_client.evaluations.create(evaluation)\n",
    "\n",
    "print(\"Evaluation created:\", evaluation_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Evaluation(advanced)\n",
    "\n",
    "Create an evaluation with custom parameters and custom evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define common data mapping and initialization parameters\n",
    "common_data_mapping = {\n",
    "    \"query\": \"${data.query}\",\n",
    "    \"response\": \"${data.response}\",\n",
    "}\n",
    "\n",
    "common_init_params = {\n",
    "    \"deployment_name\": model_deployment_name,\n",
    "}\n",
    "\n",
    "# Create and register a custom evaluator. Details of how to define a custom evaluator can be found at https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-evaluators/custom-evaluators\n",
    "custom_evaluator = Evaluator(\n",
    "    id =\"custom://friendliness\",\n",
    "    path=\"<local_folder_path>\",\n",
    "    name=\"Custom friendliness evaluator\",\n",
    "    description=\"prompt-based evaluator measuring response friendliness.\",\n",
    ")\n",
    "registered_evaluator = project_client.evaluators.create_or_update(custom_evaluator)\n",
    "print(\"Registered evaluator id:\", registered_evaluator.id);\n",
    "\n",
    "data = InputDataset(id=\"<>\")\n",
    "\n",
    "# Create an evaluation\n",
    "evaluation = Evaluation(\n",
    "    display_name=\"Sample Evaluation Test\",\n",
    "    description=\"Sample evaluation for testing\",\n",
    "    data=data,\n",
    "    data_mapping=common_data_mapping,\n",
    "    init_params=common_init_params,\n",
    "    evaluators={\n",
    "        \"relevance\": EvaluatorConfiguration(\n",
    "            id=EvaluatorIds.RELEVANCE.value,\n",
    "            data_mapping={\n",
    "                        \"query\": \"${data.query}\",\n",
    "                        \"response\": \"${data.response}\",\n",
    "                        \"context\": \"${data.context}\",\n",
    "                        },\n",
    "        ),\n",
    "        \"violence\": EvaluatorConfiguration(\n",
    "            id=EvaluatorIds.VIOLENCE.value,\n",
    "            \n",
    "        ),\n",
    "        \"bleu_score\": EvaluatorConfiguration(\n",
    "            id=EvaluatorIds.BLEU_SCORE.value,\n",
    "        ),\n",
    "        \"friendliness\": EvaluatorConfiguration(\n",
    "            id=registered_evaluator.id,\n",
    "        ),\n",
    ")\n",
    "\n",
    "evaluation_response = project_client.evaluations.create(evaluation)\n",
    "\n",
    "print(\"Evaluation created:\", evaluation_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create evaluation with raw data(single run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation input data option 1: non-agentic evaluation with raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {\n",
    "    \"query\": \"sample query\",\n",
    "    \"response\": \"sample response\",\n",
    "    \"context\": \"sample context\",\n",
    "    \"groundtruth\": \"sample groundtruth\",\n",
    "}\n",
    "data = InputRawData(raw_data=raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation input data option 2: agent evaluation with raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly and helpful customer service agent.\"\n",
    "    }, {\n",
    "        \"createdAt\": \"2025-03-14T06:14:20Z\",\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"Hi, I need help with the last 2 orders on my account #888. Could you please update me on their status?\"\n",
    "        }]\n",
    "    }\n",
    "]\n",
    "\n",
    "response = [{\n",
    "    \"createdAt\": \"2025-03-14T06:14:35Z\",\n",
    "    \"run_id\": \"0\",\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": [{\n",
    "        \"type\": \"tool_call\",\n",
    "        \"tool_call_id\": \"tool_call_20250310_001\",\n",
    "        \"name\": \"get_orders\",\n",
    "        \"arguments\": {\n",
    "            \"account_number\": \"888\"\n",
    "        }\n",
    "    }]\n",
    "}, {\n",
    "    \"createdAt\": \"2025-03-14T06:15:05Z\",\n",
    "    \"run_id\": \"0\",\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": [{\n",
    "        \"type\": \"text\",\n",
    "        \"text\": \"<orders-summary-for-the-account>\"\n",
    "    }]\n",
    "}]\n",
    "\n",
    "tool_definitions = [{\n",
    "    \"name\": \"get_orders\",\n",
    "    \"description\": \"Get the list of orders for a given account number.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"account_number\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The account number to get the orders for.\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "data = InputRawData(query=query, response=response, tool_definitions=tool_definitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation input data option 3: Semantic Kernel evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"my sample query\"\n",
    "response = \"my sample response\"\n",
    "\n",
    "search_wikipedia = KernelFunctionFromPrompt(\n",
    "    function_name=\"search_wikipedia\",\n",
    "    prompt=f\"\"\"Sample prompt\"\"\",\n",
    ")\n",
    "\n",
    "query = AIAgentConverter.convert_query(query)\n",
    "response = AIAgentConverter.convert_response(response)\n",
    "tool_definitions = AIAgentConverter.convert_semantickernel_tools([search_wikipedia])\n",
    "\n",
    "data = InputRawData(query=query, response=response, tool_definitions=tool_definitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation input data option 4: LangChain evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "query = \"my sample query\"\n",
    "response = \"my sample response\"\n",
    "\n",
    "@tool\n",
    "def search_wikipedia(term: str) -> str:\n",
    "    \"\"\"Search wikipedia for the term provided\"\"\"\n",
    "    pass\n",
    "\n",
    "query = AIAgentConverter.convert_query(query)\n",
    "response = AIAgentConverter.convert_response(response)\n",
    "tool_definitions = AIAgentConverter.convert_langchain_tools([search_wikipedia])\n",
    "\n",
    "data = InputRawData(query=query, response=response, tool_definitions=tool_definitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an evaluation\n",
    "evaluation = Evaluation(\n",
    "    display_name=\"Sample Evaluation Test\",\n",
    "    description=\"Sample evaluation for testing\",\n",
    "    data=data,\n",
    "    init_params={\n",
    "        \"deployment_name\": model_deployment_name,\n",
    "    },\n",
    "    evaluators={\n",
    "        \"relevance\": EvaluatorConfiguration(\n",
    "            id=EvaluatorIds.RELEVANCE.value,\n",
    "        ),\n",
    "        \"violence\": EvaluatorConfiguration(\n",
    "            id=EvaluatorIds.VIOLENCE.value,\n",
    "        ),\n",
    "        \"bleu_score\": EvaluatorConfiguration(\n",
    "            id=EvaluatorIds.BLEU_SCORE.value,\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "\n",
    "evaluation_response = project_client.evaluations.create(\n",
    "    evaluation,\n",
    "    headers={\n",
    "        \"model-endpoint\": model_endpoint,\n",
    "        \"api-key\": model_api_key,\n",
    "    },\n",
    ")\n",
    "print(\"Evaluation created:\", evaluation_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Evaluations\n",
    "\n",
    "List all evaluations in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all evaluations\n",
    "# Sample URL: https://[resource-name].services.ai.azure.com/api/projects/[project-name]/evaluations?api-version=2025-05-15-preview\n",
    "print(\"Listing all evaluations...\")\n",
    "for evaluation in project_client.evaluations.list():\n",
    "    print(evaluation)\n",
    "\n",
    "# Find evaluations for an agent run\n",
    "# Sample URL: https://[resource-name].services.ai.azure.com/api/projects/[project-name]/evaluations?agent_run_id=[my-agent-run-id]&api-version=2025-05-15-preview\n",
    "print(\"Find evaluations with agent run Id...\")\n",
    "for evaluation in project_client.evaluations.list(agent_run_id=agent_run_id):\n",
    "    # A single agent run may have multiple evaluation runs (e.g. re-run evaluation)\n",
    "    print(evaluation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get an Evaluation\n",
    "\n",
    "Retrieve the metadata of a created evaluation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get evaluation details\n",
    "print(\"Getting evaluation details...\")\n",
    "get_evaluation_response = project_client.evaluations.get(evaluation_response.name)\n",
    "print(\"Evaluation details:\", get_evaluation_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Evaluation results for raw data input\n",
    "\n",
    "Retrieve the evaluation results from an evaluation run. The response for raw data input will be raw data, based on the assumption: only the users wants to evaluation big data will use dataset, which results in big data as output in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get evaluation results\n",
    "print(\"Getting evaluation results...\")\n",
    "get_evaluation_results = project_client.evaluations.get_results(evaluation_response.name)\n",
    "print(\"Evaluation results:\", get_evaluation_results)\n",
    "\n",
    "# Sample output\n",
    "# {\n",
    "#     \"id\": \"evaluation_run_id\",\n",
    "#     ...\n",
    "#     \"type\": \"raw\",\n",
    "#     \"result\": [{\n",
    "#         \"evaluator\": \"Intent Resolution Evaluator\",\n",
    "#         \"evaluatorId\": \"azureai://built-in/evaluators/intent_resolution\",\n",
    "#         \"score\": 5.0,\n",
    "#         \"status\": \"Completed\",\n",
    "#         \"reason\": \"The agent's response directly addresses the user's request for a joke by providing a humorous punchline. The joke is relevant and fulfills the user's intent to hear a joke.\",\n",
    "#         \"version\": \"1\",\n",
    "#         \"responseId\": \"thread_dtykPFfWERcAvifwrkuu67ki;run_yR09Fby0Neqo7Ip0evhYRuuP\",\n",
    "#         \"messageId\": \"d14d3b1d-4f3d-4b5f-9a86-ee56928049a9\",\n",
    "#         \"threadId\": \"thread_dtykPFfWERcAvifwrkuu67ki\",\n",
    "#         \"runId\": \"run_yR09Fby0Neqo7Ip0evhYRuuP\",\n",
    "#         \"error\": null,\n",
    "#         \"additionalDetails\": null\n",
    "#   }],\n",
    "#   \"error\": null\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Evaluation results for dataset input\n",
    "\n",
    "Retrieve the evaluation results from an evaluation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get evaluation results\n",
    "print(\"Getting evaluation results...\")\n",
    "get_evaluation_results = project_client.evaluations.get_results(evaluation_response.name)\n",
    "print(\"Evaluation results:\", get_evaluation_results)\n",
    "# Sample output\n",
    "# {\n",
    "#     \"id\": \"evaluation_run_id\",\n",
    "#     ...\n",
    "#     \"type\": \"blob\",\n",
    "#     \"result\": {\n",
    "#         \"dataset_name\": \"sample_dataset_name\",\n",
    "#         \"dataset_version\": \"sample_dataset_version\",\n",
    "#     }\n",
    "# }\n",
    "\n",
    "asset_credential = project_client.datasets.get_credentials(name=get_evaluation_results.result.dataset_name, version=get_evaluation_results.result.dataset_version)\n",
    "print(asset_credential)\n",
    "# Sample output\n",
    "# {\n",
    "#     \"blob_reference\": {\n",
    "#         \"blob_uri\": \"https://myaccount.blob.core.windows.net/mycontainer/mypath1/mypath2/myblob\",\n",
    "#         ...\n",
    "#         \"credential\": {\n",
    "#             \"sas_uri\": \"https://mystorageaccount.blob.core.windows.net/mycontainer/myblob?sv=2019-12-12&ss=bjqt&srt=sco&sp=rwdlacupx&se=2020-10-01T05:00:00Z&st=2020-09-30T17:00:00Z&spr=https&sig=my-sig\",\n",
    "#             \"type\": \"SAS\"\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# Download file\n",
    "with urlopen(asset_credential.blob_reference.credential.sas_uri) as response:\n",
    "    content = response.read().decode('utf-8')\n",
    "    print(\"Evaluation results: \", content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
