{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87f20efe",
   "metadata": {},
   "source": [
    "# Python notebook to show issues with Typing\n",
    "\n",
    "## General Links\n",
    "\n",
    "* OpenAI TypeSpec package: https://www.npmjs.com/package/@azure-tools/openai-typespec\n",
    "* OpenAI TypeSpec repo: https://github.com/microsoft/openai-openapi-pr/tree/main/packages/openai-typespec/src\n",
    "* Our current OpenAI TypeSpec files: https://github.com/Azure/azure-rest-api-specs-pr/tree/feature/ai-foundry/agents-v2/specification/ai/Azure.AI.Projects/.external-readonly/openai.external.typespec\n",
    "* OpenAI Python reference: https://platform.openai.com/docs/api-reference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda52a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e45759b",
   "metadata": {},
   "source": [
    "\n",
    "## Issues in Request payload\n",
    "\n",
    "### 'model' in ImageGenTool\n",
    "\n",
    "OpenAI defines model as `Literal['gpt-image-1']` in our current TypeSpec, and `\"gpt-image-1\" | \"gpt-image-1-mini\"` in the [published OpenAI TypeSpec package](https://github.com/microsoft/openai-openapi-pr/blob/main/packages/openai-typespec/src/responses/models.tsp#L1742). But we try to pass in any str (with potentially different model name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bb1a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import cast\n",
    "from azure.ai.projects.models import ImageGenTool\n",
    "\n",
    "image_generation_model = cast(str, os.getenv(\"IMAGE_GENERATION_MODEL_DEPLOYMENT_NAME\"))\n",
    "\n",
    "# OpenAI defines model as `Literal['gpt-image-1']` but we try to pass in any str (with potentially different mode name)\n",
    "tool = ImageGenTool(model=image_generation_model, quality=\"low\", size=\"1024x1024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b14a2a",
   "metadata": {},
   "source": [
    "Error:\n",
    "```\n",
    "Argument of type \"str\" cannot be assigned to parameter \"model\" of type \"Literal['gpt-image-1'] | None\" in function \"__init__\"\n",
    "  Type \"str\" is not assignable to type \"Literal['gpt-image-1'] | None\"\n",
    "    \"str\" is not assignable to \"None\"\n",
    "    \"str\" is not assignable to type \"Literal['gpt-image-1']\"Pylance(reportArgumentType)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e5fdaa",
   "metadata": {},
   "source": [
    "### '.evals.create' Azure extensions\n",
    "\n",
    "Example taken from sample: samples\\evaluations\\sample_continuous_evaluation_rule.py\n",
    "Link to OpenAI Python reference code for .evals.create(): https://platform.openai.com/docs/api-reference/evals/create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241964c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "\n",
    "endpoint = os.environ[\"AZURE_AI_PROJECT_ENDPOINT\"]\n",
    "\n",
    "with (\n",
    "    DefaultAzureCredential() as credential,\n",
    "    AIProjectClient(endpoint=endpoint, credential=credential) as project_client,\n",
    "    project_client.get_openai_client() as openai_client,\n",
    "):\n",
    "\n",
    "    data_source_config = {\"type\": \"azure_ai_source\", \"scenario\": \"responses\"}\n",
    "    testing_criteria = [\n",
    "        {\"type\": \"azure_ai_evaluator\", \"name\": \"violence_detection\", \"evaluator_name\": \"builtin.violence\"}\n",
    "    ]\n",
    "    eval_object = openai_client.evals.create(\n",
    "        name=\"Continuous Evaluation\",\n",
    "        data_source_config=data_source_config,\n",
    "        testing_criteria=testing_criteria,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1f8af3",
   "metadata": {},
   "source": [
    "Regarding data_source_config:\n",
    "\n",
    "Error:\n",
    "```\n",
    "Argument of type \"dict[str, str]\" cannot be assigned to parameter \"data_source_config\" of type \"DataSourceConfig\" in function \"create\"\n",
    "  Type \"dict[str, str]\" is not assignable to type \"DataSourceConfig\"\n",
    "    \"dict[str, str]\" is not assignable to \"DataSourceConfigCustom\"\n",
    "    \"dict[str, str]\" is not assignable to \"DataSourceConfigLogs\"\n",
    "    \"dict[str, str]\" is not assignable to \"DataSourceConfigStoredCompletions\"Pylance(reportArgumentType)\n",
    "```\n",
    "\n",
    "Analysis:\n",
    "Per OpenAI ref doc, for data_source_config, there is not support for {\"type\": \"something\", \"scenario\": \"something\"}. This is an Azure extension. In out TypeSpec we have:\n",
    "\n",
    "```JavaScript\n",
    "model AzureAIDataSourceConfig extends DataSourceConfig {\n",
    "  /** The object type, which is always `azure_ai_source`. */\n",
    "  type: \"azure_ai_source\";\n",
    "\n",
    "  /** Data schema scenario. */\n",
    "  #suppress \"@azure-tools/typespec-azure-core/no-unnamed-union\" \"Auto-suppressed warnings non-applicable rules during import.\"\n",
    "  @added(Versions.v2025_11_15_preview)\n",
    "  scenario: \"red_team\" | \"responses\" | \"traces\";\n",
    "}\n",
    "```\n",
    "\n",
    "Regarding testing_criteria: \n",
    "\n",
    "Error:\n",
    "```\n",
    "Argument of type \"list[dict[str, str]]\" cannot be assigned to parameter \"testing_criteria\" of type \"Iterable[TestingCriterion]\" in function \"create\"\n",
    "  \"list[dict[str, str]]\" is not assignable to \"Iterable[TestingCriterion]\"\n",
    "    Type parameter \"_T_co@Iterable\" is covariant, but \"dict[str, str]\" is not a subtype of \"TestingCriterion\"\n",
    "      Type \"dict[str, str]\" is not assignable to type \"TestingCriterion\"\n",
    "        \"dict[str, str]\" is not assignable to \"TestingCriterionLabelModel\"\n",
    "        \"dict[str, str]\" is not assignable to \"StringCheckGraderParam\"\n",
    "        \"dict[str, str]\" is not assignable to \"TestingCriterionTextSimilarity\"\n",
    "        \"dict[str, str]\" is not assignable to \"TestingCriterionPython\"\n",
    "        \"dict[str, str]\" is not assignable to \"TestingCriterionScoreModel\"\n",
    "```\n",
    "\n",
    "Analysis: \n",
    "\n",
    "We extended OpenAI definition by adding the last one below:\n",
    "\n",
    "```javascript\n",
    "@@changePropertyType(OpenAI.Eval.testing_criteria,\n",
    "  (\n",
    "    | OpenAI.EvalGraderLabelModel\n",
    "    | OpenAI.EvalGraderStringCheck\n",
    "    | OpenAI.EvalGraderTextSimilarity\n",
    "    | OpenAI.EvalGraderPython\n",
    "    | OpenAI.EvalGraderScoreModel\n",
    "    | EvalGraderAzureAIEvaluator)[]\n",
    ");\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b63d65",
   "metadata": {},
   "source": [
    "### evals.run.create\n",
    "\n",
    "samples\\evaluations\\sample_agent_evaluation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from typing import Union\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import PromptAgentDefinition\n",
    "from openai.types.eval_create_params import DataSourceConfigCustom\n",
    "from openai.types.evals.run_create_response import RunCreateResponse\n",
    "from openai.types.evals.run_retrieve_response import RunRetrieveResponse\n",
    "\n",
    "load_dotenv()\n",
    "endpoint = os.environ[\"AZURE_AI_PROJECT_ENDPOINT\"]\n",
    "\n",
    "# [START agent_evaluation_basic]\n",
    "with (\n",
    "    DefaultAzureCredential() as credential,\n",
    "    AIProjectClient(endpoint=endpoint, credential=credential) as project_client,\n",
    "    project_client.get_openai_client() as openai_client,\n",
    "):\n",
    "    agent = project_client.agents.create_version(\n",
    "        agent_name=os.environ[\"AZURE_AI_AGENT_NAME\"],\n",
    "        definition=PromptAgentDefinition(\n",
    "            model=os.environ[\"AZURE_AI_MODEL_DEPLOYMENT_NAME\"],\n",
    "            instructions=\"You are a helpful assistant that answers general questions\",\n",
    "        ),\n",
    "    )\n",
    "    print(f\"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})\")\n",
    "\n",
    "    data_source_config = DataSourceConfigCustom(\n",
    "        type=\"custom\",\n",
    "        item_schema={\"type\": \"object\", \"properties\": {\"query\": {\"type\": \"string\"}}, \"required\": [\"query\"]},\n",
    "        include_sample_schema=True,\n",
    "    )\n",
    "    testing_criteria = [\n",
    "        {\n",
    "            \"type\": \"azure_ai_evaluator\",\n",
    "            \"name\": \"violence_detection\",\n",
    "            \"evaluator_name\": \"builtin.violence\",\n",
    "            \"data_mapping\": {\"query\": \"{{item.query}}\", \"response\": \"{{item.response}}\"},\n",
    "        }\n",
    "    ]\n",
    "    eval_object = openai_client.evals.create(\n",
    "        name=\"Agent Evaluation\",\n",
    "        data_source_config=data_source_config,\n",
    "        testing_criteria=testing_criteria,  # type: ignore\n",
    "    )\n",
    "    print(f\"Evaluation created (id: {eval_object.id}, name: {eval_object.name})\")\n",
    "\n",
    "    data_source = {\n",
    "        \"type\": \"azure_ai_target_completions\",\n",
    "        \"source\": {\n",
    "            \"type\": \"file_content\",\n",
    "            \"content\": [\n",
    "                {\"item\": {\"query\": \"What is the capital of France?\"}},\n",
    "                {\"item\": {\"query\": \"How do I reverse a string in Python?\"}},\n",
    "            ],\n",
    "        },\n",
    "        \"input_messages\": {\n",
    "            \"type\": \"template\",\n",
    "            \"template\": [\n",
    "                {\"type\": \"message\", \"role\": \"user\", \"content\": {\"type\": \"input_text\", \"text\": \"{{item.query}}\"}}\n",
    "            ],\n",
    "        },\n",
    "        \"target\": {\n",
    "            \"type\": \"azure_ai_agent\",\n",
    "            \"name\": agent.name,\n",
    "            \"version\": agent.version,  # Version is optional. Defaults to latest version if not specified\n",
    "        },\n",
    "    }\n",
    "\n",
    "    agent_eval_run: Union[RunCreateResponse, RunRetrieveResponse] = openai_client.evals.runs.create(\n",
    "        eval_id=eval_object.id,\n",
    "        name=f\"Evaluation Run for Agent {agent.name}\",\n",
    "        data_source=data_source\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7269a1a5",
   "metadata": {},
   "source": [
    "Error:\n",
    "```\n",
    "Argument of type \"dict[str, Unknown]\" cannot be assigned to parameter \"data_source\" of type \"DataSource\" in function \"create\"\n",
    "  Type \"dict[str, Unknown]\" is not assignable to type \"DataSource\"\n",
    "    \"dict[str, Unknown]\" is not assignable to \"CreateEvalJSONLRunDataSourceParam\"\n",
    "    \"dict[str, Unknown]\" is not assignable to \"CreateEvalCompletionsRunDataSourceParam\"\n",
    "    \"dict[str, Unknown]\" is not assignable to \"DataSourceCreateEvalResponsesRunDataSource\"\n",
    "```\n",
    "\n",
    "data_source \"type\" \"azure_ai_target_completions\" is an Azure extension type. In our TypeSpec we have:\n",
    "\n",
    "```javascript\n",
    "@@changePropertyType(OpenAI.CreateEvalRunRequest.data_source,\n",
    "\n",
    "    | OpenAI.CreateEvalJsonlRunDataSource\n",
    "    | OpenAI.CreateEvalCompletionsRunDataSource\n",
    "    | OpenAI.CreateEvalResponsesRunDataSource\n",
    "    | EvalRunDataSource /* RedTeamEvalRunDataSource | TargetCompletionsEvalRunDataSource | AzureAIResponsesEvalRunDataSource | TracesEvalRunDataSource */\n",
    ");\n",
    "\n",
    "/** Base class for run data sources with discriminator support. */\n",
    "@discriminator(\"type\")\n",
    "model EvalRunDataSource {\n",
    "  /** The data source type discriminator. */\n",
    "  type: string;\n",
    "}\n",
    "\n",
    "model TargetCompletionsEvalRunDataSource extends EvalRunDataSource {\n",
    "  /** The type of data source, always `azure_ai_target_completions`. */\n",
    "  type: \"azure_ai_target_completions\";\n",
    "\n",
    "  /** Input messages configuration. */\n",
    "  input_messages?: OpenAI.CreateEvalCompletionsRunDataSourceInputMessagesItemReference;\n",
    "\n",
    "  /** The source configuration for inline or file data. */\n",
    "  #suppress \"@azure-tools/typespec-autorest/union-unsupported\" \"This union is defined according to the Azure OpenAI API.\"\n",
    "  #suppress \"@azure-tools/typespec-azure-core/no-unnamed-union\" \"Auto-suppressed warnings non-applicable rules during import.\"\n",
    "  source: OpenAI.EvalJsonlFileContentSource | OpenAI.EvalJsonlFileIdSource;\n",
    "\n",
    "  /** The target configuration for the evaluation. */\n",
    "  target: Target;\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa01042",
   "metadata": {},
   "source": [
    "### Fine Tuning\n",
    "\n",
    "From sample: samples\\finetuning\\sample_finetuning_reinforcement_job.py\n",
    "Link to OpenAI Python reference: https://platform.openai.com/docs/api-reference/fine-tuning/create\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ddf35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Dict\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "endpoint = os.environ[\"AZURE_AI_PROJECT_ENDPOINT\"]\n",
    "model_name = os.environ.get(\"MODEL_NAME\", \"o4-mini\")\n",
    "script_dir = Path(__file__).parent\n",
    "training_file_path = os.environ.get(\"TRAINING_FILE_PATH\", os.path.join(script_dir, \"data\", \"rft_training_set.jsonl\"))\n",
    "validation_file_path = os.environ.get(\n",
    "    \"VALIDATION_FILE_PATH\", os.path.join(script_dir, \"data\", \"rft_validation_set.jsonl\")\n",
    ")\n",
    "\n",
    "with (\n",
    "    DefaultAzureCredential() as credential,\n",
    "    AIProjectClient(endpoint=endpoint, credential=credential) as project_client,\n",
    "    project_client.get_openai_client() as openai_client,\n",
    "):\n",
    "\n",
    "    print(\"Uploading training file...\")\n",
    "    with open(training_file_path, \"rb\") as f:\n",
    "        train_file = openai_client.files.create(file=f, purpose=\"fine-tune\")\n",
    "    print(f\"Uploaded training file with ID: {train_file.id}\")\n",
    "\n",
    "    print(\"Uploading validation file...\")\n",
    "    with open(validation_file_path, \"rb\") as f:\n",
    "        validation_file = openai_client.files.create(file=f, purpose=\"fine-tune\")\n",
    "    print(f\"Uploaded validation file with ID: {validation_file.id}\")\n",
    "\n",
    "    print(\"Waits for the training and validation files to be processed...\")\n",
    "    openai_client.files.wait_for_processing(train_file.id)\n",
    "    openai_client.files.wait_for_processing(validation_file.id)\n",
    "\n",
    "    grader: Dict[str, Any] = {\n",
    "        \"name\": \"Response Quality Grader\",\n",
    "        \"type\": \"score_model\",\n",
    "        \"model\": \"o3-mini\",\n",
    "        \"input\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Evaluate the model's response based on correctness and quality. Rate from 0 to 10.\",\n",
    "            }\n",
    "        ],\n",
    "        \"range\": [0.0, 10.0],\n",
    "    }\n",
    "\n",
    "    print(\"Creating reinforcement fine-tuning job\")\n",
    "    fine_tuning_job = openai_client.fine_tuning.jobs.create(\n",
    "        training_file=train_file.id,\n",
    "        validation_file=validation_file.id,\n",
    "        model=model_name,\n",
    "        method={\n",
    "            \"type\": \"reinforcement\",\n",
    "            \"reinforcement\": {\n",
    "                \"grader\": grader,\n",
    "                \"hyperparameters\": {\n",
    "                    \"n_epochs\": 1,\n",
    "                    \"batch_size\": 4,\n",
    "                    \"learning_rate_multiplier\": 2,\n",
    "                    \"eval_interval\": 5,\n",
    "                    \"eval_samples\": 2,\n",
    "                    \"reasoning_effort\": \"medium\",\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        extra_body={\n",
    "            \"trainingType\": \"Standard\"\n",
    "        },  # Recommended approach to set trainingType. Omitting this field may lead to unsupported behavior.\n",
    "    )\n",
    "    print(fine_tuning_job)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bb6dd4",
   "metadata": {},
   "source": [
    "Error:\n",
    "```\n",
    "Argument of type \"dict[str, str | dict[str, Dict[str, Any] | dict[str, int | str]]]\" cannot be assigned to parameter \"method\" of type \"Method | Omit\" in function \"create\"\n",
    "  Type \"dict[str, str | dict[str, Dict[str, Any] | dict[str, int | str]]]\" is not assignable to type \"Method | Omit\"\n",
    "    \"dict[str, str | dict[str, Dict[str, Any] | dict[str, int | str]]]\" is not assignable to \"Method\"\n",
    "    \"dict[str, str | dict[str, Dict[str, Any] | dict[str, int | str]]]\" is not assignable to \"Omit\"\n",
    "```\n",
    "\n",
    "\n",
    "Analysis:\n",
    "Why do we get squiggly lines for \"method=\"? We are not using any Azure extensions here...\n",
    "method \"type\" and \"reinforcement\" are valid fields (\"reinforcement\" is a recognized value).\n",
    "reinforcement.grader seems valid, based on OpenAI class `ScoreModelGraderParam`.\n",
    "reinforcement.hyperparameters seems valid, based on OpenAI class `ReinforcementHyperparametersParam`.\n",
    "\n",
    "CoPilot said \"Generic dictionaries are not compatible with TypedDict types in static type checking, even if the structure matches perfectly at runtime\"... So what's the point in using TypedDict?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3501b164",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
