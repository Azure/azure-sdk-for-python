#!/usr/bin/env python3
"""
Example: Analyzing Merged Pytest Report

This script demonstrates how to parse and analyze the merged pytest report
generated by the multi-project test orchestrator.
"""

import json
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
from collections import defaultdict


def load_merged_report(report_path: str) -> Dict[str, Any]:
    """Load the merged pytest report JSON file."""
    with open(report_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def print_table(headers: List[str], rows: List[List[str]], col_widths: Optional[List[int]] = None) -> None:
    """Print a formatted table with headers and rows."""
    if not col_widths:
        # Auto-calculate column widths
        col_widths = [len(h) for h in headers]
        for row in rows:
            for i, cell in enumerate(row):
                col_widths[i] = max(col_widths[i], len(str(cell)))
    
    # Add padding
    col_widths = [w + 2 for w in col_widths]
    
    # Print header
    header_line = "‚îÇ".join(f" {h:<{col_widths[i]-1}}" for i, h in enumerate(headers))
    separator = "‚îÄ" * (sum(col_widths) + len(headers) - 1)
    
    print(separator)
    print(header_line)
    print(separator)
    
    # Print rows
    for row in rows:
        row_line = "‚îÇ".join(f" {str(cell):<{col_widths[i]-1}}" for i, cell in enumerate(row))
        print(row_line)
    
    print(separator)


def analyze_report(report: Dict[str, Any]) -> None:
    """Analyze and print statistics from the merged report in table format."""
    
    print("\n" + "=" * 100)
    print("MERGED PYTEST REPORT ANALYSIS")
    print("=" * 100 + "\n")
    
    # Overall summary
    summary = report['summary']
    print("üìä OVERALL SUMMARY")
    print(f"   Total Tests: {summary['total_tests']} | "
          f"‚úÖ Passed: {summary['passed']} | "
          f"‚ùå Failed: {summary['failed']} | "
          f"‚è≠Ô∏è  Skipped: {summary['skipped']} | "
          f"‚ö†Ô∏è  Errors: {summary['error']} | "
          f"‚è±Ô∏è  Duration: {summary['total_duration']:.2f}s\n")
    
    # Environment Comparison Table
    print("üåç ENVIRONMENT COMPARISON")
    env_names = sorted(report['environments'].keys())
    
    headers = ["Metric"] + env_names
    rows = []
    
    # Tests count
    test_counts = ["Tests"] + [str(report['environments'][env]['test_count']) for env in env_names]
    rows.append(test_counts)
    
    # Passed
    passed_counts = ["‚úÖ Passed"] + [str(report['environments'][env]['summary'].get('passed', 0)) for env in env_names]
    rows.append(passed_counts)
    
    # Failed
    failed_counts = ["‚ùå Failed"] + [str(report['environments'][env]['summary'].get('failed', 0)) for env in env_names]
    rows.append(failed_counts)
    
    # Skipped
    skipped_counts = ["‚è≠Ô∏è  Skipped"] + [str(report['environments'][env]['summary'].get('skipped', 0)) for env in env_names]
    rows.append(skipped_counts)
    
    # Errors
    error_counts = ["‚ö†Ô∏è  Errors"] + [str(report['environments'][env]['summary'].get('error', 0)) for env in env_names]
    rows.append(error_counts)
    
    # Duration
    duration_counts = ["‚è±Ô∏è  Duration"] + [f"{report['environments'][env]['summary']['duration']:.2f}s" for env in env_names]
    rows.append(duration_counts)
    
    print_table(headers, rows)
    print()


def analyze_test_outcomes_by_environment(report: Dict[str, Any]) -> None:
    """Create a matrix showing each test's outcome across environments."""
    print("\nÔøΩ TEST OUTCOMES BY ENVIRONMENT\n")
    
    # Group tests by test name
    test_groups = defaultdict(dict)
    for test in report['all_tests']:
        # Extract just the test function name for readability
        test_name = test['name'].split('::')[-1]
        full_path = test['name']
        environment = test['environment']
        outcome = test['outcome']
        
        test_groups[full_path][environment] = outcome
    
    # Get all environments
    env_names = sorted(report['environments'].keys())
    
    # Build table
    headers = ["Test"] + env_names
    rows = []
    
    for test_path in sorted(test_groups.keys()):
        test_name = test_path.split('::')[-1]
        outcomes = test_groups[test_path]
        
        # Truncate long test names
        display_name = test_name if len(test_name) <= 50 else test_name[:47] + "..."
        
        row = [display_name]
        for env in env_names:
            outcome = outcomes.get(env, "N/A")
            # Add emoji for quick visual scanning
            if outcome == "passed":
                row.append("‚úÖ PASS")
            elif outcome == "failed":
                row.append("‚ùå FAIL")
            elif outcome == "skipped":
                row.append("‚è≠Ô∏è  SKIP")
            elif outcome == "error":
                row.append("‚ö†Ô∏è  ERROR")
            else:
                row.append(outcome)
        
        rows.append(row)
    
    print_table(headers, rows)
    print()


def format_table_as_string(headers: List[str], rows: List[List[str]]) -> str:
    """Format a table as a string for indenting."""
    # Calculate column widths
    col_widths = [len(h) for h in headers]
    for row in rows:
        for i, cell in enumerate(row):
            col_widths[i] = max(col_widths[i], len(str(cell)))
    
    # Add padding
    col_widths = [w + 2 for w in col_widths]
    
    lines = []
    
    # Header
    header_line = "‚îÇ".join(f" {h:<{col_widths[i]-1}}" for i, h in enumerate(headers))
    separator = "‚îÄ" * (sum(col_widths) + len(headers) - 1)
    
    lines.append(separator)
    lines.append(header_line)
    lines.append(separator)
    
    # Rows
    for row in rows:
        row_line = "‚îÇ".join(f" {str(cell):<{col_widths[i]-1}}" for i, cell in enumerate(row))
        lines.append(row_line)
    
    lines.append(separator)
    
    return '\n'.join(lines)


def parse_environment_name(env_name: str) -> Tuple[str, str]:
    """
    Parse environment name into region and model.
    Examples: 'usw21.gpt-4o' -> ('usw21', 'gpt-4o')
              'swc1.grok-3' -> ('swc1', 'grok-3')
    """
    parts = env_name.split('.', 1)
    if len(parts) == 2:
        return parts[0], parts[1]
    return env_name, "unknown"


def find_region_differences(report: Dict[str, Any]) -> None:
    """
    Identify tests that pass/fail differently across regions (for the same model).
    This helps identify region-specific issues.
    """
    print("\nüåç REGION-SPECIFIC DIFFERENCES\n")
    print("   (Same model, different outcomes across regions)\n")
    
    # Group tests by test name and model
    test_model_groups = defaultdict(lambda: defaultdict(dict))
    for test in report['all_tests']:
        test_name = test['name']
        short_name = test_name.split('::')[-1]
        environment = test['environment']
        outcome = test['outcome']
        
        region, model = parse_environment_name(environment)
        test_model_groups[test_name][model][region] = {
            'outcome': outcome,
            'short_name': short_name,
            'environment': environment
        }
    
    # Find models that have region differences
    models_with_differences = defaultdict(list)
    for test_name, models in test_model_groups.items():
        for model, regions in models.items():
            if len(regions) > 1:
                outcomes = set(r['outcome'] for r in regions.values())
                if len(outcomes) > 1:
                    short_name = list(regions.values())[0]['short_name']
                    models_with_differences[model].append((test_name, regions, short_name))
    
    if not models_with_differences:
        print("   ‚úÖ No region-specific differences found\n")
        print("   All tests have consistent outcomes across regions for the same model.\n")
        return
    
    # Display table for each model that has differences
    for model in sorted(models_with_differences.keys()):
        test_cases = models_with_differences[model]
        print(f"   Model: {model}")
        print(f"   Found {len(test_cases)} test(s) with different outcomes across regions:\n")
        
        # Get all regions for this model
        all_regions = set()
        for test_name, regions, short_name in test_cases:
            all_regions.update(regions.keys())
        
        region_list = sorted(all_regions)
        
        # Build table headers (regions)
        headers = ["Test"] + region_list
        rows = []
        
        for test_name, regions, short_name in test_cases:
            # Truncate long test names
            display_name = short_name if len(short_name) <= 50 else short_name[:47] + "..."
            row = [display_name]
            
            for region in region_list:
                if region in regions:
                    outcome = regions[region]['outcome']
                    if outcome == "passed":
                        row.append("‚úÖ PASS")
                    elif outcome == "failed":
                        row.append("‚ùå FAIL")
                    elif outcome == "skipped":
                        row.append("‚è≠Ô∏è  SKIP")
                    elif outcome == "error":
                        row.append("‚ö†Ô∏è  ERROR")
                    else:
                        row.append(outcome)
                else:
                    row.append("N/A")
            
            rows.append(row)
        
        print_table(headers, rows)
        print()


def find_model_differences(report: Dict[str, Any]) -> None:
    """
    Identify tests that pass/fail differently across models (for the same region).
    This helps identify model-specific issues or capability limitations.
    """
    print("\nü§ñ MODEL-SPECIFIC DIFFERENCES\n")
    print("   (Same region, different outcomes across models)\n")
    
    # Group tests by test name and region
    test_region_groups = defaultdict(lambda: defaultdict(dict))
    for test in report['all_tests']:
        test_name = test['name']
        short_name = test_name.split('::')[-1]
        environment = test['environment']
        outcome = test['outcome']
        
        region, model = parse_environment_name(environment)
        test_region_groups[test_name][region][model] = {
            'outcome': outcome,
            'short_name': short_name,
            'environment': environment
        }
    
    # Find regions that have model differences
    regions_with_differences = defaultdict(list)
    for test_name, regions in test_region_groups.items():
        for region, models in regions.items():
            if len(models) > 1:
                outcomes = set(m['outcome'] for m in models.values())
                if len(outcomes) > 1:
                    short_name = list(models.values())[0]['short_name']
                    regions_with_differences[region].append((test_name, models, short_name))
    
    if not regions_with_differences:
        print("   ‚úÖ No model-specific differences found\n")
        print("   All models have consistent outcomes in the same region.\n")
        return
    
    # Display table for each region that has differences
    for region in sorted(regions_with_differences.keys()):
        test_cases = regions_with_differences[region]
        print(f"   Region: {region}")
        print(f"   Found {len(test_cases)} test(s) with different outcomes across models:\n")
        
        # Get all models for this region
        all_models = set()
        for test_name, models, short_name in test_cases:
            all_models.update(models.keys())
        
        model_list = sorted(all_models)
        
        # Build table headers (models)
        headers = ["Test"] + model_list
        rows = []
        
        for test_name, models, short_name in test_cases:
            # Truncate long test names
            display_name = short_name if len(short_name) <= 50 else short_name[:47] + "..."
            row = [display_name]
            
            for model in model_list:
                if model in models:
                    outcome = models[model]['outcome']
                    if outcome == "passed":
                        row.append("‚úÖ PASS")
                    elif outcome == "failed":
                        row.append("‚ùå FAIL")
                    elif outcome == "skipped":
                        row.append("‚è≠Ô∏è  SKIP")
                    elif outcome == "error":
                        row.append("‚ö†Ô∏è  ERROR")
                    else:
                        row.append(outcome)
                else:
                    row.append("N/A")
            
            rows.append(row)
        
        print_table(headers, rows)
        print()


def analyze_slowest_tests(report: Dict[str, Any], top_n: int = 10) -> None:
    """Show the slowest tests across all environments."""
    print(f"\nüêå SLOWEST TESTS (Top {top_n})\n")
    
    all_tests = report['all_tests']
    sorted_tests = sorted(all_tests, key=lambda t: t['duration'], reverse=True)[:top_n]
    
    headers = ["Rank", "Test", "Environment", "Duration"]
    rows = []
    
    for i, test in enumerate(sorted_tests, 1):
        test_name = test['name'].split('::')[-1]
        # Truncate long test names
        display_name = test_name if len(test_name) <= 40 else test_name[:37] + "..."
        rows.append([
            str(i),
            display_name,
            test['environment'],
            f"{test['duration']:.4f}s"
        ])
    
    print_table(headers, rows)
    print()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Analyze merged pytest report from multi-project test runs",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument(
        'run_folder',
        nargs='?',
        help='Specific run folder to analyze (e.g., run_20251113_141828). If not provided, analyzes the latest run.'
    )
    
    args = parser.parse_args()
    
    # Example: Find the latest run or use specified run
    test_output_dir = Path(__file__).parent.parent / "test_output"
    
    if args.run_folder:
        # Use specified run folder
        run_path = test_output_dir / args.run_folder
        if not run_path.exists():
            print(f"Error: Run folder not found: {run_path}")
            exit(1)
        latest_run = run_path
    else:
        # Find most recent run folder
        run_folders = sorted([d for d in test_output_dir.glob("run_*") if d.is_dir()])
        if not run_folders:
            print("No test runs found!")
            print(f"Searched in: {test_output_dir}")
            exit(1)
        latest_run = run_folders[-1]
    
    merged_report_path = latest_run / "merged_pytest_report.json"
    
    if not merged_report_path.exists():
        print(f"Merged report not found: {merged_report_path}")
        print("Make sure the test run completed and generated a merged report.")
        exit(1)
    
    print(f"Analyzing: {merged_report_path}")
    
    # Load and analyze
    report = load_merged_report(str(merged_report_path))
    
    # Main analysis sections
    analyze_report(report)
    analyze_test_outcomes_by_environment(report)
    find_region_differences(report)
    find_model_differences(report)
    analyze_slowest_tests(report)
    
    print("=" * 100)
    print("Analysis complete!")
    print("=" * 100 + "\n")
