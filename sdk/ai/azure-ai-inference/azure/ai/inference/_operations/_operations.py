# pylint: disable=too-many-lines,too-many-statements
# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for license information.
# Code generated by Microsoft (R) Python Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------
from io import IOBase
import json
import sys
from typing import Any, Callable, Dict, IO, List, Optional, TypeVar, Union, overload

from azure.core.exceptions import (
    ClientAuthenticationError,
    HttpResponseError,
    ResourceExistsError,
    ResourceNotFoundError,
    ResourceNotModifiedError,
    map_error,
)
from azure.core.pipeline import PipelineResponse
from azure.core.rest import HttpRequest, HttpResponse
from azure.core.tracing.decorator import distributed_trace
from azure.core.utils import case_insensitive_dict

from .. import models as _models
from .._model_base import SdkJSONEncoder, _deserialize
from .._serialization import Serializer
from .._vendor import ModelClientMixinABC

if sys.version_info >= (3, 9):
    from collections.abc import MutableMapping
else:
    from typing import MutableMapping  # type: ignore  # pylint: disable=ungrouped-imports
JSON = MutableMapping[str, Any]  # pylint: disable=unsubscriptable-object
_Unset: Any = object()
T = TypeVar("T")
ClsType = Optional[Callable[[PipelineResponse[HttpRequest, HttpResponse], T, Dict[str, Any]], Any]]

_SERIALIZER = Serializer()
_SERIALIZER.client_side_validation = False


def build_model_get_chat_completions_request(
    *,
    extra_parameters: Optional[Union[str, _models.ExtraParameters]] = None,
    model_deployemnt: Optional[str] = None,
    **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
    _params = case_insensitive_dict(kwargs.pop("params", {}) or {})

    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2024-04-01-preview"))
    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/v1/chat/completions"

    # Construct parameters
    _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")

    # Construct headers
    if extra_parameters is not None:
        _headers["extra-parameters"] = _SERIALIZER.header("extra_parameters", extra_parameters, "str")
    if model_deployemnt is not None:
        _headers["azureml-model-deployment"] = _SERIALIZER.header("model_deployemnt", model_deployemnt, "str")
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")
    if content_type is not None:
        _headers["Content-Type"] = _SERIALIZER.header("content_type", content_type, "str")

    return HttpRequest(method="POST", url=_url, params=_params, headers=_headers, **kwargs)


def build_model_get_embeddings_request(
    *,
    extra_parameters: Optional[Union[str, _models.ExtraParameters]] = None,
    model_deployemnt: Optional[str] = None,
    **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
    _params = case_insensitive_dict(kwargs.pop("params", {}) or {})

    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2024-04-01-preview"))
    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/v1/embeddings"

    # Construct parameters
    _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")

    # Construct headers
    if extra_parameters is not None:
        _headers["extra-parameters"] = _SERIALIZER.header("extra_parameters", extra_parameters, "str")
    if model_deployemnt is not None:
        _headers["azureml-model-deployment"] = _SERIALIZER.header("model_deployemnt", model_deployemnt, "str")
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")
    if content_type is not None:
        _headers["Content-Type"] = _SERIALIZER.header("content_type", content_type, "str")

    return HttpRequest(method="POST", url=_url, params=_params, headers=_headers, **kwargs)


def build_model_get_image_generations_request(  # pylint: disable=name-too-long
    *,
    extra_parameters: Optional[Union[str, _models.ExtraParameters]] = None,
    model_deployemnt: Optional[str] = None,
    **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
    _params = case_insensitive_dict(kwargs.pop("params", {}) or {})

    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2024-04-01-preview"))
    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/images/generations"

    # Construct parameters
    _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")

    # Construct headers
    if extra_parameters is not None:
        _headers["extra-parameters"] = _SERIALIZER.header("extra_parameters", extra_parameters, "str")
    if model_deployemnt is not None:
        _headers["azureml-model-deployment"] = _SERIALIZER.header("model_deployemnt", model_deployemnt, "str")
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")
    if content_type is not None:
        _headers["Content-Type"] = _SERIALIZER.header("content_type", content_type, "str")

    return HttpRequest(method="POST", url=_url, params=_params, headers=_headers, **kwargs)


class ModelClientOperationsMixin(ModelClientMixinABC):
    @overload
    def get_chat_completions(
        self,
        body: JSON,
        *,
        extra_parameters: Optional[Union[str, _models.ExtraParameters]] = None,
        model_deployemnt: Optional[str] = None,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models.ChatCompletions:
        # pylint: disable=line-too-long
        """Gets chat completions for the provided chat messages.
        Completions support a wide variety of tasks and generate text that continues from or
        "completes"
        provided prompt data.

        :param body: Required.
        :type body: JSON
        :keyword extra_parameters: Controls what happens if extra parameters are passed in the request
         payload. Known values are: "error", "ignore", and "allow". Default value is None.
        :paramtype extra_parameters: str or ~azure.ai.inference.models.ExtraParameters
        :keyword model_deployemnt: Name of the deployment to which you would like to route the request.
         Relevant only to Model-as-a-Platform (MaaP) deployments.
         Typically used when you want to target a test environment instead of production environment.
         Default value is None.
        :paramtype model_deployemnt: str
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: ChatCompletions. The ChatCompletions is compatible with MutableMapping
        :rtype: ~azure.ai.inference.models.ChatCompletions
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # The input is polymorphic. The following are possible polymorphic inputs based off
                  discriminator "type":

                # JSON input template for discriminator value "json_object":
                chat_completions_response_format = {
                    "type": "json_object"
                }

                # JSON input template for discriminator value "text":
                chat_completions_response_format = {
                    "type": "text"
                }

                # JSON input template you can fill out and use as your body input.
                body = {
                    "messages": [
                        chat_request_message
                    ],
                    "extras": {
                        "str": "str"  # Optional. Extra parameters (in the form of string
                          key-value pairs) that are not in the standard request payload. They will be
                          passed to the service as-is in the root of the JSON request payload. How the
                          service handles these extra parameters depends on the value of the
                          ``extra-parameters`` HTTP request header.
                    },
                    "frequency_penalty": 0.0,  # Optional. A value that influences the
                      probability of generated tokens appearing based on their cumulative frequency in
                      generated text. Positive values will make tokens less likely to appear as their
                      frequency increases and decrease the likelihood of the model repeating the same
                      statements verbatim.
                    "max_tokens": 0,  # Optional. The maximum number of tokens to generate.
                    "presence_penalty": 0.0,  # Optional. A value that influences the probability
                      of generated tokens appearing based on their existing presence in generated text.
                      Positive values will make tokens less likely to appear when they already exist
                      and increase the model's likelihood to output new topics.
                    "response_format": chat_completions_response_format,
                    "seed": 0,  # Optional. If specified, the system will make a best effort to
                      sample deterministically such that repeated requests with the same seed and
                      parameters should return the same result. Determinism is not guaranteed.".
                    "stop": [
                        "str"  # Optional. A collection of textual sequences that will end
                          completions generation.
                    ],
                    "stream": bool,  # Optional. A value indicating whether chat completions
                      should be streamed for this request.
                    "temperature": 0.0,  # Optional. The sampling temperature to use that
                      controls the apparent creativity of generated completions. Higher values will
                      make output more random while lower values will make results more focused and
                      deterministic. It is not recommended to modify temperature and top_p for the same
                      completions request as the interaction of these two settings is difficult to
                      predict.
                    "tool_choice": "str",  # Optional. If specified, the model will configure
                      which of the provided tools it can use for the chat completions response. Is
                      either a Union[str, "_models.ChatCompletionsToolSelectionPreset"] type or a
                      ChatCompletionsNamedToolSelection type.
                    "tools": [
                        chat_completions_tool_definition
                    ],
                    "top_p": 0.0  # Optional. An alternative to sampling with temperature called
                      nucleus sampling. This value causes the model to consider the results of tokens
                      with the provided probability mass. As an example, a value of 0.15 will cause
                      only the tokens comprising the top 15% of probability mass to be considered. It
                      is not recommended to modify temperature and top_p for the same completions
                      request as the interaction of these two settings is difficult to predict.
                }

                # response body for status code(s): 200
                response == {
                    "choices": [
                        {
                            "finish_reason": "str",  # The reason that this chat
                              completions choice completed its generated. Required. Known values are:
                              "stop", "length", and "content_filter".
                            "index": 0,  # The ordered index associated with this chat
                              completions choice. Required.
                            "message": {
                                "content": "str",  # The content of the message.
                                  Required.
                                "role": "str",  # The chat role associated with the
                                  message. Required. Known values are: "system", "user", "assistant",
                                  and "tool".
                                "tool_calls": [
                                    chat_completions_tool_call
                                ]
                            }
                        }
                    ],
                    "created": "2020-02-20 00:00:00",  # The first timestamp associated with
                      generation activity for this completions response, represented as seconds since
                      the beginning of the Unix epoch of 00:00 on 1 Jan 1970. Required.
                    "id": "str",  # A unique identifier associated with this chat completions
                      response. Required.
                    "model": "str",  # The model used for the chat completion. Required.
                    "object": "str",  # The response object type, which is always
                      ``chat.completion``. Required.
                    "usage": {
                        "capacity_type": "str",  # Indicates whether your capacity has been
                          affected by the usage amount (token count) reported here. Required. Known
                          values are: "usage" and "fixed".
                        "completion_tokens": 0,  # The number of tokens generated across all
                          completions emissions. Required.
                        "prompt_tokens": 0,  # The number of tokens in the provided prompts
                          for the completions request. Required.
                        "total_tokens": 0  # The total number of tokens processed for the
                          completions request and response. Required.
                    }
                }
        """

    @overload
    def get_chat_completions(
        self,
        *,
        messages: List[_models.ChatRequestMessage],
        extra_parameters: Optional[Union[str, _models.ExtraParameters]] = None,
        model_deployemnt: Optional[str] = None,
        content_type: str = "application/json",
        extras: Optional[Dict[str, str]] = None,
        frequency_penalty: Optional[float] = None,
        presence_penalty: Optional[float] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        max_tokens: Optional[int] = None,
        response_format: Optional[_models.ChatCompletionsResponseFormat] = None,
        stop: Optional[List[str]] = None,
        stream_parameter: Optional[bool] = None,
        tools: Optional[List[_models.ChatCompletionsToolDefinition]] = None,
        tool_choice: Optional[
            Union[str, _models.ChatCompletionsToolSelectionPreset, _models.ChatCompletionsNamedToolSelection]
        ] = None,
        seed: Optional[int] = None,
        **kwargs: Any
    ) -> _models.ChatCompletions:
        # pylint: disable=line-too-long
        """Gets chat completions for the provided chat messages.
        Completions support a wide variety of tasks and generate text that continues from or
        "completes"
        provided prompt data.

        :keyword messages: The collection of context messages associated with this chat completions
         request.
         Typical usage begins with a chat message for the System role that provides instructions for
         the behavior of the assistant, followed by alternating messages between the User and
         Assistant roles. Required.
        :paramtype messages: list[~azure.ai.inference.models.ChatRequestMessage]
        :keyword extra_parameters: Controls what happens if extra parameters are passed in the request
         payload. Known values are: "error", "ignore", and "allow". Default value is None.
        :paramtype extra_parameters: str or ~azure.ai.inference.models.ExtraParameters
        :keyword model_deployemnt: Name of the deployment to which you would like to route the request.
         Relevant only to Model-as-a-Platform (MaaP) deployments.
         Typically used when you want to target a test environment instead of production environment.
         Default value is None.
        :paramtype model_deployemnt: str
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :keyword extras: Extra parameters (in the form of string key-value pairs) that are not in the
         standard request payload.
         They will be passed to the service as-is in the root of the JSON request payload.
         How the service handles these extra parameters depends on the value of the
         ``extra-parameters``
         HTTP request header. Default value is None.
        :paramtype extras: dict[str, str]
        :keyword frequency_penalty: A value that influences the probability of generated tokens
         appearing based on their cumulative
         frequency in generated text.
         Positive values will make tokens less likely to appear as their frequency increases and
         decrease the likelihood of the model repeating the same statements verbatim. Default value is
         None.
        :paramtype frequency_penalty: float
        :keyword presence_penalty: A value that influences the probability of generated tokens
         appearing based on their existing
         presence in generated text.
         Positive values will make tokens less likely to appear when they already exist and increase
         the
         model's likelihood to output new topics. Default value is None.
        :paramtype presence_penalty: float
        :keyword temperature: The sampling temperature to use that controls the apparent creativity of
         generated completions.
         Higher values will make output more random while lower values will make results more focused
         and deterministic.
         It is not recommended to modify temperature and top_p for the same completions request as the
         interaction of these two settings is difficult to predict. Default value is None.
        :paramtype temperature: float
        :keyword top_p: An alternative to sampling with temperature called nucleus sampling. This value
         causes the
         model to consider the results of tokens with the provided probability mass. As an example, a
         value of 0.15 will cause only the tokens comprising the top 15% of probability mass to be
         considered.
         It is not recommended to modify temperature and top_p for the same completions request as the
         interaction of these two settings is difficult to predict. Default value is None.
        :paramtype top_p: float
        :keyword max_tokens: The maximum number of tokens to generate. Default value is None.
        :paramtype max_tokens: int
        :keyword response_format: An object specifying the format that the model must output. Used to
         enable JSON mode. Default value is None.
        :paramtype response_format: ~azure.ai.inference.models.ChatCompletionsResponseFormat
        :keyword stop: A collection of textual sequences that will end completions generation. Default
         value is None.
        :paramtype stop: list[str]
        :keyword stream_parameter: A value indicating whether chat completions should be streamed for
         this request. Default value is None.
        :paramtype stream_parameter: bool
        :keyword tools: The available tool definitions that the chat completions request can use,
         including caller-defined functions. Default value is None.
        :paramtype tools: list[~azure.ai.inference.models.ChatCompletionsToolDefinition]
        :keyword tool_choice: If specified, the model will configure which of the provided tools it can
         use for the chat completions response. Is either a Union[str,
         "_models.ChatCompletionsToolSelectionPreset"] type or a ChatCompletionsNamedToolSelection type.
         Default value is None.
        :paramtype tool_choice: str or ~azure.ai.inference.models.ChatCompletionsToolSelectionPreset or
         ~azure.ai.inference.models.ChatCompletionsNamedToolSelection
        :keyword seed: If specified, the system will make a best effort to sample deterministically
         such that repeated requests with the
         same seed and parameters should return the same result. Determinism is not guaranteed.".
         Default value is None.
        :paramtype seed: int
        :return: ChatCompletions. The ChatCompletions is compatible with MutableMapping
        :rtype: ~azure.ai.inference.models.ChatCompletions
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # response body for status code(s): 200
                response == {
                    "choices": [
                        {
                            "finish_reason": "str",  # The reason that this chat
                              completions choice completed its generated. Required. Known values are:
                              "stop", "length", and "content_filter".
                            "index": 0,  # The ordered index associated with this chat
                              completions choice. Required.
                            "message": {
                                "content": "str",  # The content of the message.
                                  Required.
                                "role": "str",  # The chat role associated with the
                                  message. Required. Known values are: "system", "user", "assistant",
                                  and "tool".
                                "tool_calls": [
                                    chat_completions_tool_call
                                ]
                            }
                        }
                    ],
                    "created": "2020-02-20 00:00:00",  # The first timestamp associated with
                      generation activity for this completions response, represented as seconds since
                      the beginning of the Unix epoch of 00:00 on 1 Jan 1970. Required.
                    "id": "str",  # A unique identifier associated with this chat completions
                      response. Required.
                    "model": "str",  # The model used for the chat completion. Required.
                    "object": "str",  # The response object type, which is always
                      ``chat.completion``. Required.
                    "usage": {
                        "capacity_type": "str",  # Indicates whether your capacity has been
                          affected by the usage amount (token count) reported here. Required. Known
                          values are: "usage" and "fixed".
                        "completion_tokens": 0,  # The number of tokens generated across all
                          completions emissions. Required.
                        "prompt_tokens": 0,  # The number of tokens in the provided prompts
                          for the completions request. Required.
                        "total_tokens": 0  # The total number of tokens processed for the
                          completions request and response. Required.
                    }
                }
        """

    @overload
    def get_chat_completions(
        self,
        body: IO[bytes],
        *,
        extra_parameters: Optional[Union[str, _models.ExtraParameters]] = None,
        model_deployemnt: Optional[str] = None,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models.ChatCompletions:
        # pylint: disable=line-too-long
        """Gets chat completions for the provided chat messages.
        Completions support a wide variety of tasks and generate text that continues from or
        "completes"
        provided prompt data.

        :param body: Required.
        :type body: IO[bytes]
        :keyword extra_parameters: Controls what happens if extra parameters are passed in the request
         payload. Known values are: "error", "ignore", and "allow". Default value is None.
        :paramtype extra_parameters: str or ~azure.ai.inference.models.ExtraParameters
        :keyword model_deployemnt: Name of the deployment to which you would like to route the request.
         Relevant only to Model-as-a-Platform (MaaP) deployments.
         Typically used when you want to target a test environment instead of production environment.
         Default value is None.
        :paramtype model_deployemnt: str
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: ChatCompletions. The ChatCompletions is compatible with MutableMapping
        :rtype: ~azure.ai.inference.models.ChatCompletions
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # response body for status code(s): 200
                response == {
                    "choices": [
                        {
                            "finish_reason": "str",  # The reason that this chat
                              completions choice completed its generated. Required. Known values are:
                              "stop", "length", and "content_filter".
                            "index": 0,  # The ordered index associated with this chat
                              completions choice. Required.
                            "message": {
                                "content": "str",  # The content of the message.
                                  Required.
                                "role": "str",  # The chat role associated with the
                                  message. Required. Known values are: "system", "user", "assistant",
                                  and "tool".
                                "tool_calls": [
                                    chat_completions_tool_call
                                ]
                            }
                        }
                    ],
                    "created": "2020-02-20 00:00:00",  # The first timestamp associated with
                      generation activity for this completions response, represented as seconds since
                      the beginning of the Unix epoch of 00:00 on 1 Jan 1970. Required.
                    "id": "str",  # A unique identifier associated with this chat completions
                      response. Required.
                    "model": "str",  # The model used for the chat completion. Required.
                    "object": "str",  # The response object type, which is always
                      ``chat.completion``. Required.
                    "usage": {
                        "capacity_type": "str",  # Indicates whether your capacity has been
                          affected by the usage amount (token count) reported here. Required. Known
                          values are: "usage" and "fixed".
                        "completion_tokens": 0,  # The number of tokens generated across all
                          completions emissions. Required.
                        "prompt_tokens": 0,  # The number of tokens in the provided prompts
                          for the completions request. Required.
                        "total_tokens": 0  # The total number of tokens processed for the
                          completions request and response. Required.
                    }
                }
        """

    @distributed_trace
    def get_chat_completions(
        self,
        body: Union[JSON, IO[bytes]] = _Unset,
        *,
        messages: List[_models.ChatRequestMessage] = _Unset,
        extra_parameters: Optional[Union[str, _models.ExtraParameters]] = None,
        model_deployemnt: Optional[str] = None,
        extras: Optional[Dict[str, str]] = None,
        frequency_penalty: Optional[float] = None,
        presence_penalty: Optional[float] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        max_tokens: Optional[int] = None,
        response_format: Optional[_models.ChatCompletionsResponseFormat] = None,
        stop: Optional[List[str]] = None,
        stream_parameter: Optional[bool] = None,
        tools: Optional[List[_models.ChatCompletionsToolDefinition]] = None,
        tool_choice: Optional[
            Union[str, _models.ChatCompletionsToolSelectionPreset, _models.ChatCompletionsNamedToolSelection]
        ] = None,
        seed: Optional[int] = None,
        **kwargs: Any
    ) -> _models.ChatCompletions:
        # pylint: disable=line-too-long
        """Gets chat completions for the provided chat messages.
        Completions support a wide variety of tasks and generate text that continues from or
        "completes"
        provided prompt data.

        :param body: Is either a JSON type or a IO[bytes] type. Required.
        :type body: JSON or IO[bytes]
        :keyword messages: The collection of context messages associated with this chat completions
         request.
         Typical usage begins with a chat message for the System role that provides instructions for
         the behavior of the assistant, followed by alternating messages between the User and
         Assistant roles. Required.
        :paramtype messages: list[~azure.ai.inference.models.ChatRequestMessage]
        :keyword extra_parameters: Controls what happens if extra parameters are passed in the request
         payload. Known values are: "error", "ignore", and "allow". Default value is None.
        :paramtype extra_parameters: str or ~azure.ai.inference.models.ExtraParameters
        :keyword model_deployemnt: Name of the deployment to which you would like to route the request.
         Relevant only to Model-as-a-Platform (MaaP) deployments.
         Typically used when you want to target a test environment instead of production environment.
         Default value is None.
        :paramtype model_deployemnt: str
        :keyword extras: Extra parameters (in the form of string key-value pairs) that are not in the
         standard request payload.
         They will be passed to the service as-is in the root of the JSON request payload.
         How the service handles these extra parameters depends on the value of the
         ``extra-parameters``
         HTTP request header. Default value is None.
        :paramtype extras: dict[str, str]
        :keyword frequency_penalty: A value that influences the probability of generated tokens
         appearing based on their cumulative
         frequency in generated text.
         Positive values will make tokens less likely to appear as their frequency increases and
         decrease the likelihood of the model repeating the same statements verbatim. Default value is
         None.
        :paramtype frequency_penalty: float
        :keyword presence_penalty: A value that influences the probability of generated tokens
         appearing based on their existing
         presence in generated text.
         Positive values will make tokens less likely to appear when they already exist and increase
         the
         model's likelihood to output new topics. Default value is None.
        :paramtype presence_penalty: float
        :keyword temperature: The sampling temperature to use that controls the apparent creativity of
         generated completions.
         Higher values will make output more random while lower values will make results more focused
         and deterministic.
         It is not recommended to modify temperature and top_p for the same completions request as the
         interaction of these two settings is difficult to predict. Default value is None.
        :paramtype temperature: float
        :keyword top_p: An alternative to sampling with temperature called nucleus sampling. This value
         causes the
         model to consider the results of tokens with the provided probability mass. As an example, a
         value of 0.15 will cause only the tokens comprising the top 15% of probability mass to be
         considered.
         It is not recommended to modify temperature and top_p for the same completions request as the
         interaction of these two settings is difficult to predict. Default value is None.
        :paramtype top_p: float
        :keyword max_tokens: The maximum number of tokens to generate. Default value is None.
        :paramtype max_tokens: int
        :keyword response_format: An object specifying the format that the model must output. Used to
         enable JSON mode. Default value is None.
        :paramtype response_format: ~azure.ai.inference.models.ChatCompletionsResponseFormat
        :keyword stop: A collection of textual sequences that will end completions generation. Default
         value is None.
        :paramtype stop: list[str]
        :keyword stream_parameter: A value indicating whether chat completions should be streamed for
         this request. Default value is None.
        :paramtype stream_parameter: bool
        :keyword tools: The available tool definitions that the chat completions request can use,
         including caller-defined functions. Default value is None.
        :paramtype tools: list[~azure.ai.inference.models.ChatCompletionsToolDefinition]
        :keyword tool_choice: If specified, the model will configure which of the provided tools it can
         use for the chat completions response. Is either a Union[str,
         "_models.ChatCompletionsToolSelectionPreset"] type or a ChatCompletionsNamedToolSelection type.
         Default value is None.
        :paramtype tool_choice: str or ~azure.ai.inference.models.ChatCompletionsToolSelectionPreset or
         ~azure.ai.inference.models.ChatCompletionsNamedToolSelection
        :keyword seed: If specified, the system will make a best effort to sample deterministically
         such that repeated requests with the
         same seed and parameters should return the same result. Determinism is not guaranteed.".
         Default value is None.
        :paramtype seed: int
        :return: ChatCompletions. The ChatCompletions is compatible with MutableMapping
        :rtype: ~azure.ai.inference.models.ChatCompletions
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # The input is polymorphic. The following are possible polymorphic inputs based off
                  discriminator "type":

                # JSON input template for discriminator value "json_object":
                chat_completions_response_format = {
                    "type": "json_object"
                }

                # JSON input template for discriminator value "text":
                chat_completions_response_format = {
                    "type": "text"
                }

                # JSON input template you can fill out and use as your body input.
                body = {
                    "messages": [
                        chat_request_message
                    ],
                    "extras": {
                        "str": "str"  # Optional. Extra parameters (in the form of string
                          key-value pairs) that are not in the standard request payload. They will be
                          passed to the service as-is in the root of the JSON request payload. How the
                          service handles these extra parameters depends on the value of the
                          ``extra-parameters`` HTTP request header.
                    },
                    "frequency_penalty": 0.0,  # Optional. A value that influences the
                      probability of generated tokens appearing based on their cumulative frequency in
                      generated text. Positive values will make tokens less likely to appear as their
                      frequency increases and decrease the likelihood of the model repeating the same
                      statements verbatim.
                    "max_tokens": 0,  # Optional. The maximum number of tokens to generate.
                    "presence_penalty": 0.0,  # Optional. A value that influences the probability
                      of generated tokens appearing based on their existing presence in generated text.
                      Positive values will make tokens less likely to appear when they already exist
                      and increase the model's likelihood to output new topics.
                    "response_format": chat_completions_response_format,
                    "seed": 0,  # Optional. If specified, the system will make a best effort to
                      sample deterministically such that repeated requests with the same seed and
                      parameters should return the same result. Determinism is not guaranteed.".
                    "stop": [
                        "str"  # Optional. A collection of textual sequences that will end
                          completions generation.
                    ],
                    "stream": bool,  # Optional. A value indicating whether chat completions
                      should be streamed for this request.
                    "temperature": 0.0,  # Optional. The sampling temperature to use that
                      controls the apparent creativity of generated completions. Higher values will
                      make output more random while lower values will make results more focused and
                      deterministic. It is not recommended to modify temperature and top_p for the same
                      completions request as the interaction of these two settings is difficult to
                      predict.
                    "tool_choice": "str",  # Optional. If specified, the model will configure
                      which of the provided tools it can use for the chat completions response. Is
                      either a Union[str, "_models.ChatCompletionsToolSelectionPreset"] type or a
                      ChatCompletionsNamedToolSelection type.
                    "tools": [
                        chat_completions_tool_definition
                    ],
                    "top_p": 0.0  # Optional. An alternative to sampling with temperature called
                      nucleus sampling. This value causes the model to consider the results of tokens
                      with the provided probability mass. As an example, a value of 0.15 will cause
                      only the tokens comprising the top 15% of probability mass to be considered. It
                      is not recommended to modify temperature and top_p for the same completions
                      request as the interaction of these two settings is difficult to predict.
                }

                # response body for status code(s): 200
                response == {
                    "choices": [
                        {
                            "finish_reason": "str",  # The reason that this chat
                              completions choice completed its generated. Required. Known values are:
                              "stop", "length", and "content_filter".
                            "index": 0,  # The ordered index associated with this chat
                              completions choice. Required.
                            "message": {
                                "content": "str",  # The content of the message.
                                  Required.
                                "role": "str",  # The chat role associated with the
                                  message. Required. Known values are: "system", "user", "assistant",
                                  and "tool".
                                "tool_calls": [
                                    chat_completions_tool_call
                                ]
                            }
                        }
                    ],
                    "created": "2020-02-20 00:00:00",  # The first timestamp associated with
                      generation activity for this completions response, represented as seconds since
                      the beginning of the Unix epoch of 00:00 on 1 Jan 1970. Required.
                    "id": "str",  # A unique identifier associated with this chat completions
                      response. Required.
                    "model": "str",  # The model used for the chat completion. Required.
                    "object": "str",  # The response object type, which is always
                      ``chat.completion``. Required.
                    "usage": {
                        "capacity_type": "str",  # Indicates whether your capacity has been
                          affected by the usage amount (token count) reported here. Required. Known
                          values are: "usage" and "fixed".
                        "completion_tokens": 0,  # The number of tokens generated across all
                          completions emissions. Required.
                        "prompt_tokens": 0,  # The number of tokens in the provided prompts
                          for the completions request. Required.
                        "total_tokens": 0  # The total number of tokens processed for the
                          completions request and response. Required.
                    }
                }
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.ChatCompletions] = kwargs.pop("cls", None)

        if body is _Unset:
            if messages is _Unset:
                raise TypeError("missing required argument: messages")
            body = {
                "extras": extras,
                "frequency_penalty": frequency_penalty,
                "max_tokens": max_tokens,
                "messages": messages,
                "presence_penalty": presence_penalty,
                "response_format": response_format,
                "seed": seed,
                "stop": stop,
                "stream": stream_parameter,
                "temperature": temperature,
                "tool_choice": tool_choice,
                "tools": tools,
                "top_p": top_p,
            }
            body = {k: v for k, v in body.items() if v is not None}
        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IOBase, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_model_get_chat_completions_request(
            extra_parameters=extra_parameters,
            model_deployemnt=model_deployemnt,
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                response.read()  # Load the body in memory and close the socket
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.ChatCompletions, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    def get_embeddings(
        self,
        body: JSON,
        *,
        extra_parameters: Optional[Union[str, _models.ExtraParameters]] = None,
        model_deployemnt: Optional[str] = None,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models.EmbeddingsResult:
        # pylint: disable=line-too-long
        """Return the embeddings for a given prompt.

        :param body: Required.
        :type body: JSON
        :keyword extra_parameters: Controls what happens if extra parameters are passed in the request
         payload. Known values are: "error", "ignore", and "allow". Default value is None.
        :paramtype extra_parameters: str or ~azure.ai.inference.models.ExtraParameters
        :keyword model_deployemnt: Name of the deployment to which you would like to route the request.
         Relevant only to Model-as-a-Platform (MaaP) deployments.
         Typically used when you want to target a test environment instead of production environment.
         Default value is None.
        :paramtype model_deployemnt: str
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: EmbeddingsResult. The EmbeddingsResult is compatible with MutableMapping
        :rtype: ~azure.ai.inference.models.EmbeddingsResult
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # JSON input template you can fill out and use as your body input.
                body = {
                    "input": [
                        "str"  # Input texts to get embeddings for, encoded as a an array of
                          strings. Required.
                    ],
                    "extras": {
                        "str": "str"  # Optional. Extra parameters (in the form of string
                          key-value pairs) that are not in the standard request payload. They will be
                          passed to the service as-is in the root of the JSON request payload. How the
                          service handles these extra parameters depends on the value of the
                          ``extra-parameters`` HTTP request header.
                    },
                    "input_type": "str"  # Optional. Specifies the input type to use for
                      embedding search. Known values are: "text", "query", and "document".
                }

                # response body for status code(s): 200
                response == {
                    "data": [
                        {
                            "embedding": [
                                0.0  # List of embeddings value for the input prompt.
                                  These represent a measurement of the vector-based relatedness of the
                                  provided input. Required.
                            ],
                            "index": 0,  # Index of the prompt to which the EmbeddingItem
                              corresponds. Required.
                            "object": "str"  # The object type of this embeddings item.
                              Will always be ``embedding``. Required.
                        }
                    ],
                    "id": "str",  # Unique identifier for the embeddings result. Required.
                    "model": "str",  # The model ID used to generate this result. Required.
                    "object": "str",  # The object type of the embeddings result. Will always be
                      ``list``. Required.
                    "usage": {
                        "capacity_type": "str",  # Indicates whether your capacity has been
                          affected by the usage amount (token count) reported here. Required. Known
                          values are: "usage" and "fixed".
                        "input_tokens": 0,  # Number of tokens in the request prompt.
                          Required.
                        "prompt_tokens": 0,  # Number of tokens used for the prompt sent to
                          the AI model. Typically identical to"" ``input_tokens``. However, certain AI
                          models may add extra tokens to the input hence the number can be higher. (for
                          example when input_type="query"). Required.
                        "total_tokens": 0  # Total number of tokens transacted in this
                          request/response. Required.
                    }
                }
        """

    @overload
    def get_embeddings(
        self,
        *,
        input: List[str],
        extra_parameters: Optional[Union[str, _models.ExtraParameters]] = None,
        model_deployemnt: Optional[str] = None,
        content_type: str = "application/json",
        extras: Optional[Dict[str, str]] = None,
        input_type: Optional[Union[str, _models.EmbeddingInputType]] = None,
        **kwargs: Any
    ) -> _models.EmbeddingsResult:
        # pylint: disable=line-too-long
        """Return the embeddings for a given prompt.

        :keyword input: Input texts to get embeddings for, encoded as a an array of strings. Required.
        :paramtype input: list[str]
        :keyword extra_parameters: Controls what happens if extra parameters are passed in the request
         payload. Known values are: "error", "ignore", and "allow". Default value is None.
        :paramtype extra_parameters: str or ~azure.ai.inference.models.ExtraParameters
        :keyword model_deployemnt: Name of the deployment to which you would like to route the request.
         Relevant only to Model-as-a-Platform (MaaP) deployments.
         Typically used when you want to target a test environment instead of production environment.
         Default value is None.
        :paramtype model_deployemnt: str
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :keyword extras: Extra parameters (in the form of string key-value pairs) that are not in the
         standard request payload.
         They will be passed to the service as-is in the root of the JSON request payload.
         How the service handles these extra parameters depends on the value of the
         ``extra-parameters``
         HTTP request header. Default value is None.
        :paramtype extras: dict[str, str]
        :keyword input_type: Specifies the input type to use for embedding search. Known values are:
         "text", "query", and "document". Default value is None.
        :paramtype input_type: str or ~azure.ai.inference.models.EmbeddingInputType
        :return: EmbeddingsResult. The EmbeddingsResult is compatible with MutableMapping
        :rtype: ~azure.ai.inference.models.EmbeddingsResult
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # response body for status code(s): 200
                response == {
                    "data": [
                        {
                            "embedding": [
                                0.0  # List of embeddings value for the input prompt.
                                  These represent a measurement of the vector-based relatedness of the
                                  provided input. Required.
                            ],
                            "index": 0,  # Index of the prompt to which the EmbeddingItem
                              corresponds. Required.
                            "object": "str"  # The object type of this embeddings item.
                              Will always be ``embedding``. Required.
                        }
                    ],
                    "id": "str",  # Unique identifier for the embeddings result. Required.
                    "model": "str",  # The model ID used to generate this result. Required.
                    "object": "str",  # The object type of the embeddings result. Will always be
                      ``list``. Required.
                    "usage": {
                        "capacity_type": "str",  # Indicates whether your capacity has been
                          affected by the usage amount (token count) reported here. Required. Known
                          values are: "usage" and "fixed".
                        "input_tokens": 0,  # Number of tokens in the request prompt.
                          Required.
                        "prompt_tokens": 0,  # Number of tokens used for the prompt sent to
                          the AI model. Typically identical to"" ``input_tokens``. However, certain AI
                          models may add extra tokens to the input hence the number can be higher. (for
                          example when input_type="query"). Required.
                        "total_tokens": 0  # Total number of tokens transacted in this
                          request/response. Required.
                    }
                }
        """

    @overload
    def get_embeddings(
        self,
        body: IO[bytes],
        *,
        extra_parameters: Optional[Union[str, _models.ExtraParameters]] = None,
        model_deployemnt: Optional[str] = None,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models.EmbeddingsResult:
        # pylint: disable=line-too-long
        """Return the embeddings for a given prompt.

        :param body: Required.
        :type body: IO[bytes]
        :keyword extra_parameters: Controls what happens if extra parameters are passed in the request
         payload. Known values are: "error", "ignore", and "allow". Default value is None.
        :paramtype extra_parameters: str or ~azure.ai.inference.models.ExtraParameters
        :keyword model_deployemnt: Name of the deployment to which you would like to route the request.
         Relevant only to Model-as-a-Platform (MaaP) deployments.
         Typically used when you want to target a test environment instead of production environment.
         Default value is None.
        :paramtype model_deployemnt: str
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: EmbeddingsResult. The EmbeddingsResult is compatible with MutableMapping
        :rtype: ~azure.ai.inference.models.EmbeddingsResult
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # response body for status code(s): 200
                response == {
                    "data": [
                        {
                            "embedding": [
                                0.0  # List of embeddings value for the input prompt.
                                  These represent a measurement of the vector-based relatedness of the
                                  provided input. Required.
                            ],
                            "index": 0,  # Index of the prompt to which the EmbeddingItem
                              corresponds. Required.
                            "object": "str"  # The object type of this embeddings item.
                              Will always be ``embedding``. Required.
                        }
                    ],
                    "id": "str",  # Unique identifier for the embeddings result. Required.
                    "model": "str",  # The model ID used to generate this result. Required.
                    "object": "str",  # The object type of the embeddings result. Will always be
                      ``list``. Required.
                    "usage": {
                        "capacity_type": "str",  # Indicates whether your capacity has been
                          affected by the usage amount (token count) reported here. Required. Known
                          values are: "usage" and "fixed".
                        "input_tokens": 0,  # Number of tokens in the request prompt.
                          Required.
                        "prompt_tokens": 0,  # Number of tokens used for the prompt sent to
                          the AI model. Typically identical to"" ``input_tokens``. However, certain AI
                          models may add extra tokens to the input hence the number can be higher. (for
                          example when input_type="query"). Required.
                        "total_tokens": 0  # Total number of tokens transacted in this
                          request/response. Required.
                    }
                }
        """

    @distributed_trace
    def get_embeddings(
        self,
        body: Union[JSON, IO[bytes]] = _Unset,
        *,
        input: List[str] = _Unset,
        extra_parameters: Optional[Union[str, _models.ExtraParameters]] = None,
        model_deployemnt: Optional[str] = None,
        extras: Optional[Dict[str, str]] = None,
        input_type: Optional[Union[str, _models.EmbeddingInputType]] = None,
        **kwargs: Any
    ) -> _models.EmbeddingsResult:
        # pylint: disable=line-too-long
        """Return the embeddings for a given prompt.

        :param body: Is either a JSON type or a IO[bytes] type. Required.
        :type body: JSON or IO[bytes]
        :keyword input: Input texts to get embeddings for, encoded as a an array of strings. Required.
        :paramtype input: list[str]
        :keyword extra_parameters: Controls what happens if extra parameters are passed in the request
         payload. Known values are: "error", "ignore", and "allow". Default value is None.
        :paramtype extra_parameters: str or ~azure.ai.inference.models.ExtraParameters
        :keyword model_deployemnt: Name of the deployment to which you would like to route the request.
         Relevant only to Model-as-a-Platform (MaaP) deployments.
         Typically used when you want to target a test environment instead of production environment.
         Default value is None.
        :paramtype model_deployemnt: str
        :keyword extras: Extra parameters (in the form of string key-value pairs) that are not in the
         standard request payload.
         They will be passed to the service as-is in the root of the JSON request payload.
         How the service handles these extra parameters depends on the value of the
         ``extra-parameters``
         HTTP request header. Default value is None.
        :paramtype extras: dict[str, str]
        :keyword input_type: Specifies the input type to use for embedding search. Known values are:
         "text", "query", and "document". Default value is None.
        :paramtype input_type: str or ~azure.ai.inference.models.EmbeddingInputType
        :return: EmbeddingsResult. The EmbeddingsResult is compatible with MutableMapping
        :rtype: ~azure.ai.inference.models.EmbeddingsResult
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # JSON input template you can fill out and use as your body input.
                body = {
                    "input": [
                        "str"  # Input texts to get embeddings for, encoded as a an array of
                          strings. Required.
                    ],
                    "extras": {
                        "str": "str"  # Optional. Extra parameters (in the form of string
                          key-value pairs) that are not in the standard request payload. They will be
                          passed to the service as-is in the root of the JSON request payload. How the
                          service handles these extra parameters depends on the value of the
                          ``extra-parameters`` HTTP request header.
                    },
                    "input_type": "str"  # Optional. Specifies the input type to use for
                      embedding search. Known values are: "text", "query", and "document".
                }

                # response body for status code(s): 200
                response == {
                    "data": [
                        {
                            "embedding": [
                                0.0  # List of embeddings value for the input prompt.
                                  These represent a measurement of the vector-based relatedness of the
                                  provided input. Required.
                            ],
                            "index": 0,  # Index of the prompt to which the EmbeddingItem
                              corresponds. Required.
                            "object": "str"  # The object type of this embeddings item.
                              Will always be ``embedding``. Required.
                        }
                    ],
                    "id": "str",  # Unique identifier for the embeddings result. Required.
                    "model": "str",  # The model ID used to generate this result. Required.
                    "object": "str",  # The object type of the embeddings result. Will always be
                      ``list``. Required.
                    "usage": {
                        "capacity_type": "str",  # Indicates whether your capacity has been
                          affected by the usage amount (token count) reported here. Required. Known
                          values are: "usage" and "fixed".
                        "input_tokens": 0,  # Number of tokens in the request prompt.
                          Required.
                        "prompt_tokens": 0,  # Number of tokens used for the prompt sent to
                          the AI model. Typically identical to"" ``input_tokens``. However, certain AI
                          models may add extra tokens to the input hence the number can be higher. (for
                          example when input_type="query"). Required.
                        "total_tokens": 0  # Total number of tokens transacted in this
                          request/response. Required.
                    }
                }
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.EmbeddingsResult] = kwargs.pop("cls", None)

        if body is _Unset:
            if input is _Unset:
                raise TypeError("missing required argument: input")
            body = {"extras": extras, "input": input, "input_type": input_type}
            body = {k: v for k, v in body.items() if v is not None}
        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IOBase, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_model_get_embeddings_request(
            extra_parameters=extra_parameters,
            model_deployemnt=model_deployemnt,
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                response.read()  # Load the body in memory and close the socket
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.EmbeddingsResult, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    def get_image_generations(
        self,
        body: JSON,
        *,
        extra_parameters: Optional[Union[str, _models.ExtraParameters]] = None,
        model_deployemnt: Optional[str] = None,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models.ImageGenerations:
        # pylint: disable=line-too-long
        """Creates images given a prompt.

        :param body: Required.
        :type body: JSON
        :keyword extra_parameters: Controls what happens if extra parameters are passed in the request
         payload. Known values are: "error", "ignore", and "allow". Default value is None.
        :paramtype extra_parameters: str or ~azure.ai.inference.models.ExtraParameters
        :keyword model_deployemnt: Name of the deployment to which you would like to route the request.
         Relevant only to Model-as-a-Platform (MaaP) deployments.
         Typically used when you want to target a test environment instead of production environment.
         Default value is None.
        :paramtype model_deployemnt: str
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: ImageGenerations. The ImageGenerations is compatible with MutableMapping
        :rtype: ~azure.ai.inference.models.ImageGenerations
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # JSON input template you can fill out and use as your body input.
                body = {
                    "prompt": "str",  # A description of the desired images. Required.
                    "size": "str",  # The desired dimension in pixels of the generated images, in
                      the format ":code:`<Width>`x:code:`<Hight>`". For example: "1024x1024",
                      "1792x1024". Required.
                    "extras": {
                        "str": "str"  # Optional. Extra parameters (in the form of string
                          key-value pairs) that are not in the standard request payload. They will be
                          passed to the service as-is in the root of the JSON request payload. How the
                          service handles these extra parameters depends on the value of the
                          ``extra-parameters`` HTTP request header.
                    },
                    "quality": "str",  # Optional. The desired image generation quality level to
                      use. Known values are: "standard" and "hd".
                    "response_format": "str",  # Optional. The format in which image generation
                      response items should be presented. Known values are: "url" and "b64_json".
                    "seed": 0  # Optional. If specified, the system will make a best effort to
                      sample deterministically such that repeated requests with the same seed and
                      parameters should return the same result. Determinism is not guaranteed.".
                }

                # response body for status code(s): 200
                response == {
                    "created": "2020-02-20 00:00:00",  # A timestamp representing when this
                      operation was started. Represented as seconds since the beginning of the Unix
                      epoch of 00:00 on 1 Jan 1970. Required.
                    "data": [
                        {
                            "b64_json": "str",  # Optional. The complete data for an
                              image, represented as a base64-encoded string.
                            "url": "str"  # Optional. The URL that provides temporary
                              access to download the generated image.
                        }
                    ],
                    "id": "str",  # A unique identifier associated with this image generation
                      response. Required.
                    "model": "str"  # The model used for the image generation. Required.
                }
        """

    @overload
    def get_image_generations(
        self,
        *,
        prompt: str,
        size: str,
        extra_parameters: Optional[Union[str, _models.ExtraParameters]] = None,
        model_deployemnt: Optional[str] = None,
        content_type: str = "application/json",
        extras: Optional[Dict[str, str]] = None,
        quality: Optional[Union[str, _models.ImageGenerationQuality]] = None,
        response_format: Optional[Union[str, _models.ImageGenerationResponseFormat]] = None,
        seed: Optional[int] = None,
        **kwargs: Any
    ) -> _models.ImageGenerations:
        # pylint: disable=line-too-long
        """Creates images given a prompt.

        :keyword prompt: A description of the desired images. Required.
        :paramtype prompt: str
        :keyword size: The desired dimension in pixels of the generated images, in the format
         ":code:`<Width>`x:code:`<Hight>`".
         For example: "1024x1024", "1792x1024". Required.
        :paramtype size: str
        :keyword extra_parameters: Controls what happens if extra parameters are passed in the request
         payload. Known values are: "error", "ignore", and "allow". Default value is None.
        :paramtype extra_parameters: str or ~azure.ai.inference.models.ExtraParameters
        :keyword model_deployemnt: Name of the deployment to which you would like to route the request.
         Relevant only to Model-as-a-Platform (MaaP) deployments.
         Typically used when you want to target a test environment instead of production environment.
         Default value is None.
        :paramtype model_deployemnt: str
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :keyword extras: Extra parameters (in the form of string key-value pairs) that are not in the
         standard request payload.
         They will be passed to the service as-is in the root of the JSON request payload.
         How the service handles these extra parameters depends on the value of the
         ``extra-parameters``
         HTTP request header. Default value is None.
        :paramtype extras: dict[str, str]
        :keyword quality: The desired image generation quality level to use. Known values are:
         "standard" and "hd". Default value is None.
        :paramtype quality: str or ~azure.ai.inference.models.ImageGenerationQuality
        :keyword response_format: The format in which image generation response items should be
         presented. Known values are: "url" and "b64_json". Default value is None.
        :paramtype response_format: str or ~azure.ai.inference.models.ImageGenerationResponseFormat
        :keyword seed: If specified, the system will make a best effort to sample deterministically
         such that repeated requests with the
         same seed and parameters should return the same result. Determinism is not guaranteed.".
         Default value is None.
        :paramtype seed: int
        :return: ImageGenerations. The ImageGenerations is compatible with MutableMapping
        :rtype: ~azure.ai.inference.models.ImageGenerations
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # response body for status code(s): 200
                response == {
                    "created": "2020-02-20 00:00:00",  # A timestamp representing when this
                      operation was started. Represented as seconds since the beginning of the Unix
                      epoch of 00:00 on 1 Jan 1970. Required.
                    "data": [
                        {
                            "b64_json": "str",  # Optional. The complete data for an
                              image, represented as a base64-encoded string.
                            "url": "str"  # Optional. The URL that provides temporary
                              access to download the generated image.
                        }
                    ],
                    "id": "str",  # A unique identifier associated with this image generation
                      response. Required.
                    "model": "str"  # The model used for the image generation. Required.
                }
        """

    @overload
    def get_image_generations(
        self,
        body: IO[bytes],
        *,
        extra_parameters: Optional[Union[str, _models.ExtraParameters]] = None,
        model_deployemnt: Optional[str] = None,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models.ImageGenerations:
        # pylint: disable=line-too-long
        """Creates images given a prompt.

        :param body: Required.
        :type body: IO[bytes]
        :keyword extra_parameters: Controls what happens if extra parameters are passed in the request
         payload. Known values are: "error", "ignore", and "allow". Default value is None.
        :paramtype extra_parameters: str or ~azure.ai.inference.models.ExtraParameters
        :keyword model_deployemnt: Name of the deployment to which you would like to route the request.
         Relevant only to Model-as-a-Platform (MaaP) deployments.
         Typically used when you want to target a test environment instead of production environment.
         Default value is None.
        :paramtype model_deployemnt: str
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: ImageGenerations. The ImageGenerations is compatible with MutableMapping
        :rtype: ~azure.ai.inference.models.ImageGenerations
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # response body for status code(s): 200
                response == {
                    "created": "2020-02-20 00:00:00",  # A timestamp representing when this
                      operation was started. Represented as seconds since the beginning of the Unix
                      epoch of 00:00 on 1 Jan 1970. Required.
                    "data": [
                        {
                            "b64_json": "str",  # Optional. The complete data for an
                              image, represented as a base64-encoded string.
                            "url": "str"  # Optional. The URL that provides temporary
                              access to download the generated image.
                        }
                    ],
                    "id": "str",  # A unique identifier associated with this image generation
                      response. Required.
                    "model": "str"  # The model used for the image generation. Required.
                }
        """

    @distributed_trace
    def get_image_generations(
        self,
        body: Union[JSON, IO[bytes]] = _Unset,
        *,
        prompt: str = _Unset,
        size: str = _Unset,
        extra_parameters: Optional[Union[str, _models.ExtraParameters]] = None,
        model_deployemnt: Optional[str] = None,
        extras: Optional[Dict[str, str]] = None,
        quality: Optional[Union[str, _models.ImageGenerationQuality]] = None,
        response_format: Optional[Union[str, _models.ImageGenerationResponseFormat]] = None,
        seed: Optional[int] = None,
        **kwargs: Any
    ) -> _models.ImageGenerations:
        # pylint: disable=line-too-long
        """Creates images given a prompt.

        :param body: Is either a JSON type or a IO[bytes] type. Required.
        :type body: JSON or IO[bytes]
        :keyword prompt: A description of the desired images. Required.
        :paramtype prompt: str
        :keyword size: The desired dimension in pixels of the generated images, in the format
         ":code:`<Width>`x:code:`<Hight>`".
         For example: "1024x1024", "1792x1024". Required.
        :paramtype size: str
        :keyword extra_parameters: Controls what happens if extra parameters are passed in the request
         payload. Known values are: "error", "ignore", and "allow". Default value is None.
        :paramtype extra_parameters: str or ~azure.ai.inference.models.ExtraParameters
        :keyword model_deployemnt: Name of the deployment to which you would like to route the request.
         Relevant only to Model-as-a-Platform (MaaP) deployments.
         Typically used when you want to target a test environment instead of production environment.
         Default value is None.
        :paramtype model_deployemnt: str
        :keyword extras: Extra parameters (in the form of string key-value pairs) that are not in the
         standard request payload.
         They will be passed to the service as-is in the root of the JSON request payload.
         How the service handles these extra parameters depends on the value of the
         ``extra-parameters``
         HTTP request header. Default value is None.
        :paramtype extras: dict[str, str]
        :keyword quality: The desired image generation quality level to use. Known values are:
         "standard" and "hd". Default value is None.
        :paramtype quality: str or ~azure.ai.inference.models.ImageGenerationQuality
        :keyword response_format: The format in which image generation response items should be
         presented. Known values are: "url" and "b64_json". Default value is None.
        :paramtype response_format: str or ~azure.ai.inference.models.ImageGenerationResponseFormat
        :keyword seed: If specified, the system will make a best effort to sample deterministically
         such that repeated requests with the
         same seed and parameters should return the same result. Determinism is not guaranteed.".
         Default value is None.
        :paramtype seed: int
        :return: ImageGenerations. The ImageGenerations is compatible with MutableMapping
        :rtype: ~azure.ai.inference.models.ImageGenerations
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # JSON input template you can fill out and use as your body input.
                body = {
                    "prompt": "str",  # A description of the desired images. Required.
                    "size": "str",  # The desired dimension in pixels of the generated images, in
                      the format ":code:`<Width>`x:code:`<Hight>`". For example: "1024x1024",
                      "1792x1024". Required.
                    "extras": {
                        "str": "str"  # Optional. Extra parameters (in the form of string
                          key-value pairs) that are not in the standard request payload. They will be
                          passed to the service as-is in the root of the JSON request payload. How the
                          service handles these extra parameters depends on the value of the
                          ``extra-parameters`` HTTP request header.
                    },
                    "quality": "str",  # Optional. The desired image generation quality level to
                      use. Known values are: "standard" and "hd".
                    "response_format": "str",  # Optional. The format in which image generation
                      response items should be presented. Known values are: "url" and "b64_json".
                    "seed": 0  # Optional. If specified, the system will make a best effort to
                      sample deterministically such that repeated requests with the same seed and
                      parameters should return the same result. Determinism is not guaranteed.".
                }

                # response body for status code(s): 200
                response == {
                    "created": "2020-02-20 00:00:00",  # A timestamp representing when this
                      operation was started. Represented as seconds since the beginning of the Unix
                      epoch of 00:00 on 1 Jan 1970. Required.
                    "data": [
                        {
                            "b64_json": "str",  # Optional. The complete data for an
                              image, represented as a base64-encoded string.
                            "url": "str"  # Optional. The URL that provides temporary
                              access to download the generated image.
                        }
                    ],
                    "id": "str",  # A unique identifier associated with this image generation
                      response. Required.
                    "model": "str"  # The model used for the image generation. Required.
                }
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.ImageGenerations] = kwargs.pop("cls", None)

        if body is _Unset:
            if prompt is _Unset:
                raise TypeError("missing required argument: prompt")
            if size is _Unset:
                raise TypeError("missing required argument: size")
            body = {
                "extras": extras,
                "prompt": prompt,
                "quality": quality,
                "response_format": response_format,
                "seed": seed,
                "size": size,
            }
            body = {k: v for k, v in body.items() if v is not None}
        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IOBase, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_model_get_image_generations_request(
            extra_parameters=extra_parameters,
            model_deployemnt=model_deployemnt,
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                response.read()  # Load the body in memory and close the socket
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.ImageGenerations, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore
