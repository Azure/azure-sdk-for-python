# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for license information.
# Code generated by Microsoft (R) Python Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------
import pytest
from devtools_testutils.aio import recorded_by_proxy_async
from testpreparer import UnifiedPreparer
from testpreparer_async import UnifiedClientTestBaseAsync


@pytest.mark.skip("you may need to update the auto-generated test case before run it")
class TestUnifiedModelsOperationsAsync(UnifiedClientTestBaseAsync):
    @UnifiedPreparer()
    @recorded_by_proxy_async
    async def test_models_get(self, unified_endpoint):
        client = self.create_async_client(endpoint=unified_endpoint)
        response = await client.models.get(
            deployment_name="str",
        )

        # please add some check logic here by yourself
        # ...

    @UnifiedPreparer()
    @recorded_by_proxy_async
    async def test_models_list(self, unified_endpoint):
        client = self.create_async_client(endpoint=unified_endpoint)
        response = client.models.list()
        result = [r async for r in response]
        # please add some check logic here by yourself
        # ...

    @UnifiedPreparer()
    @recorded_by_proxy_async
    async def test_models_get_chat_completions(self, unified_endpoint):
        client = self.create_async_client(endpoint=unified_endpoint)
        response = await client.models.get_chat_completions(
            body={
                "messages": ["chat_request_message"],
                "frequency_penalty": 0.0,
                "max_tokens": 0,
                "modalities": ["str"],
                "model": "str",
                "presence_penalty": 0.0,
                "response_format": "chat_completions_response_format",
                "seed": 0,
                "stop": ["str"],
                "stream": bool,
                "temperature": 0.0,
                "tool_choice": "str",
                "tools": [
                    {"function": {"name": "str", "description": "str", "parameters": {"str": {}}}, "type": "function"}
                ],
                "top_p": 0.0,
            },
        )

        # please add some check logic here by yourself
        # ...

    @UnifiedPreparer()
    @recorded_by_proxy_async
    async def test_models_get_embeddings(self, unified_endpoint):
        client = self.create_async_client(endpoint=unified_endpoint)
        response = await client.models.get_embeddings(
            body={"input": ["str"], "dimensions": 0, "encoding_format": "str", "input_type": "str", "model": "str"},
        )

        # please add some check logic here by yourself
        # ...

    @UnifiedPreparer()
    @recorded_by_proxy_async
    async def test_models_get_image_embeddings(self, unified_endpoint):
        client = self.create_async_client(endpoint=unified_endpoint)
        response = await client.models.get_image_embeddings(
            body={
                "input": [{"image": "str", "text": "str"}],
                "dimensions": 0,
                "encoding_format": "str",
                "input_type": "str",
                "model": "str",
            },
        )

        # please add some check logic here by yourself
        # ...
