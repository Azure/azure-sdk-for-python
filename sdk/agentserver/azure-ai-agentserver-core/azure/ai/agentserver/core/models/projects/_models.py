# pylint: disable=line-too-long,useless-suppression,too-many-lines
# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for license information.
# Code generated by Microsoft (R) Python Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------
# pylint: disable=useless-super-delegation

from .utils.model_base  import Model as _Model
from typing import Literal, Union, Dict, Optional, List, Any
from typing_extensions import NotRequired, TypedDict, TypeAlias
from ._enums import ItemType, ServiceTier, ToolChoiceOptions, ResponseTextFormatConfigurationType, ToolType, ResponseErrorCode
import datetime


class AgentId(TypedDict):
    type: str  # TODO: should this be a Literal?
    """Required. Default value is \"agent_id\"."""
    name: str
    """The name of the agent. Required."""
    version: str
    """The version identifier of the agent. Required.""" 

class AgentReference(TypedDict):
    type: str  # TODO: should this be a Literal?
    """Required. Default value is \"agent_reference\"."""
    name: str
    """The name of the agent. Required."""
    version: NotRequired[str]
    """The version identifier of the agent."""    

class CreatedBy(TypedDict):
    agent: NotRequired[AgentId]
    """The agent that created the item."""
    response_id: NotRequired[str]
    """The response on which the item is created."""

class ItemResource(TypedDict):
    type: Union[str, Literal[
        "message", "file_search_call", "function_call", "function_call_output", 
        "computer_call", "computer_call_output", "web_search_call", "reasoning", 
        "item_reference", "image_generation_call", "code_interpreter_call",
        "local_shell_call", "local_shell_call_output", "mcp_list_tools", 
        "mcp_approval_request", "mcp_approval_response", "mcp_call", 
        "structured_outputs", "workflow_action", "memory_search_call", 
        "oauth_consent_request"
    ]]
    id: str
    created_by: NotRequired[CreatedBy]

class FunctionToolCallItemResource(ItemResource):
    """Function tool call item resource with discriminator 'function_call'."""
    type: Literal[ItemType.FUNCTION_CALL] # TODO: how to best represent this?
    """Required. Discriminator value is \"function_call\"."""
    status: Union[str, Literal["in_progress"], Literal["completed"], Literal["incomplete"]]
    """The status of the function call."""
    call_id: str
    """The call ID of the function call."""
    name: str
    """The name of the function."""
    arguments: str
    """The arguments passed to the function."""

class FunctionToolCallOutputItemResource(ItemResource):
    """Function tool call output item resource with discriminator 'function_call_output'."""
    type: Literal[ItemType.FUNCTION_CALL_OUTPUT] # TODO: how to best represent this?
    """Required. Discriminator value is \"function_call_output\"."""
    status: Union[str, Literal["in_progress"], Literal["completed"], Literal["incomplete"]]
    """The status of the function call."""
    call_id: str
    """The call ID of the function call."""
    output: str
    """The output returned from the function call."""

class ItemContent(TypedDict):
    type: Union[str, Literal[
        "input_text", "input_audio", "input_image", "input_file", 
        "output_text", "output_audio", "refusal"
    ]]

class Annotation(TypedDict):
    type: Union[str, Literal["file_citation", "url_citation", "file_path", "container_file_citation"]]

class TopLogProb(TypedDict):
    token: str
    logprob: float
    bytes: List[int]

class LogProb(TypedDict):
    token: str
    logprob: float
    bytes: List[int]
    top_logprobs: List[TopLogProb]

class ItemContentOutputText(ItemContent):

    type: Literal["output_text"]
    """The type of the output text. Always ``output_text``. Required."""
    text: str
    """The text output from the model. Required."""
    annotations: List[Annotation]
    """The annotations of the text output. Required."""
    logprobs: NotRequired[List[LogProb]]
    """Log probabilities for the output text. Optional."""

class ItemParam(TypedDict):
    type: Union[str, Literal[
        "message", "file_search_call", "function_call", "function_call_output", 
        "computer_call", "computer_call_output", "web_search_call", "reasoning", 
        "item_reference", "image_generation_call", "code_interpreter_call",
        "local_shell_call", "local_shell_call_output", "mcp_list_tools", 
        "mcp_approval_request", "mcp_approval_response", "mcp_call", 
        "structured_outputs", "workflow_action", "memory_search_call", 
        "oauth_consent_request"
    ]]


### Response type and associated types

class Reasoning(TypedDict):
    """**o-series models only**
    Configuration options for
    `reasoning models <https://platform.openai.com/docs/guides/reasoning>`_.
    """
    
    effort: NotRequired[Union[str, Literal["low", "medium", "high"]]]
    """Known values are: \"low\", \"medium\", and \"high\"."""
    summary: NotRequired[Literal["auto", "concise", "detailed"]]
    """A summary of the reasoning performed by the model. This can be
     useful for debugging and understanding the model's reasoning process.
     One of ``auto``, ``concise``, or ``detailed``."""
    generate_summary: NotRequired[Literal["auto", "concise", "detailed"]]
    """**Deprecated:** use ``summary`` instead.
     A summary of the reasoning performed by the model. This can be
     useful for debugging and understanding the model's reasoning process.
     One of ``auto``, ``concise``, or ``detailed``."""

class ResponseTextFormatConfiguration(TypedDict):
    type: Union[str, ResponseTextFormatConfigurationType]

class ResponseText(TypedDict):
    format: NotRequired[ResponseTextFormatConfiguration]

ResponsePromptVariables: TypeAlias = Dict[str, Any]

class Tool(TypedDict):
    type: Union[str, ToolType]

class Prompt(TypedDict):
    id: str
    version: NotRequired[str]
    variables: NotRequired[ResponsePromptVariables]

class ResponseError(TypedDict):
    code: Union[str, ResponseErrorCode]
    message: str

class ResponseIncompleteDetails1(TypedDict):
    reason: Literal["max_output_tokens", "content_filter"]

class MemoryStoreOperationUsageInputTokenDetails(TypedDict):
    cached_tokens: int
    """Number of input tokens retrieved from cache."""

class MemoryStoreOperationUsageOutputTokenDetails(TypedDict):
    cached_tokens: int
    """Number of output tokens retrieved from cache."""

class ResponseUsage(TypedDict):
    input_tokens: int
    """The number of input tokens used in the response."""
    input_tokens_details: MemoryStoreOperationUsageInputTokenDetails
    """A detailed breakdown of input token usage."""
    output_tokens: int
    """The number of output tokens"""
    output_tokens_details: MemoryStoreOperationUsageOutputTokenDetails
    """A detailed breakdown of output token usage."""
    total_tokens: int
    """The total number of tokens used."""

class ResponseConversation1(TypedDict):
    id: str


class Response(TypedDict):

    metadata: Dict[str, str]
    """Set of 16 key-value pairs that can be attached to an object. This can be
     useful for storing additional information about the object in a structured
     format, and querying for objects via API or the dashboard.
     Keys are strings with a maximum length of 64 characters. Values are strings
     with a maximum length of 512 characters. Required."""
    temperature: float
    """What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output
     more random, while lower values like 0.2 will make it more focused and deterministic.
     We generally recommend altering this or ``top_p`` but not both. Required."""
    top_p: float
    """An alternative to sampling with temperature, called nucleus sampling,
     where the model considers the results of the tokens with top_p probability
     mass. So 0.1 means only the tokens comprising the top 10% probability mass
     are considered. We generally recommend altering this or ``temperature`` but not both. Required."""
    user: str
    """A unique identifier representing your end-user, which can help OpenAI to monitor and detect
     abuse. Required."""
    id: str
    """Unique identifier for this Response. Required."""
    object: Literal["response"]
    """The object type of this resource - always set to ``response``. Required."""
    created_at: datetime.datetime
    """Unix timestamp (in seconds) of when this Response was created. Required."""
    output: List[ItemResource]
    """An array of content items generated by the model. Required."""
    instructions: Union[str, List[ItemParam]]
    """A system (or developer) message inserted into the model's context. Required."""
    parallel_tool_calls: bool
    """Whether to allow the model to run tool calls in parallel. Required."""
    
    # Optional fields
    service_tier: NotRequired[Union[str, Literal["auto", "default", "flex", "scale", "priority"], ServiceTier]]
    """Note: service_tier is not applicable to Azure OpenAI. Known values are:
     \"auto\", \"default\", \"flex\", \"scale\", and \"priority\"."""
    top_logprobs: NotRequired[int]
    """An integer between 0 and 20 specifying the number of most likely tokens to return at each token
     position, each with an associated log probability."""
    previous_response_id: NotRequired[str]
    """The unique ID of the previous response to the model. Use this to
     create multi-turn conversations."""
    model: NotRequired[str]
    """The model deployment to use for the creation of this response."""
    reasoning: NotRequired[Reasoning]  # TODO: Replace with proper Reasoning type when available
    background: NotRequired[bool]
    """Whether to run the model response in the background."""
    max_output_tokens: NotRequired[int]
    """An upper bound for the number of tokens that can be generated for a response."""
    max_tool_calls: NotRequired[int]
    """The maximum number of total calls to built-in tools that can be processed in a response."""
    text: NotRequired[ResponseText]  # TODO: Replace with proper ResponseText type when available
    """Configuration options for a text response from the model."""
    tools: NotRequired[List[Tool]]  # TODO: Replace with proper Tool type when available
    """An array of tools the model may call while generating a response."""
    tool_choice: NotRequired[Union[str, ToolChoiceOptions]]  # TODO: Replace with proper types when available
    """How the model should select which tool (or tools) to use when generating a response."""
    prompt: NotRequired[Prompt]  # TODO: Replace with proper Prompt type when available
    truncation: NotRequired[Literal["auto", "disabled"]]
    """The truncation strategy to use for the model response."""
    status: NotRequired[Literal["completed", "failed", "in_progress", "cancelled", "queued", "incomplete"]]
    """The status of the response generation."""
    error: NotRequired[ResponseError]  # TODO: Replace with proper ResponseError type when available
    incomplete_details: NotRequired[ResponseIncompleteDetails1]  # TODO: Replace with proper ResponseIncompleteDetails1 type when available
    """Details about why the response is incomplete."""
    output_text: NotRequired[str]
    """SDK-only convenience property that contains the aggregated text output."""
    usage: NotRequired[ResponseUsage]  # TODO: Replace with proper ResponseUsage type when available
    conversation: NotRequired[ResponseConversation1]  # TODO: Replace with proper ResponseConversation type when available
    agent: NotRequired[AgentId]
    """The agent used for this response."""
    structured_inputs: NotRequired[Dict[str, Any]]
    """The structured inputs to the response that can participate in prompt template substitution."""

