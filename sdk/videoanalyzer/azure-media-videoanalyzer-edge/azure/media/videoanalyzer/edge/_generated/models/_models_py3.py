# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for license information.
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------

import datetime
from typing import List, Optional, Union

import msrest.serialization

from ._direct_methodsfor_azure_video_analyzeron_io_tedge_enums import *


class SinkNodeBase(msrest.serialization.Model):
    """Enables a pipeline topology to write media data to a destination outside of the Azure Video Analyzer IoT Edge module.

    You probably want to use the sub-classes and not this class directly. Known
    sub-classes are: AssetSink, FileSink, IotHubMessageSink, VideoSink.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param name: Required. The name to be used for the topology sink.
    :type name: str
    :param inputs: Required. An array of the names of the other nodes in the pipeline topology, the
     outputs of which are used as input for this sink node.
    :type inputs: list[~azure.media.videoanalyzer.edge.models.NodeInput]
    """

    _validation = {
        'type': {'required': True},
        'name': {'required': True},
        'inputs': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'inputs': {'key': 'inputs', 'type': '[NodeInput]'},
    }

    _subtype_map = {
        'type': {'#Microsoft.VideoAnalyzer.AssetSink': 'AssetSink', '#Microsoft.VideoAnalyzer.FileSink': 'FileSink', '#Microsoft.VideoAnalyzer.IotHubMessageSink': 'IotHubMessageSink', '#Microsoft.VideoAnalyzer.VideoSink': 'VideoSink'}
    }

    def __init__(
        self,
        *,
        name: str,
        inputs: List["NodeInput"],
        **kwargs
    ):
        super(SinkNodeBase, self).__init__(**kwargs)
        self.type = None  # type: Optional[str]
        self.name = name
        self.inputs = inputs


class AssetSink(SinkNodeBase):
    """Enables a pipeline topology to record media to an Azure Media Services asset for subsequent playback.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param name: Required. The name to be used for the topology sink.
    :type name: str
    :param inputs: Required. An array of the names of the other nodes in the pipeline topology, the
     outputs of which are used as input for this sink node.
    :type inputs: list[~azure.media.videoanalyzer.edge.models.NodeInput]
    :param asset_container_sas_url: Required. An Azure Storage SAS Url which points to container,
     such as the one created for an Azure Media Services asset.
    :type asset_container_sas_url: str
    :param segment_length: When writing media to an asset, wait until at least this duration of
     media has been accumulated on the Edge. Expressed in increments of 30 seconds, with a minimum
     of 30 seconds and a recommended maximum of 5 minutes.
    :type segment_length: str
    :param local_media_cache_path: Required. Path to a local file system directory for temporary
     caching of media before writing to an Asset. Used when the Edge device is temporarily
     disconnected from Azure.
    :type local_media_cache_path: str
    :param local_media_cache_maximum_size_mi_b: Required. Maximum amount of disk space that can be
     used for temporary caching of media.
    :type local_media_cache_maximum_size_mi_b: str
    """

    _validation = {
        'type': {'required': True},
        'name': {'required': True},
        'inputs': {'required': True},
        'asset_container_sas_url': {'required': True},
        'local_media_cache_path': {'required': True},
        'local_media_cache_maximum_size_mi_b': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'inputs': {'key': 'inputs', 'type': '[NodeInput]'},
        'asset_container_sas_url': {'key': 'assetContainerSasUrl', 'type': 'str'},
        'segment_length': {'key': 'segmentLength', 'type': 'str'},
        'local_media_cache_path': {'key': 'localMediaCachePath', 'type': 'str'},
        'local_media_cache_maximum_size_mi_b': {'key': 'localMediaCacheMaximumSizeMiB', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        name: str,
        inputs: List["NodeInput"],
        asset_container_sas_url: str,
        local_media_cache_path: str,
        local_media_cache_maximum_size_mi_b: str,
        segment_length: Optional[str] = None,
        **kwargs
    ):
        super(AssetSink, self).__init__(name=name, inputs=inputs, **kwargs)
        self.type = '#Microsoft.VideoAnalyzer.AssetSink'  # type: str
        self.asset_container_sas_url = asset_container_sas_url
        self.segment_length = segment_length
        self.local_media_cache_path = local_media_cache_path
        self.local_media_cache_maximum_size_mi_b = local_media_cache_maximum_size_mi_b


class CertificateSource(msrest.serialization.Model):
    """Base class for certificate sources.

    You probably want to use the sub-classes and not this class directly. Known
    sub-classes are: PemCertificateList.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    """

    _validation = {
        'type': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
    }

    _subtype_map = {
        'type': {'#Microsoft.VideoAnalyzer.PemCertificateList': 'PemCertificateList'}
    }

    def __init__(
        self,
        **kwargs
    ):
        super(CertificateSource, self).__init__(**kwargs)
        self.type = None  # type: Optional[str]


class ProcessorNodeBase(msrest.serialization.Model):
    """A node that represents the desired processing of media in a topology. Takes media and/or events as inputs, and emits media and/or event as output.

    You probably want to use the sub-classes and not this class directly. Known
    sub-classes are: CognitiveServicesVisionProcessor, ExtensionProcessorBase, LineCrossingProcessor, MotionDetectionProcessor, ObjectTrackingProcessor, SignalGateProcessor.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param name: Required. The name for this processor node.
    :type name: str
    :param inputs: Required. An array of the names of the other nodes in the topology, the outputs
     of which are used as input for this processor node.
    :type inputs: list[~azure.media.videoanalyzer.edge.models.NodeInput]
    """

    _validation = {
        'type': {'required': True},
        'name': {'required': True},
        'inputs': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'inputs': {'key': 'inputs', 'type': '[NodeInput]'},
    }

    _subtype_map = {
        'type': {'#Microsoft.VideoAnalyzer.CognitiveServicesVisionProcessor': 'CognitiveServicesVisionProcessor', '#Microsoft.VideoAnalyzer.ExtensionProcessorBase': 'ExtensionProcessorBase', '#Microsoft.VideoAnalyzer.LineCrossingProcessor': 'LineCrossingProcessor', '#Microsoft.VideoAnalyzer.MotionDetectionProcessor': 'MotionDetectionProcessor', '#Microsoft.VideoAnalyzer.ObjectTrackingProcessor': 'ObjectTrackingProcessor', '#Microsoft.VideoAnalyzer.SignalGateProcessor': 'SignalGateProcessor'}
    }

    def __init__(
        self,
        *,
        name: str,
        inputs: List["NodeInput"],
        **kwargs
    ):
        super(ProcessorNodeBase, self).__init__(**kwargs)
        self.type = None  # type: Optional[str]
        self.name = name
        self.inputs = inputs


class CognitiveServicesVisionProcessor(ProcessorNodeBase):
    """A processor that allows the pipeline topology to send video frames to a Cognitive Services Vision extension. Inference results are relayed to downstream nodes.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param name: Required. The name for this processor node.
    :type name: str
    :param inputs: Required. An array of the names of the other nodes in the topology, the outputs
     of which are used as input for this processor node.
    :type inputs: list[~azure.media.videoanalyzer.edge.models.NodeInput]
    :param endpoint: Required. Endpoint to which this processor should connect.
    :type endpoint: ~azure.media.videoanalyzer.edge.models.EndpointBase
    :param image: Describes the parameters of the image that is sent as input to the endpoint.
    :type image: ~azure.media.videoanalyzer.edge.models.ImageProperties
    :param sampling_options: Describes the sampling options to be applied when forwarding samples
     to the extension.
    :type sampling_options: ~azure.media.videoanalyzer.edge.models.SamplingOptions
    :param operation: Required. Describes the Spatial Analysis operation to be used in the
     Cognitive Services Vision processor.
    :type operation: ~azure.media.videoanalyzer.edge.models.SpatialAnalysisOperationBase
    """

    _validation = {
        'type': {'required': True},
        'name': {'required': True},
        'inputs': {'required': True},
        'endpoint': {'required': True},
        'operation': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'inputs': {'key': 'inputs', 'type': '[NodeInput]'},
        'endpoint': {'key': 'endpoint', 'type': 'EndpointBase'},
        'image': {'key': 'image', 'type': 'ImageProperties'},
        'sampling_options': {'key': 'samplingOptions', 'type': 'SamplingOptions'},
        'operation': {'key': 'operation', 'type': 'SpatialAnalysisOperationBase'},
    }

    def __init__(
        self,
        *,
        name: str,
        inputs: List["NodeInput"],
        endpoint: "EndpointBase",
        operation: "SpatialAnalysisOperationBase",
        image: Optional["ImageProperties"] = None,
        sampling_options: Optional["SamplingOptions"] = None,
        **kwargs
    ):
        super(CognitiveServicesVisionProcessor, self).__init__(name=name, inputs=inputs, **kwargs)
        self.type = '#Microsoft.VideoAnalyzer.CognitiveServicesVisionProcessor'  # type: str
        self.endpoint = endpoint
        self.image = image
        self.sampling_options = sampling_options
        self.operation = operation


class CredentialsBase(msrest.serialization.Model):
    """Credentials to present during authentication.

    You probably want to use the sub-classes and not this class directly. Known
    sub-classes are: HttpHeaderCredentials, SymmetricKeyCredentials, UsernamePasswordCredentials.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    """

    _validation = {
        'type': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
    }

    _subtype_map = {
        'type': {'#Microsoft.VideoAnalyzer.HttpHeaderCredentials': 'HttpHeaderCredentials', '#Microsoft.VideoAnalyzer.SymmetricKeyCredentials': 'SymmetricKeyCredentials', '#Microsoft.VideoAnalyzer.UsernamePasswordCredentials': 'UsernamePasswordCredentials'}
    }

    def __init__(
        self,
        **kwargs
    ):
        super(CredentialsBase, self).__init__(**kwargs)
        self.type = None  # type: Optional[str]


class EndpointBase(msrest.serialization.Model):
    """Base class for endpoints.

    You probably want to use the sub-classes and not this class directly. Known
    sub-classes are: TlsEndpoint, UnsecuredEndpoint.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param credentials: Polymorphic credentials to be presented to the endpoint.
    :type credentials: ~azure.media.videoanalyzer.edge.models.CredentialsBase
    :param url: Required. Url for the endpoint.
    :type url: str
    """

    _validation = {
        'type': {'required': True},
        'url': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'credentials': {'key': 'credentials', 'type': 'CredentialsBase'},
        'url': {'key': 'url', 'type': 'str'},
    }

    _subtype_map = {
        'type': {'#Microsoft.VideoAnalyzer.TlsEndpoint': 'TlsEndpoint', '#Microsoft.VideoAnalyzer.UnsecuredEndpoint': 'UnsecuredEndpoint'}
    }

    def __init__(
        self,
        *,
        url: str,
        credentials: Optional["CredentialsBase"] = None,
        **kwargs
    ):
        super(EndpointBase, self).__init__(**kwargs)
        self.type = None  # type: Optional[str]
        self.credentials = credentials
        self.url = url


class ExtensionProcessorBase(ProcessorNodeBase):
    """Processor that allows for extensions outside of the Azure Video Analyzer Edge module to be integrated into the pipeline topology. It is the base class for various different kinds of extension processor types.

    You probably want to use the sub-classes and not this class directly. Known
    sub-classes are: GrpcExtension, HttpExtension.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param name: Required. The name for this processor node.
    :type name: str
    :param inputs: Required. An array of the names of the other nodes in the topology, the outputs
     of which are used as input for this processor node.
    :type inputs: list[~azure.media.videoanalyzer.edge.models.NodeInput]
    :param endpoint: Required. Endpoint to which this processor should connect.
    :type endpoint: ~azure.media.videoanalyzer.edge.models.EndpointBase
    :param image: Required. Describes the parameters of the image that is sent as input to the
     endpoint.
    :type image: ~azure.media.videoanalyzer.edge.models.ImageProperties
    :param sampling_options: Describes the sampling options to be applied when forwarding samples
     to the extension.
    :type sampling_options: ~azure.media.videoanalyzer.edge.models.SamplingOptions
    """

    _validation = {
        'type': {'required': True},
        'name': {'required': True},
        'inputs': {'required': True},
        'endpoint': {'required': True},
        'image': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'inputs': {'key': 'inputs', 'type': '[NodeInput]'},
        'endpoint': {'key': 'endpoint', 'type': 'EndpointBase'},
        'image': {'key': 'image', 'type': 'ImageProperties'},
        'sampling_options': {'key': 'samplingOptions', 'type': 'SamplingOptions'},
    }

    _subtype_map = {
        'type': {'#Microsoft.VideoAnalyzer.GrpcExtension': 'GrpcExtension', '#Microsoft.VideoAnalyzer.HttpExtension': 'HttpExtension'}
    }

    def __init__(
        self,
        *,
        name: str,
        inputs: List["NodeInput"],
        endpoint: "EndpointBase",
        image: "ImageProperties",
        sampling_options: Optional["SamplingOptions"] = None,
        **kwargs
    ):
        super(ExtensionProcessorBase, self).__init__(name=name, inputs=inputs, **kwargs)
        self.type = '#Microsoft.VideoAnalyzer.ExtensionProcessorBase'  # type: str
        self.endpoint = endpoint
        self.image = image
        self.sampling_options = sampling_options


class FileSink(SinkNodeBase):
    """Enables a topology to write/store media (video and audio) to a file on the Edge device.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param name: Required. The name to be used for the topology sink.
    :type name: str
    :param inputs: Required. An array of the names of the other nodes in the pipeline topology, the
     outputs of which are used as input for this sink node.
    :type inputs: list[~azure.media.videoanalyzer.edge.models.NodeInput]
    :param base_directory_path: Required. Absolute directory for all outputs to the Edge device
     from this sink.
    :type base_directory_path: str
    :param file_name_pattern: Required. File name pattern for creating new files on the Edge
     device. The pattern must include at least one system variable. See the documentation for
     available variables and additional examples.
    :type file_name_pattern: str
    :param maximum_size_mi_b: Required. Maximum amount of disk space that can be used for storing
     files from this sink.
    :type maximum_size_mi_b: str
    """

    _validation = {
        'type': {'required': True},
        'name': {'required': True},
        'inputs': {'required': True},
        'base_directory_path': {'required': True},
        'file_name_pattern': {'required': True},
        'maximum_size_mi_b': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'inputs': {'key': 'inputs', 'type': '[NodeInput]'},
        'base_directory_path': {'key': 'baseDirectoryPath', 'type': 'str'},
        'file_name_pattern': {'key': 'fileNamePattern', 'type': 'str'},
        'maximum_size_mi_b': {'key': 'maximumSizeMiB', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        name: str,
        inputs: List["NodeInput"],
        base_directory_path: str,
        file_name_pattern: str,
        maximum_size_mi_b: str,
        **kwargs
    ):
        super(FileSink, self).__init__(name=name, inputs=inputs, **kwargs)
        self.type = '#Microsoft.VideoAnalyzer.FileSink'  # type: str
        self.base_directory_path = base_directory_path
        self.file_name_pattern = file_name_pattern
        self.maximum_size_mi_b = maximum_size_mi_b


class GrpcExtension(ExtensionProcessorBase):
    """A processor that allows the pipeline topology to send video frames to an external inference container over a gRPC connection. This can be done using shared memory (for high frame rates), or over the network. Inference results are relayed to downstream nodes.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param name: Required. The name for this processor node.
    :type name: str
    :param inputs: Required. An array of the names of the other nodes in the topology, the outputs
     of which are used as input for this processor node.
    :type inputs: list[~azure.media.videoanalyzer.edge.models.NodeInput]
    :param endpoint: Required. Endpoint to which this processor should connect.
    :type endpoint: ~azure.media.videoanalyzer.edge.models.EndpointBase
    :param image: Required. Describes the parameters of the image that is sent as input to the
     endpoint.
    :type image: ~azure.media.videoanalyzer.edge.models.ImageProperties
    :param sampling_options: Describes the sampling options to be applied when forwarding samples
     to the extension.
    :type sampling_options: ~azure.media.videoanalyzer.edge.models.SamplingOptions
    :param data_transfer: Required. How media should be transferred to the inference engine.
    :type data_transfer: ~azure.media.videoanalyzer.edge.models.GrpcExtensionDataTransfer
    :param extension_configuration: Optional configuration to pass to the gRPC extension.
    :type extension_configuration: str
    """

    _validation = {
        'type': {'required': True},
        'name': {'required': True},
        'inputs': {'required': True},
        'endpoint': {'required': True},
        'image': {'required': True},
        'data_transfer': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'inputs': {'key': 'inputs', 'type': '[NodeInput]'},
        'endpoint': {'key': 'endpoint', 'type': 'EndpointBase'},
        'image': {'key': 'image', 'type': 'ImageProperties'},
        'sampling_options': {'key': 'samplingOptions', 'type': 'SamplingOptions'},
        'data_transfer': {'key': 'dataTransfer', 'type': 'GrpcExtensionDataTransfer'},
        'extension_configuration': {'key': 'extensionConfiguration', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        name: str,
        inputs: List["NodeInput"],
        endpoint: "EndpointBase",
        image: "ImageProperties",
        data_transfer: "GrpcExtensionDataTransfer",
        sampling_options: Optional["SamplingOptions"] = None,
        extension_configuration: Optional[str] = None,
        **kwargs
    ):
        super(GrpcExtension, self).__init__(name=name, inputs=inputs, endpoint=endpoint, image=image, sampling_options=sampling_options, **kwargs)
        self.type = '#Microsoft.VideoAnalyzer.GrpcExtension'  # type: str
        self.data_transfer = data_transfer
        self.extension_configuration = extension_configuration


class GrpcExtensionDataTransfer(msrest.serialization.Model):
    """Describes how media should be transferred to the inference engine.

    All required parameters must be populated in order to send to Azure.

    :param shared_memory_size_mi_b: The size of the buffer for all in-flight frames in mebibytes if
     mode is SharedMemory. Should not be specified otherwise.
    :type shared_memory_size_mi_b: str
    :param mode: Required. How frame data should be transmitted to the inference engine. Possible
     values include: "embedded", "sharedMemory".
    :type mode: str or ~azure.media.videoanalyzer.edge.models.GrpcExtensionDataTransferMode
    """

    _validation = {
        'mode': {'required': True},
    }

    _attribute_map = {
        'shared_memory_size_mi_b': {'key': 'sharedMemorySizeMiB', 'type': 'str'},
        'mode': {'key': 'mode', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        mode: Union[str, "GrpcExtensionDataTransferMode"],
        shared_memory_size_mi_b: Optional[str] = None,
        **kwargs
    ):
        super(GrpcExtensionDataTransfer, self).__init__(**kwargs)
        self.shared_memory_size_mi_b = shared_memory_size_mi_b
        self.mode = mode


class HttpExtension(ExtensionProcessorBase):
    """A processor that allows the pipeline topology to send video frames (mostly at low frame rates e.g. <5 fps) to an external inference container over an HTTP-based RESTful API. Inference results are relayed to downstream nodes.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param name: Required. The name for this processor node.
    :type name: str
    :param inputs: Required. An array of the names of the other nodes in the topology, the outputs
     of which are used as input for this processor node.
    :type inputs: list[~azure.media.videoanalyzer.edge.models.NodeInput]
    :param endpoint: Required. Endpoint to which this processor should connect.
    :type endpoint: ~azure.media.videoanalyzer.edge.models.EndpointBase
    :param image: Required. Describes the parameters of the image that is sent as input to the
     endpoint.
    :type image: ~azure.media.videoanalyzer.edge.models.ImageProperties
    :param sampling_options: Describes the sampling options to be applied when forwarding samples
     to the extension.
    :type sampling_options: ~azure.media.videoanalyzer.edge.models.SamplingOptions
    """

    _validation = {
        'type': {'required': True},
        'name': {'required': True},
        'inputs': {'required': True},
        'endpoint': {'required': True},
        'image': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'inputs': {'key': 'inputs', 'type': '[NodeInput]'},
        'endpoint': {'key': 'endpoint', 'type': 'EndpointBase'},
        'image': {'key': 'image', 'type': 'ImageProperties'},
        'sampling_options': {'key': 'samplingOptions', 'type': 'SamplingOptions'},
    }

    def __init__(
        self,
        *,
        name: str,
        inputs: List["NodeInput"],
        endpoint: "EndpointBase",
        image: "ImageProperties",
        sampling_options: Optional["SamplingOptions"] = None,
        **kwargs
    ):
        super(HttpExtension, self).__init__(name=name, inputs=inputs, endpoint=endpoint, image=image, sampling_options=sampling_options, **kwargs)
        self.type = '#Microsoft.VideoAnalyzer.HttpExtension'  # type: str


class HttpHeaderCredentials(CredentialsBase):
    """Http header service credentials.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param header_name: Required. HTTP header name.
    :type header_name: str
    :param header_value: Required. HTTP header value. Please use a parameter so that the actual
     value is not returned on PUT or GET requests.
    :type header_value: str
    """

    _validation = {
        'type': {'required': True},
        'header_name': {'required': True},
        'header_value': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'header_name': {'key': 'headerName', 'type': 'str'},
        'header_value': {'key': 'headerValue', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        header_name: str,
        header_value: str,
        **kwargs
    ):
        super(HttpHeaderCredentials, self).__init__(**kwargs)
        self.type = '#Microsoft.VideoAnalyzer.HttpHeaderCredentials'  # type: str
        self.header_name = header_name
        self.header_value = header_value


class ImageFormatProperties(msrest.serialization.Model):
    """Encoding settings for an image.

    You probably want to use the sub-classes and not this class directly. Known
    sub-classes are: ImageFormatBmp, ImageFormatJpeg, ImageFormatPng, ImageFormatRaw.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    """

    _validation = {
        'type': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
    }

    _subtype_map = {
        'type': {'#Microsoft.VideoAnalyzer.ImageFormatBmp': 'ImageFormatBmp', '#Microsoft.VideoAnalyzer.ImageFormatJpeg': 'ImageFormatJpeg', '#Microsoft.VideoAnalyzer.ImageFormatPng': 'ImageFormatPng', '#Microsoft.VideoAnalyzer.ImageFormatRaw': 'ImageFormatRaw'}
    }

    def __init__(
        self,
        **kwargs
    ):
        super(ImageFormatProperties, self).__init__(**kwargs)
        self.type = None  # type: Optional[str]


class ImageFormatBmp(ImageFormatProperties):
    """Encoding settings for Bmp images.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    """

    _validation = {
        'type': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
    }

    def __init__(
        self,
        **kwargs
    ):
        super(ImageFormatBmp, self).__init__(**kwargs)
        self.type = '#Microsoft.VideoAnalyzer.ImageFormatBmp'  # type: str


class ImageFormatJpeg(ImageFormatProperties):
    """Encoding settings for Jpeg images.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param quality: The image quality. Value must be between 0 to 100 (best quality).
    :type quality: str
    """

    _validation = {
        'type': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'quality': {'key': 'quality', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        quality: Optional[str] = None,
        **kwargs
    ):
        super(ImageFormatJpeg, self).__init__(**kwargs)
        self.type = '#Microsoft.VideoAnalyzer.ImageFormatJpeg'  # type: str
        self.quality = quality


class ImageFormatPng(ImageFormatProperties):
    """Encoding settings for Png images.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    """

    _validation = {
        'type': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
    }

    def __init__(
        self,
        **kwargs
    ):
        super(ImageFormatPng, self).__init__(**kwargs)
        self.type = '#Microsoft.VideoAnalyzer.ImageFormatPng'  # type: str


class ImageFormatRaw(ImageFormatProperties):
    """Encoding settings for raw images.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param pixel_format: Required. The pixel format that will be used to encode images. Possible
     values include: "yuv420p", "rgb565be", "rgb565le", "rgb555be", "rgb555le", "rgb24", "bgr24",
     "argb", "rgba", "abgr", "bgra".
    :type pixel_format: str or ~azure.media.videoanalyzer.edge.models.ImageFormatRawPixelFormat
    """

    _validation = {
        'type': {'required': True},
        'pixel_format': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'pixel_format': {'key': 'pixelFormat', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        pixel_format: Union[str, "ImageFormatRawPixelFormat"],
        **kwargs
    ):
        super(ImageFormatRaw, self).__init__(**kwargs)
        self.type = '#Microsoft.VideoAnalyzer.ImageFormatRaw'  # type: str
        self.pixel_format = pixel_format


class ImageProperties(msrest.serialization.Model):
    """Describes the properties of an image frame.

    :param scale: The scaling mode for the image.
    :type scale: ~azure.media.videoanalyzer.edge.models.ImageScale
    :param format: Encoding settings for an image.
    :type format: ~azure.media.videoanalyzer.edge.models.ImageFormatProperties
    """

    _attribute_map = {
        'scale': {'key': 'scale', 'type': 'ImageScale'},
        'format': {'key': 'format', 'type': 'ImageFormatProperties'},
    }

    def __init__(
        self,
        *,
        scale: Optional["ImageScale"] = None,
        format: Optional["ImageFormatProperties"] = None,
        **kwargs
    ):
        super(ImageProperties, self).__init__(**kwargs)
        self.scale = scale
        self.format = format


class ImageScale(msrest.serialization.Model):
    """The scaling mode for the image.

    :param mode: Describes the modes for scaling an input video frame into an image, before it is
     sent to an inference engine. Possible values include: "preserveAspectRatio", "pad", "stretch".
    :type mode: str or ~azure.media.videoanalyzer.edge.models.ImageScaleMode
    :param width: The desired output width of the image.
    :type width: str
    :param height: The desired output height of the image.
    :type height: str
    """

    _attribute_map = {
        'mode': {'key': 'mode', 'type': 'str'},
        'width': {'key': 'width', 'type': 'str'},
        'height': {'key': 'height', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        mode: Optional[Union[str, "ImageScaleMode"]] = None,
        width: Optional[str] = None,
        height: Optional[str] = None,
        **kwargs
    ):
        super(ImageScale, self).__init__(**kwargs)
        self.mode = mode
        self.width = width
        self.height = height


class IotHubMessageSink(SinkNodeBase):
    """Enables a pipeline topology to publish messages that can be delivered via routes declared in the IoT Edge deployment manifest.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param name: Required. The name to be used for the topology sink.
    :type name: str
    :param inputs: Required. An array of the names of the other nodes in the pipeline topology, the
     outputs of which are used as input for this sink node.
    :type inputs: list[~azure.media.videoanalyzer.edge.models.NodeInput]
    :param hub_output_name: Required. Name of the output path to which the pipeline topology will
     publish message. These messages can then be delivered to desired destinations by declaring
     routes referencing the output path in the IoT Edge deployment manifest.
    :type hub_output_name: str
    """

    _validation = {
        'type': {'required': True},
        'name': {'required': True},
        'inputs': {'required': True},
        'hub_output_name': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'inputs': {'key': 'inputs', 'type': '[NodeInput]'},
        'hub_output_name': {'key': 'hubOutputName', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        name: str,
        inputs: List["NodeInput"],
        hub_output_name: str,
        **kwargs
    ):
        super(IotHubMessageSink, self).__init__(name=name, inputs=inputs, **kwargs)
        self.type = '#Microsoft.VideoAnalyzer.IotHubMessageSink'  # type: str
        self.hub_output_name = hub_output_name


class SourceNodeBase(msrest.serialization.Model):
    """A source node in a pipeline topology.

    You probably want to use the sub-classes and not this class directly. Known
    sub-classes are: IotHubMessageSource, RtspSource.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The type of the source node. The discriminator for derived
     types.Constant filled by server.
    :type type: str
    :param name: Required. The name to be used for this source node.
    :type name: str
    """

    _validation = {
        'type': {'required': True},
        'name': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
    }

    _subtype_map = {
        'type': {'#Microsoft.VideoAnalyzer.IotHubMessageSource': 'IotHubMessageSource', '#Microsoft.VideoAnalyzer.RtspSource': 'RtspSource'}
    }

    def __init__(
        self,
        *,
        name: str,
        **kwargs
    ):
        super(SourceNodeBase, self).__init__(**kwargs)
        self.type = None  # type: Optional[str]
        self.name = name


class IotHubMessageSource(SourceNodeBase):
    """Enables a pipeline topology to receive messages via routes declared in the IoT Edge deployment manifest.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The type of the source node. The discriminator for derived
     types.Constant filled by server.
    :type type: str
    :param name: Required. The name to be used for this source node.
    :type name: str
    :param hub_input_name: Name of the input path where messages can be routed to (via routes
     declared in the IoT Edge deployment manifest).
    :type hub_input_name: str
    """

    _validation = {
        'type': {'required': True},
        'name': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'hub_input_name': {'key': 'hubInputName', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        name: str,
        hub_input_name: Optional[str] = None,
        **kwargs
    ):
        super(IotHubMessageSource, self).__init__(name=name, **kwargs)
        self.type = '#Microsoft.VideoAnalyzer.IotHubMessageSource'  # type: str
        self.hub_input_name = hub_input_name


class MethodRequest(msrest.serialization.Model):
    """Base Class for Method Requests.

    You probably want to use the sub-classes and not this class directly. Known
    sub-classes are: ItemNonSetRequestBase, LivePipelineSetRequestBody, PipelineTopologySetRequestBody, LivePipelineListRequest, LivePipelineSetRequest, PipelineTopologyListRequest, PipelineTopologySetRequest.

    Variables are only populated by the server, and will be ignored when sending a request.

    All required parameters must be populated in order to send to Azure.

    :ivar method_name: Required. method name.Constant filled by server.
    :vartype method_name: str
    :ivar api_version: api version. Default value: "1.0".
    :vartype api_version: str
    """

    _validation = {
        'method_name': {'required': True, 'readonly': True},
        'api_version': {'constant': True},
    }

    _attribute_map = {
        'method_name': {'key': 'methodName', 'type': 'str'},
        'api_version': {'key': '@apiVersion', 'type': 'str'},
    }

    _subtype_map = {
        'method_name': {'ItemNonSetRequestBase': 'ItemNonSetRequestBase', 'LivePipelineSetRequestBody': 'LivePipelineSetRequestBody', 'PipelineTopologySetRequestBody': 'PipelineTopologySetRequestBody', 'livePipelineList': 'LivePipelineListRequest', 'livePipelineSet': 'LivePipelineSetRequest', 'pipelineTopologyList': 'PipelineTopologyListRequest', 'pipelineTopologySet': 'PipelineTopologySetRequest'}
    }

    api_version = "1.0"

    def __init__(
        self,
        **kwargs
    ):
        super(MethodRequest, self).__init__(**kwargs)
        self.method_name = None  # type: Optional[str]


class ItemNonSetRequestBase(MethodRequest):
    """ItemNonSetRequestBase.

    You probably want to use the sub-classes and not this class directly. Known
    sub-classes are: LivePipelineActivateRequest, LivePipelineDeactivateRequest, LivePipelineDeleteRequest, LivePipelineGetRequest, PipelineTopologyDeleteRequest, PipelineTopologyGetRequest.

    Variables are only populated by the server, and will be ignored when sending a request.

    All required parameters must be populated in order to send to Azure.

    :ivar method_name: Required. method name.Constant filled by server.
    :vartype method_name: str
    :ivar api_version: api version. Default value: "1.0".
    :vartype api_version: str
    :param name: Required. method name.
    :type name: str
    """

    _validation = {
        'method_name': {'required': True, 'readonly': True},
        'api_version': {'constant': True},
        'name': {'required': True},
    }

    _attribute_map = {
        'method_name': {'key': 'methodName', 'type': 'str'},
        'api_version': {'key': '@apiVersion', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
    }

    _subtype_map = {
        'method_name': {'livePipelineActivate': 'LivePipelineActivateRequest', 'livePipelineDeactivate': 'LivePipelineDeactivateRequest', 'livePipelineDelete': 'LivePipelineDeleteRequest', 'livePipelineGet': 'LivePipelineGetRequest', 'pipelineTopologyDelete': 'PipelineTopologyDeleteRequest', 'pipelineTopologyGet': 'PipelineTopologyGetRequest'}
    }

    api_version = "1.0"

    def __init__(
        self,
        *,
        name: str,
        **kwargs
    ):
        super(ItemNonSetRequestBase, self).__init__(**kwargs)
        self.method_name = 'ItemNonSetRequestBase'  # type: str
        self.name = name


class LineCrossingProcessor(ProcessorNodeBase):
    """A node that accepts raw video as input, and detects when an object crosses a line.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param name: Required. The name for this processor node.
    :type name: str
    :param inputs: Required. An array of the names of the other nodes in the topology, the outputs
     of which are used as input for this processor node.
    :type inputs: list[~azure.media.videoanalyzer.edge.models.NodeInput]
    :param lines: Required. An array of lines used to compute line crossing events.
    :type lines: list[~azure.media.videoanalyzer.edge.models.NamedLineBase]
    """

    _validation = {
        'type': {'required': True},
        'name': {'required': True},
        'inputs': {'required': True},
        'lines': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'inputs': {'key': 'inputs', 'type': '[NodeInput]'},
        'lines': {'key': 'lines', 'type': '[NamedLineBase]'},
    }

    def __init__(
        self,
        *,
        name: str,
        inputs: List["NodeInput"],
        lines: List["NamedLineBase"],
        **kwargs
    ):
        super(LineCrossingProcessor, self).__init__(name=name, inputs=inputs, **kwargs)
        self.type = '#Microsoft.VideoAnalyzer.LineCrossingProcessor'  # type: str
        self.lines = lines


class LivePipeline(msrest.serialization.Model):
    """Represents a unique live pipeline.

    All required parameters must be populated in order to send to Azure.

    :param name: Required. The identifier for the live pipeline.
    :type name: str
    :param system_data: The system data for a resource.
    :type system_data: ~azure.media.videoanalyzer.edge.models.SystemData
    :param properties: The properties of the live pipeline.
    :type properties: ~azure.media.videoanalyzer.edge.models.LivePipelineProperties
    """

    _validation = {
        'name': {'required': True},
    }

    _attribute_map = {
        'name': {'key': 'name', 'type': 'str'},
        'system_data': {'key': 'systemData', 'type': 'SystemData'},
        'properties': {'key': 'properties', 'type': 'LivePipelineProperties'},
    }

    def __init__(
        self,
        *,
        name: str,
        system_data: Optional["SystemData"] = None,
        properties: Optional["LivePipelineProperties"] = None,
        **kwargs
    ):
        super(LivePipeline, self).__init__(**kwargs)
        self.name = name
        self.system_data = system_data
        self.properties = properties


class LivePipelineActivateRequest(ItemNonSetRequestBase):
    """Represents the livePipelineActivate request.

    Variables are only populated by the server, and will be ignored when sending a request.

    All required parameters must be populated in order to send to Azure.

    :ivar method_name: Required. method name.Constant filled by server.
    :vartype method_name: str
    :ivar api_version: api version. Default value: "1.0".
    :vartype api_version: str
    :param name: Required. method name.
    :type name: str
    """

    _validation = {
        'method_name': {'required': True, 'readonly': True},
        'api_version': {'constant': True},
        'name': {'required': True},
    }

    _attribute_map = {
        'method_name': {'key': 'methodName', 'type': 'str'},
        'api_version': {'key': '@apiVersion', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
    }

    api_version = "1.0"

    def __init__(
        self,
        *,
        name: str,
        **kwargs
    ):
        super(LivePipelineActivateRequest, self).__init__(name=name, **kwargs)
        self.method_name = 'livePipelineActivate'  # type: str


class LivePipelineCollection(msrest.serialization.Model):
    """A collection of streams.

    :param value: A collection of live pipelines.
    :type value: list[~azure.media.videoanalyzer.edge.models.LivePipeline]
    :param continuation_token: A continuation token to use in subsequent calls to enumerate through
     the live pipeline collection. This is used when the collection contains too many results to
     return in one response.
    :type continuation_token: str
    """

    _attribute_map = {
        'value': {'key': 'value', 'type': '[LivePipeline]'},
        'continuation_token': {'key': '@continuationToken', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        value: Optional[List["LivePipeline"]] = None,
        continuation_token: Optional[str] = None,
        **kwargs
    ):
        super(LivePipelineCollection, self).__init__(**kwargs)
        self.value = value
        self.continuation_token = continuation_token


class LivePipelineDeactivateRequest(ItemNonSetRequestBase):
    """Represents the livePipelineDeactivate request.

    Variables are only populated by the server, and will be ignored when sending a request.

    All required parameters must be populated in order to send to Azure.

    :ivar method_name: Required. method name.Constant filled by server.
    :vartype method_name: str
    :ivar api_version: api version. Default value: "1.0".
    :vartype api_version: str
    :param name: Required. method name.
    :type name: str
    """

    _validation = {
        'method_name': {'required': True, 'readonly': True},
        'api_version': {'constant': True},
        'name': {'required': True},
    }

    _attribute_map = {
        'method_name': {'key': 'methodName', 'type': 'str'},
        'api_version': {'key': '@apiVersion', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
    }

    api_version = "1.0"

    def __init__(
        self,
        *,
        name: str,
        **kwargs
    ):
        super(LivePipelineDeactivateRequest, self).__init__(name=name, **kwargs)
        self.method_name = 'livePipelineDeactivate'  # type: str


class LivePipelineDeleteRequest(ItemNonSetRequestBase):
    """Represents the livePipelineDelete request.

    Variables are only populated by the server, and will be ignored when sending a request.

    All required parameters must be populated in order to send to Azure.

    :ivar method_name: Required. method name.Constant filled by server.
    :vartype method_name: str
    :ivar api_version: api version. Default value: "1.0".
    :vartype api_version: str
    :param name: Required. method name.
    :type name: str
    """

    _validation = {
        'method_name': {'required': True, 'readonly': True},
        'api_version': {'constant': True},
        'name': {'required': True},
    }

    _attribute_map = {
        'method_name': {'key': 'methodName', 'type': 'str'},
        'api_version': {'key': '@apiVersion', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
    }

    api_version = "1.0"

    def __init__(
        self,
        *,
        name: str,
        **kwargs
    ):
        super(LivePipelineDeleteRequest, self).__init__(name=name, **kwargs)
        self.method_name = 'livePipelineDelete'  # type: str


class LivePipelineGetRequest(ItemNonSetRequestBase):
    """Represents the livePipelineGet request.

    Variables are only populated by the server, and will be ignored when sending a request.

    All required parameters must be populated in order to send to Azure.

    :ivar method_name: Required. method name.Constant filled by server.
    :vartype method_name: str
    :ivar api_version: api version. Default value: "1.0".
    :vartype api_version: str
    :param name: Required. method name.
    :type name: str
    """

    _validation = {
        'method_name': {'required': True, 'readonly': True},
        'api_version': {'constant': True},
        'name': {'required': True},
    }

    _attribute_map = {
        'method_name': {'key': 'methodName', 'type': 'str'},
        'api_version': {'key': '@apiVersion', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
    }

    api_version = "1.0"

    def __init__(
        self,
        *,
        name: str,
        **kwargs
    ):
        super(LivePipelineGetRequest, self).__init__(name=name, **kwargs)
        self.method_name = 'livePipelineGet'  # type: str


class LivePipelineListRequest(MethodRequest):
    """Represents the livePipelineList request.

    Variables are only populated by the server, and will be ignored when sending a request.

    All required parameters must be populated in order to send to Azure.

    :ivar method_name: Required. method name.Constant filled by server.
    :vartype method_name: str
    :ivar api_version: api version. Default value: "1.0".
    :vartype api_version: str
    """

    _validation = {
        'method_name': {'required': True, 'readonly': True},
        'api_version': {'constant': True},
    }

    _attribute_map = {
        'method_name': {'key': 'methodName', 'type': 'str'},
        'api_version': {'key': '@apiVersion', 'type': 'str'},
    }

    api_version = "1.0"

    def __init__(
        self,
        **kwargs
    ):
        super(LivePipelineListRequest, self).__init__(**kwargs)
        self.method_name = 'livePipelineList'  # type: str


class LivePipelineProperties(msrest.serialization.Model):
    """Properties of a live pipeline.

    :param description: An optional description for the live pipeline.
    :type description: str
    :param topology_name: The name of the pipeline topology that this live pipeline will run. A
     pipeline topology with this name should already have been set in the Edge module.
    :type topology_name: str
    :param parameters: List of one or more live pipeline parameters.
    :type parameters: list[~azure.media.videoanalyzer.edge.models.ParameterDefinition]
    :param state: Allowed states for a live pipeline. Possible values include: "inactive",
     "activating", "active", "deactivating".
    :type state: str or ~azure.media.videoanalyzer.edge.models.LivePipelineState
    """

    _attribute_map = {
        'description': {'key': 'description', 'type': 'str'},
        'topology_name': {'key': 'topologyName', 'type': 'str'},
        'parameters': {'key': 'parameters', 'type': '[ParameterDefinition]'},
        'state': {'key': 'state', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        description: Optional[str] = None,
        topology_name: Optional[str] = None,
        parameters: Optional[List["ParameterDefinition"]] = None,
        state: Optional[Union[str, "LivePipelineState"]] = None,
        **kwargs
    ):
        super(LivePipelineProperties, self).__init__(**kwargs)
        self.description = description
        self.topology_name = topology_name
        self.parameters = parameters
        self.state = state


class LivePipelineSetRequest(MethodRequest):
    """Represents the livePipelineSet request.

    Variables are only populated by the server, and will be ignored when sending a request.

    All required parameters must be populated in order to send to Azure.

    :ivar method_name: Required. method name.Constant filled by server.
    :vartype method_name: str
    :ivar api_version: api version. Default value: "1.0".
    :vartype api_version: str
    :param live_pipeline: Required. Represents a unique live pipeline.
    :type live_pipeline: ~azure.media.videoanalyzer.edge.models.LivePipeline
    """

    _validation = {
        'method_name': {'required': True, 'readonly': True},
        'api_version': {'constant': True},
        'live_pipeline': {'required': True},
    }

    _attribute_map = {
        'method_name': {'key': 'methodName', 'type': 'str'},
        'api_version': {'key': '@apiVersion', 'type': 'str'},
        'live_pipeline': {'key': 'livePipeline', 'type': 'LivePipeline'},
    }

    api_version = "1.0"

    def __init__(
        self,
        *,
        live_pipeline: "LivePipeline",
        **kwargs
    ):
        super(LivePipelineSetRequest, self).__init__(**kwargs)
        self.method_name = 'livePipelineSet'  # type: str
        self.live_pipeline = live_pipeline


class LivePipelineSetRequestBody(LivePipeline, MethodRequest):
    """Represents the livePipelineSet request body.

    Variables are only populated by the server, and will be ignored when sending a request.

    All required parameters must be populated in order to send to Azure.

    :ivar method_name: Required. method name.Constant filled by server.
    :vartype method_name: str
    :ivar api_version: api version. Default value: "1.0".
    :vartype api_version: str
    :param name: Required. The identifier for the live pipeline.
    :type name: str
    :param system_data: The system data for a resource.
    :type system_data: ~azure.media.videoanalyzer.edge.models.SystemData
    :param properties: The properties of the live pipeline.
    :type properties: ~azure.media.videoanalyzer.edge.models.LivePipelineProperties
    """

    _validation = {
        'method_name': {'required': True, 'readonly': True},
        'api_version': {'constant': True},
        'name': {'required': True},
    }

    _attribute_map = {
        'method_name': {'key': 'methodName', 'type': 'str'},
        'api_version': {'key': '@apiVersion', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'system_data': {'key': 'systemData', 'type': 'SystemData'},
        'properties': {'key': 'properties', 'type': 'LivePipelineProperties'},
    }

    api_version = "1.0"

    def __init__(
        self,
        *,
        name: str,
        system_data: Optional["SystemData"] = None,
        properties: Optional["LivePipelineProperties"] = None,
        **kwargs
    ):
        super(LivePipelineSetRequestBody, self).__init__(name=name, system_data=system_data, properties=properties, **kwargs)
        self.method_name = 'LivePipelineSetRequestBody'  # type: str
        self.method_name = 'LivePipelineSetRequestBody'  # type: str
        self.name = name
        self.system_data = system_data
        self.properties = properties


class MotionDetectionProcessor(ProcessorNodeBase):
    """A node that accepts raw video as input, and detects if there are moving objects present. If so, then it emits an event, and allows frames where motion was detected to pass through. Other frames are blocked/dropped.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param name: Required. The name for this processor node.
    :type name: str
    :param inputs: Required. An array of the names of the other nodes in the topology, the outputs
     of which are used as input for this processor node.
    :type inputs: list[~azure.media.videoanalyzer.edge.models.NodeInput]
    :param sensitivity: Enumeration that specifies the sensitivity of the motion detection
     processor. Possible values include: "low", "medium", "high".
    :type sensitivity: str or ~azure.media.videoanalyzer.edge.models.MotionDetectionSensitivity
    :param output_motion_region: Indicates whether the processor should detect and output the
     regions, within the video frame, where motion was detected. Default is true.
    :type output_motion_region: bool
    :param event_aggregation_window: Event aggregation window duration, or 0 for no aggregation.
    :type event_aggregation_window: str
    """

    _validation = {
        'type': {'required': True},
        'name': {'required': True},
        'inputs': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'inputs': {'key': 'inputs', 'type': '[NodeInput]'},
        'sensitivity': {'key': 'sensitivity', 'type': 'str'},
        'output_motion_region': {'key': 'outputMotionRegion', 'type': 'bool'},
        'event_aggregation_window': {'key': 'eventAggregationWindow', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        name: str,
        inputs: List["NodeInput"],
        sensitivity: Optional[Union[str, "MotionDetectionSensitivity"]] = None,
        output_motion_region: Optional[bool] = None,
        event_aggregation_window: Optional[str] = None,
        **kwargs
    ):
        super(MotionDetectionProcessor, self).__init__(name=name, inputs=inputs, **kwargs)
        self.type = '#Microsoft.VideoAnalyzer.MotionDetectionProcessor'  # type: str
        self.sensitivity = sensitivity
        self.output_motion_region = output_motion_region
        self.event_aggregation_window = event_aggregation_window


class NamedLineBase(msrest.serialization.Model):
    """Describes the named line.

    You probably want to use the sub-classes and not this class directly. Known
    sub-classes are: NamedLineString.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param name: Required. The name of the line.
    :type name: str
    """

    _validation = {
        'type': {'required': True},
        'name': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
    }

    _subtype_map = {
        'type': {'#Microsoft.VideoAnalyzer.NamedLineString': 'NamedLineString'}
    }

    def __init__(
        self,
        *,
        name: str,
        **kwargs
    ):
        super(NamedLineBase, self).__init__(**kwargs)
        self.type = None  # type: Optional[str]
        self.name = name


class NamedLineString(NamedLineBase):
    """Describes the start point and end point of a line in the frame.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param name: Required. The name of the line.
    :type name: str
    :param line: Required. Sets the properties of the line.
    :type line: str
    """

    _validation = {
        'type': {'required': True},
        'name': {'required': True},
        'line': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'line': {'key': 'line', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        name: str,
        line: str,
        **kwargs
    ):
        super(NamedLineString, self).__init__(name=name, **kwargs)
        self.type = '#Microsoft.VideoAnalyzer.NamedLineString'  # type: str
        self.line = line


class NamedPolygonBase(msrest.serialization.Model):
    """Describes the named polygon.

    You probably want to use the sub-classes and not this class directly. Known
    sub-classes are: NamedPolygonString.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param name: Required. The name of the polygon.
    :type name: str
    """

    _validation = {
        'type': {'required': True},
        'name': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
    }

    _subtype_map = {
        'type': {'#Microsoft.VideoAnalyzer.NamedPolygonString': 'NamedPolygonString'}
    }

    def __init__(
        self,
        *,
        name: str,
        **kwargs
    ):
        super(NamedPolygonBase, self).__init__(**kwargs)
        self.type = None  # type: Optional[str]
        self.name = name


class NamedPolygonString(NamedPolygonBase):
    """Describes a closed polygon in the frame.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param name: Required. The name of the polygon.
    :type name: str
    :param polygon: Required. Sets the properties of the polygon.
    :type polygon: str
    """

    _validation = {
        'type': {'required': True},
        'name': {'required': True},
        'polygon': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'polygon': {'key': 'polygon', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        name: str,
        polygon: str,
        **kwargs
    ):
        super(NamedPolygonString, self).__init__(name=name, **kwargs)
        self.type = '#Microsoft.VideoAnalyzer.NamedPolygonString'  # type: str
        self.polygon = polygon


class NodeInput(msrest.serialization.Model):
    """Represents the input to any node in a topology.

    All required parameters must be populated in order to send to Azure.

    :param node_name: Required. The name of another node in the pipeline topology, the output of
     which is used as input to this node.
    :type node_name: str
    :param output_selectors: Allows for the selection of particular streams from another node.
    :type output_selectors: list[~azure.media.videoanalyzer.edge.models.OutputSelector]
    """

    _validation = {
        'node_name': {'required': True},
    }

    _attribute_map = {
        'node_name': {'key': 'nodeName', 'type': 'str'},
        'output_selectors': {'key': 'outputSelectors', 'type': '[OutputSelector]'},
    }

    def __init__(
        self,
        *,
        node_name: str,
        output_selectors: Optional[List["OutputSelector"]] = None,
        **kwargs
    ):
        super(NodeInput, self).__init__(**kwargs)
        self.node_name = node_name
        self.output_selectors = output_selectors


class ObjectTrackingProcessor(ProcessorNodeBase):
    """A node that accepts raw video as input, and detects objects.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param name: Required. The name for this processor node.
    :type name: str
    :param inputs: Required. An array of the names of the other nodes in the topology, the outputs
     of which are used as input for this processor node.
    :type inputs: list[~azure.media.videoanalyzer.edge.models.NodeInput]
    :param accuracy: Enumeration that controls the accuracy of the tracker. Possible values
     include: "low", "medium", "high".
    :type accuracy: str or ~azure.media.videoanalyzer.edge.models.ObjectTrackingAccuracy
    """

    _validation = {
        'type': {'required': True},
        'name': {'required': True},
        'inputs': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'inputs': {'key': 'inputs', 'type': '[NodeInput]'},
        'accuracy': {'key': 'accuracy', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        name: str,
        inputs: List["NodeInput"],
        accuracy: Optional[Union[str, "ObjectTrackingAccuracy"]] = None,
        **kwargs
    ):
        super(ObjectTrackingProcessor, self).__init__(name=name, inputs=inputs, **kwargs)
        self.type = '#Microsoft.VideoAnalyzer.ObjectTrackingProcessor'  # type: str
        self.accuracy = accuracy


class OutputSelector(msrest.serialization.Model):
    """Allows for the selection of particular streams from another node.

    :param property: The stream property to compare with. Possible values include: "mediaType".
    :type property: str or ~azure.media.videoanalyzer.edge.models.OutputSelectorProperty
    :param operator: The operator to compare streams by. Possible values include: "is", "isNot".
    :type operator: str or ~azure.media.videoanalyzer.edge.models.OutputSelectorOperator
    :param value: Value to compare against.
    :type value: str
    """

    _attribute_map = {
        'property': {'key': 'property', 'type': 'str'},
        'operator': {'key': 'operator', 'type': 'str'},
        'value': {'key': 'value', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        property: Optional[Union[str, "OutputSelectorProperty"]] = None,
        operator: Optional[Union[str, "OutputSelectorOperator"]] = None,
        value: Optional[str] = None,
        **kwargs
    ):
        super(OutputSelector, self).__init__(**kwargs)
        self.property = property
        self.operator = operator
        self.value = value


class ParameterDeclaration(msrest.serialization.Model):
    """The declaration of a parameter in the pipeline topology. A topology can be authored with parameters. Then, during live pipeline creation, the value for those parameters can be specified. This allows the same pipeline topology to be used as a blueprint for multiple live pipelines with different values for the parameters.

    All required parameters must be populated in order to send to Azure.

    :param name: Required. The name of the parameter.
    :type name: str
    :param type: Required. The type of the parameter. Possible values include: "string",
     "secretString", "int", "double", "bool".
    :type type: str or ~azure.media.videoanalyzer.edge.models.ParameterType
    :param description: Description of the parameter.
    :type description: str
    :param default: The default value for the parameter to be used if the live pipeline does not
     specify a value.
    :type default: str
    """

    _validation = {
        'name': {'required': True, 'max_length': 64, 'min_length': 0},
        'type': {'required': True},
    }

    _attribute_map = {
        'name': {'key': 'name', 'type': 'str'},
        'type': {'key': 'type', 'type': 'str'},
        'description': {'key': 'description', 'type': 'str'},
        'default': {'key': 'default', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        name: str,
        type: Union[str, "ParameterType"],
        description: Optional[str] = None,
        default: Optional[str] = None,
        **kwargs
    ):
        super(ParameterDeclaration, self).__init__(**kwargs)
        self.name = name
        self.type = type
        self.description = description
        self.default = default


class ParameterDefinition(msrest.serialization.Model):
    """A key-value pair. A pipeline topology allows certain values to be parameterized. When a live pipeline is created, the parameters are supplied with arguments specific to that instance. This allows the same pipeline topology to be used as a blueprint for multiple streams with different values for the parameters.

    All required parameters must be populated in order to send to Azure.

    :param name: Required. The name of the parameter defined in the pipeline topology.
    :type name: str
    :param value: The value to supply for the named parameter defined in the pipeline topology.
    :type value: str
    """

    _validation = {
        'name': {'required': True},
    }

    _attribute_map = {
        'name': {'key': 'name', 'type': 'str'},
        'value': {'key': 'value', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        name: str,
        value: Optional[str] = None,
        **kwargs
    ):
        super(ParameterDefinition, self).__init__(**kwargs)
        self.name = name
        self.value = value


class PemCertificateList(CertificateSource):
    """A list of PEM formatted certificates.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param certificates: Required. PEM formatted public certificates one per entry.
    :type certificates: list[str]
    """

    _validation = {
        'type': {'required': True},
        'certificates': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'certificates': {'key': 'certificates', 'type': '[str]'},
    }

    def __init__(
        self,
        *,
        certificates: List[str],
        **kwargs
    ):
        super(PemCertificateList, self).__init__(**kwargs)
        self.type = '#Microsoft.VideoAnalyzer.PemCertificateList'  # type: str
        self.certificates = certificates


class PipelineTopology(msrest.serialization.Model):
    """The definition of a pipeline topology.

    All required parameters must be populated in order to send to Azure.

    :param name: Required. The identifier for the pipeline topology.
    :type name: str
    :param system_data: The system data for a resource.
    :type system_data: ~azure.media.videoanalyzer.edge.models.SystemData
    :param properties: The properties of the pipeline topology.
    :type properties: ~azure.media.videoanalyzer.edge.models.PipelineTopologyProperties
    """

    _validation = {
        'name': {'required': True},
    }

    _attribute_map = {
        'name': {'key': 'name', 'type': 'str'},
        'system_data': {'key': 'systemData', 'type': 'SystemData'},
        'properties': {'key': 'properties', 'type': 'PipelineTopologyProperties'},
    }

    def __init__(
        self,
        *,
        name: str,
        system_data: Optional["SystemData"] = None,
        properties: Optional["PipelineTopologyProperties"] = None,
        **kwargs
    ):
        super(PipelineTopology, self).__init__(**kwargs)
        self.name = name
        self.system_data = system_data
        self.properties = properties


class PipelineTopologyCollection(msrest.serialization.Model):
    """A collection of pipeline topologies.

    :param value: A collection of pipeline topologies.
    :type value: list[~azure.media.videoanalyzer.edge.models.PipelineTopology]
    :param continuation_token: A continuation token to use in subsequent calls to enumerate through
     the pipeline topology collection. This is used when the collection contains too many results to
     return in one response.
    :type continuation_token: str
    """

    _attribute_map = {
        'value': {'key': 'value', 'type': '[PipelineTopology]'},
        'continuation_token': {'key': '@continuationToken', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        value: Optional[List["PipelineTopology"]] = None,
        continuation_token: Optional[str] = None,
        **kwargs
    ):
        super(PipelineTopologyCollection, self).__init__(**kwargs)
        self.value = value
        self.continuation_token = continuation_token


class PipelineTopologyDeleteRequest(ItemNonSetRequestBase):
    """Represents the pipelineTopologyDelete request.

    Variables are only populated by the server, and will be ignored when sending a request.

    All required parameters must be populated in order to send to Azure.

    :ivar method_name: Required. method name.Constant filled by server.
    :vartype method_name: str
    :ivar api_version: api version. Default value: "1.0".
    :vartype api_version: str
    :param name: Required. method name.
    :type name: str
    """

    _validation = {
        'method_name': {'required': True, 'readonly': True},
        'api_version': {'constant': True},
        'name': {'required': True},
    }

    _attribute_map = {
        'method_name': {'key': 'methodName', 'type': 'str'},
        'api_version': {'key': '@apiVersion', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
    }

    api_version = "1.0"

    def __init__(
        self,
        *,
        name: str,
        **kwargs
    ):
        super(PipelineTopologyDeleteRequest, self).__init__(name=name, **kwargs)
        self.method_name = 'pipelineTopologyDelete'  # type: str


class PipelineTopologyGetRequest(ItemNonSetRequestBase):
    """Represents the pipelineTopologyGet request.

    Variables are only populated by the server, and will be ignored when sending a request.

    All required parameters must be populated in order to send to Azure.

    :ivar method_name: Required. method name.Constant filled by server.
    :vartype method_name: str
    :ivar api_version: api version. Default value: "1.0".
    :vartype api_version: str
    :param name: Required. method name.
    :type name: str
    """

    _validation = {
        'method_name': {'required': True, 'readonly': True},
        'api_version': {'constant': True},
        'name': {'required': True},
    }

    _attribute_map = {
        'method_name': {'key': 'methodName', 'type': 'str'},
        'api_version': {'key': '@apiVersion', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
    }

    api_version = "1.0"

    def __init__(
        self,
        *,
        name: str,
        **kwargs
    ):
        super(PipelineTopologyGetRequest, self).__init__(name=name, **kwargs)
        self.method_name = 'pipelineTopologyGet'  # type: str


class PipelineTopologyListRequest(MethodRequest):
    """Represents the pipelineTopologyList request.

    Variables are only populated by the server, and will be ignored when sending a request.

    All required parameters must be populated in order to send to Azure.

    :ivar method_name: Required. method name.Constant filled by server.
    :vartype method_name: str
    :ivar api_version: api version. Default value: "1.0".
    :vartype api_version: str
    """

    _validation = {
        'method_name': {'required': True, 'readonly': True},
        'api_version': {'constant': True},
    }

    _attribute_map = {
        'method_name': {'key': 'methodName', 'type': 'str'},
        'api_version': {'key': '@apiVersion', 'type': 'str'},
    }

    api_version = "1.0"

    def __init__(
        self,
        **kwargs
    ):
        super(PipelineTopologyListRequest, self).__init__(**kwargs)
        self.method_name = 'pipelineTopologyList'  # type: str


class PipelineTopologyProperties(msrest.serialization.Model):
    """A description of the properties of a pipeline topology.

    :param description: A description of a pipeline topology. It is recommended to use this to
     describe the expected use of the pipeline topology.
    :type description: str
    :param parameters: The list of parameters defined in the pipeline topology. The value for these
     parameters are supplied by streams of this pipeline topology.
    :type parameters: list[~azure.media.videoanalyzer.edge.models.ParameterDeclaration]
    :param sources: The list of source nodes in this pipeline topology.
    :type sources: list[~azure.media.videoanalyzer.edge.models.SourceNodeBase]
    :param processors: The list of processor nodes in this pipeline topology.
    :type processors: list[~azure.media.videoanalyzer.edge.models.ProcessorNodeBase]
    :param sinks: The list of sink nodes in this pipeline topology.
    :type sinks: list[~azure.media.videoanalyzer.edge.models.SinkNodeBase]
    """

    _attribute_map = {
        'description': {'key': 'description', 'type': 'str'},
        'parameters': {'key': 'parameters', 'type': '[ParameterDeclaration]'},
        'sources': {'key': 'sources', 'type': '[SourceNodeBase]'},
        'processors': {'key': 'processors', 'type': '[ProcessorNodeBase]'},
        'sinks': {'key': 'sinks', 'type': '[SinkNodeBase]'},
    }

    def __init__(
        self,
        *,
        description: Optional[str] = None,
        parameters: Optional[List["ParameterDeclaration"]] = None,
        sources: Optional[List["SourceNodeBase"]] = None,
        processors: Optional[List["ProcessorNodeBase"]] = None,
        sinks: Optional[List["SinkNodeBase"]] = None,
        **kwargs
    ):
        super(PipelineTopologyProperties, self).__init__(**kwargs)
        self.description = description
        self.parameters = parameters
        self.sources = sources
        self.processors = processors
        self.sinks = sinks


class PipelineTopologySetRequest(MethodRequest):
    """Represents the pipelineTopologySet request.

    Variables are only populated by the server, and will be ignored when sending a request.

    All required parameters must be populated in order to send to Azure.

    :ivar method_name: Required. method name.Constant filled by server.
    :vartype method_name: str
    :ivar api_version: api version. Default value: "1.0".
    :vartype api_version: str
    :param pipeline_topology: Required. The definition of a pipeline topology.
    :type pipeline_topology: ~azure.media.videoanalyzer.edge.models.PipelineTopology
    """

    _validation = {
        'method_name': {'required': True, 'readonly': True},
        'api_version': {'constant': True},
        'pipeline_topology': {'required': True},
    }

    _attribute_map = {
        'method_name': {'key': 'methodName', 'type': 'str'},
        'api_version': {'key': '@apiVersion', 'type': 'str'},
        'pipeline_topology': {'key': 'pipelineTopology', 'type': 'PipelineTopology'},
    }

    api_version = "1.0"

    def __init__(
        self,
        *,
        pipeline_topology: "PipelineTopology",
        **kwargs
    ):
        super(PipelineTopologySetRequest, self).__init__(**kwargs)
        self.method_name = 'pipelineTopologySet'  # type: str
        self.pipeline_topology = pipeline_topology


class PipelineTopologySetRequestBody(PipelineTopology, MethodRequest):
    """Represents the pipelineTopologySet request body.

    Variables are only populated by the server, and will be ignored when sending a request.

    All required parameters must be populated in order to send to Azure.

    :ivar method_name: Required. method name.Constant filled by server.
    :vartype method_name: str
    :ivar api_version: api version. Default value: "1.0".
    :vartype api_version: str
    :param name: Required. The identifier for the pipeline topology.
    :type name: str
    :param system_data: The system data for a resource.
    :type system_data: ~azure.media.videoanalyzer.edge.models.SystemData
    :param properties: The properties of the pipeline topology.
    :type properties: ~azure.media.videoanalyzer.edge.models.PipelineTopologyProperties
    """

    _validation = {
        'method_name': {'required': True, 'readonly': True},
        'api_version': {'constant': True},
        'name': {'required': True},
    }

    _attribute_map = {
        'method_name': {'key': 'methodName', 'type': 'str'},
        'api_version': {'key': '@apiVersion', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'system_data': {'key': 'systemData', 'type': 'SystemData'},
        'properties': {'key': 'properties', 'type': 'PipelineTopologyProperties'},
    }

    api_version = "1.0"

    def __init__(
        self,
        *,
        name: str,
        system_data: Optional["SystemData"] = None,
        properties: Optional["PipelineTopologyProperties"] = None,
        **kwargs
    ):
        super(PipelineTopologySetRequestBody, self).__init__(name=name, system_data=system_data, properties=properties, **kwargs)
        self.method_name = 'PipelineTopologySetRequestBody'  # type: str
        self.method_name = 'PipelineTopologySetRequestBody'  # type: str
        self.name = name
        self.system_data = system_data
        self.properties = properties


class RtspSource(SourceNodeBase):
    """Enables a pipeline topology to capture media from a RTSP server.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The type of the source node. The discriminator for derived
     types.Constant filled by server.
    :type type: str
    :param name: Required. The name to be used for this source node.
    :type name: str
    :param transport: Underlying RTSP transport. This is used to enable or disable HTTP tunneling.
     Possible values include: "http", "tcp".
    :type transport: str or ~azure.media.videoanalyzer.edge.models.RtspTransport
    :param endpoint: Required. RTSP endpoint of the stream that is being connected to.
    :type endpoint: ~azure.media.videoanalyzer.edge.models.EndpointBase
    """

    _validation = {
        'type': {'required': True},
        'name': {'required': True},
        'endpoint': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'transport': {'key': 'transport', 'type': 'str'},
        'endpoint': {'key': 'endpoint', 'type': 'EndpointBase'},
    }

    def __init__(
        self,
        *,
        name: str,
        endpoint: "EndpointBase",
        transport: Optional[Union[str, "RtspTransport"]] = None,
        **kwargs
    ):
        super(RtspSource, self).__init__(name=name, **kwargs)
        self.type = '#Microsoft.VideoAnalyzer.RtspSource'  # type: str
        self.transport = transport
        self.endpoint = endpoint


class SamplingOptions(msrest.serialization.Model):
    """Describes the properties of a sample.

    :param skip_samples_without_annotation: If true, limits the samples submitted to the extension
     to only samples which have associated inference(s).
    :type skip_samples_without_annotation: str
    :param maximum_samples_per_second: Maximum rate of samples submitted to the extension.
    :type maximum_samples_per_second: str
    """

    _attribute_map = {
        'skip_samples_without_annotation': {'key': 'skipSamplesWithoutAnnotation', 'type': 'str'},
        'maximum_samples_per_second': {'key': 'maximumSamplesPerSecond', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        skip_samples_without_annotation: Optional[str] = None,
        maximum_samples_per_second: Optional[str] = None,
        **kwargs
    ):
        super(SamplingOptions, self).__init__(**kwargs)
        self.skip_samples_without_annotation = skip_samples_without_annotation
        self.maximum_samples_per_second = maximum_samples_per_second


class SignalGateProcessor(ProcessorNodeBase):
    """A signal gate determines when to block (gate) incoming media, and when to allow it through. It gathers input events over the activationEvaluationWindow, and determines whether to open or close the gate.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param name: Required. The name for this processor node.
    :type name: str
    :param inputs: Required. An array of the names of the other nodes in the topology, the outputs
     of which are used as input for this processor node.
    :type inputs: list[~azure.media.videoanalyzer.edge.models.NodeInput]
    :param activation_evaluation_window: The period of time over which the gate gathers input
     events before evaluating them.
    :type activation_evaluation_window: str
    :param activation_signal_offset: Signal offset once the gate is activated (can be negative). It
     is an offset between the time the event is received, and the timestamp of the first media
     sample (eg. video frame) that is allowed through by the gate.
    :type activation_signal_offset: str
    :param minimum_activation_time: The minimum period for which the gate remains open in the
     absence of subsequent triggers (events).
    :type minimum_activation_time: str
    :param maximum_activation_time: The maximum period for which the gate remains open in the
     presence of subsequent events.
    :type maximum_activation_time: str
    """

    _validation = {
        'type': {'required': True},
        'name': {'required': True},
        'inputs': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'inputs': {'key': 'inputs', 'type': '[NodeInput]'},
        'activation_evaluation_window': {'key': 'activationEvaluationWindow', 'type': 'str'},
        'activation_signal_offset': {'key': 'activationSignalOffset', 'type': 'str'},
        'minimum_activation_time': {'key': 'minimumActivationTime', 'type': 'str'},
        'maximum_activation_time': {'key': 'maximumActivationTime', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        name: str,
        inputs: List["NodeInput"],
        activation_evaluation_window: Optional[str] = None,
        activation_signal_offset: Optional[str] = None,
        minimum_activation_time: Optional[str] = None,
        maximum_activation_time: Optional[str] = None,
        **kwargs
    ):
        super(SignalGateProcessor, self).__init__(name=name, inputs=inputs, **kwargs)
        self.type = '#Microsoft.VideoAnalyzer.SignalGateProcessor'  # type: str
        self.activation_evaluation_window = activation_evaluation_window
        self.activation_signal_offset = activation_signal_offset
        self.minimum_activation_time = minimum_activation_time
        self.maximum_activation_time = maximum_activation_time


class SpatialAnalysisOperationBase(msrest.serialization.Model):
    """Defines the Spatial Analysis operation to be used in the Cognitive Services Vision processor.

    You probably want to use the sub-classes and not this class directly. Known
    sub-classes are: SpatialAnalysisCustomOperation, SpatialAnalysisTypedOperationBase.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    """

    _validation = {
        'type': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
    }

    _subtype_map = {
        'type': {'#Microsoft.VideoAnalyzer.SpatialAnalysisCustomOperation': 'SpatialAnalysisCustomOperation', 'SpatialAnalysisTypedOperationBase': 'SpatialAnalysisTypedOperationBase'}
    }

    def __init__(
        self,
        **kwargs
    ):
        super(SpatialAnalysisOperationBase, self).__init__(**kwargs)
        self.type = None  # type: Optional[str]


class SpatialAnalysisCustomOperation(SpatialAnalysisOperationBase):
    """Defines a custom Spatial Analysis operation to be used in the Cognitive Services Vision processor.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param extension_configuration: Required. Custom configuration to pass to the Cognitive
     Services Vision processor.
    :type extension_configuration: str
    """

    _validation = {
        'type': {'required': True},
        'extension_configuration': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'extension_configuration': {'key': 'extensionConfiguration', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        extension_configuration: str,
        **kwargs
    ):
        super(SpatialAnalysisCustomOperation, self).__init__(**kwargs)
        self.type = '#Microsoft.VideoAnalyzer.SpatialAnalysisCustomOperation'  # type: str
        self.extension_configuration = extension_configuration


class SpatialAnalysisOperationEventBase(msrest.serialization.Model):
    """Defines a Spatial Analysis operation eventing configuration.

    :param threshold: The event threshold.
    :type threshold: str
    :param focus: The operation focus type. Possible values include: "center", "bottomCenter",
     "footprint".
    :type focus: str or ~azure.media.videoanalyzer.edge.models.SpatialAnalysisOperationFocus
    """

    _attribute_map = {
        'threshold': {'key': 'threshold', 'type': 'str'},
        'focus': {'key': 'focus', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        threshold: Optional[str] = None,
        focus: Optional[Union[str, "SpatialAnalysisOperationFocus"]] = None,
        **kwargs
    ):
        super(SpatialAnalysisOperationEventBase, self).__init__(**kwargs)
        self.threshold = threshold
        self.focus = focus


class SpatialAnalysisPersonCountEvent(SpatialAnalysisOperationEventBase):
    """Defines a Spatial Analysis Person Count operation eventing configuration.

    :param threshold: The event threshold.
    :type threshold: str
    :param focus: The operation focus type. Possible values include: "center", "bottomCenter",
     "footprint".
    :type focus: str or ~azure.media.videoanalyzer.edge.models.SpatialAnalysisOperationFocus
    :param trigger: The event trigger type. Possible values include: "event", "interval".
    :type trigger: str or
     ~azure.media.videoanalyzer.edge.models.SpatialAnalysisPersonCountEventTrigger
    :param output_frequency: The event or interval output frequency.
    :type output_frequency: str
    """

    _attribute_map = {
        'threshold': {'key': 'threshold', 'type': 'str'},
        'focus': {'key': 'focus', 'type': 'str'},
        'trigger': {'key': 'trigger', 'type': 'str'},
        'output_frequency': {'key': 'outputFrequency', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        threshold: Optional[str] = None,
        focus: Optional[Union[str, "SpatialAnalysisOperationFocus"]] = None,
        trigger: Optional[Union[str, "SpatialAnalysisPersonCountEventTrigger"]] = None,
        output_frequency: Optional[str] = None,
        **kwargs
    ):
        super(SpatialAnalysisPersonCountEvent, self).__init__(threshold=threshold, focus=focus, **kwargs)
        self.trigger = trigger
        self.output_frequency = output_frequency


class SpatialAnalysisTypedOperationBase(SpatialAnalysisOperationBase):
    """Defines a typed Spatial Analysis operation to be used in the Cognitive Services Vision processor.

    You probably want to use the sub-classes and not this class directly. Known
    sub-classes are: SpatialAnalysisPersonCountOperation, SpatialAnalysisPersonDistanceOperation, SpatialAnalysisPersonLineCrossingOperation, SpatialAnalysisPersonZoneCrossingOperation.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param debug: Enables debugging for the Spatial Analysis operation.
    :type debug: str
    :param camera_configuration: Advanced camera configuration.
    :type camera_configuration: str
    :param detector_node_configuration: Advanced detector node configuration.
    :type detector_node_configuration: str
    :param enable_face_mask_classifier: Enables face mask detection.
    :type enable_face_mask_classifier: str
    """

    _validation = {
        'type': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'debug': {'key': 'debug', 'type': 'str'},
        'camera_configuration': {'key': 'cameraConfiguration', 'type': 'str'},
        'detector_node_configuration': {'key': 'detectorNodeConfiguration', 'type': 'str'},
        'enable_face_mask_classifier': {'key': 'enableFaceMaskClassifier', 'type': 'str'},
    }

    _subtype_map = {
        'type': {'#Microsoft.VideoAnalyzer.SpatialAnalysisPersonCountOperation': 'SpatialAnalysisPersonCountOperation', '#Microsoft.VideoAnalyzer.SpatialAnalysisPersonDistanceOperation': 'SpatialAnalysisPersonDistanceOperation', '#Microsoft.VideoAnalyzer.SpatialAnalysisPersonLineCrossingOperation': 'SpatialAnalysisPersonLineCrossingOperation', '#Microsoft.VideoAnalyzer.SpatialAnalysisPersonZoneCrossingOperation': 'SpatialAnalysisPersonZoneCrossingOperation'}
    }

    def __init__(
        self,
        *,
        debug: Optional[str] = None,
        camera_configuration: Optional[str] = None,
        detector_node_configuration: Optional[str] = None,
        enable_face_mask_classifier: Optional[str] = None,
        **kwargs
    ):
        super(SpatialAnalysisTypedOperationBase, self).__init__(**kwargs)
        self.type = 'SpatialAnalysisTypedOperationBase'  # type: str
        self.debug = debug
        self.camera_configuration = camera_configuration
        self.detector_node_configuration = detector_node_configuration
        self.enable_face_mask_classifier = enable_face_mask_classifier


class SpatialAnalysisPersonCountOperation(SpatialAnalysisTypedOperationBase):
    """Defines a Spatial Analysis Person Count operation to be used in the Cognitive Services Vision processor.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param debug: Enables debugging for the Spatial Analysis operation.
    :type debug: str
    :param camera_configuration: Advanced camera configuration.
    :type camera_configuration: str
    :param detector_node_configuration: Advanced detector node configuration.
    :type detector_node_configuration: str
    :param enable_face_mask_classifier: Enables face mask detection.
    :type enable_face_mask_classifier: str
    :param zones: Required. The list of zones and optional events.
    :type zones: list[~azure.media.videoanalyzer.edge.models.SpatialAnalysisPersonCountZoneEvents]
    """

    _validation = {
        'type': {'required': True},
        'zones': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'debug': {'key': 'debug', 'type': 'str'},
        'camera_configuration': {'key': 'cameraConfiguration', 'type': 'str'},
        'detector_node_configuration': {'key': 'detectorNodeConfiguration', 'type': 'str'},
        'enable_face_mask_classifier': {'key': 'enableFaceMaskClassifier', 'type': 'str'},
        'zones': {'key': 'zones', 'type': '[SpatialAnalysisPersonCountZoneEvents]'},
    }

    def __init__(
        self,
        *,
        zones: List["SpatialAnalysisPersonCountZoneEvents"],
        debug: Optional[str] = None,
        camera_configuration: Optional[str] = None,
        detector_node_configuration: Optional[str] = None,
        enable_face_mask_classifier: Optional[str] = None,
        **kwargs
    ):
        super(SpatialAnalysisPersonCountOperation, self).__init__(debug=debug, camera_configuration=camera_configuration, detector_node_configuration=detector_node_configuration, enable_face_mask_classifier=enable_face_mask_classifier, **kwargs)
        self.type = '#Microsoft.VideoAnalyzer.SpatialAnalysisPersonCountOperation'  # type: str
        self.zones = zones


class SpatialAnalysisPersonCountZoneEvents(msrest.serialization.Model):
    """SpatialAnalysisPersonCountZoneEvents.

    All required parameters must be populated in order to send to Azure.

    :param zone: Required. The named zone.
    :type zone: ~azure.media.videoanalyzer.edge.models.NamedPolygonBase
    :param events: The event configuration.
    :type events: list[~azure.media.videoanalyzer.edge.models.SpatialAnalysisPersonCountEvent]
    """

    _validation = {
        'zone': {'required': True},
    }

    _attribute_map = {
        'zone': {'key': 'zone', 'type': 'NamedPolygonBase'},
        'events': {'key': 'events', 'type': '[SpatialAnalysisPersonCountEvent]'},
    }

    def __init__(
        self,
        *,
        zone: "NamedPolygonBase",
        events: Optional[List["SpatialAnalysisPersonCountEvent"]] = None,
        **kwargs
    ):
        super(SpatialAnalysisPersonCountZoneEvents, self).__init__(**kwargs)
        self.zone = zone
        self.events = events


class SpatialAnalysisPersonDistanceEvent(SpatialAnalysisOperationEventBase):
    """Defines a Spatial Analysis Person Distance operation eventing configuration.

    :param threshold: The event threshold.
    :type threshold: str
    :param focus: The operation focus type. Possible values include: "center", "bottomCenter",
     "footprint".
    :type focus: str or ~azure.media.videoanalyzer.edge.models.SpatialAnalysisOperationFocus
    :param trigger: The event trigger type. Possible values include: "event", "interval".
    :type trigger: str or
     ~azure.media.videoanalyzer.edge.models.SpatialAnalysisPersonDistanceEventTrigger
    :param output_frequency: The event or interval output frequency.
    :type output_frequency: str
    :param minimum_distance_threshold: The minimum distance threshold.
    :type minimum_distance_threshold: str
    :param maximum_distance_threshold: The maximum distance threshold.
    :type maximum_distance_threshold: str
    """

    _attribute_map = {
        'threshold': {'key': 'threshold', 'type': 'str'},
        'focus': {'key': 'focus', 'type': 'str'},
        'trigger': {'key': 'trigger', 'type': 'str'},
        'output_frequency': {'key': 'outputFrequency', 'type': 'str'},
        'minimum_distance_threshold': {'key': 'minimumDistanceThreshold', 'type': 'str'},
        'maximum_distance_threshold': {'key': 'maximumDistanceThreshold', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        threshold: Optional[str] = None,
        focus: Optional[Union[str, "SpatialAnalysisOperationFocus"]] = None,
        trigger: Optional[Union[str, "SpatialAnalysisPersonDistanceEventTrigger"]] = None,
        output_frequency: Optional[str] = None,
        minimum_distance_threshold: Optional[str] = None,
        maximum_distance_threshold: Optional[str] = None,
        **kwargs
    ):
        super(SpatialAnalysisPersonDistanceEvent, self).__init__(threshold=threshold, focus=focus, **kwargs)
        self.trigger = trigger
        self.output_frequency = output_frequency
        self.minimum_distance_threshold = minimum_distance_threshold
        self.maximum_distance_threshold = maximum_distance_threshold


class SpatialAnalysisPersonDistanceOperation(SpatialAnalysisTypedOperationBase):
    """Defines a Spatial Analysis Person Distance operation to be used in the Cognitive Services Vision processor.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param debug: Enables debugging for the Spatial Analysis operation.
    :type debug: str
    :param camera_configuration: Advanced camera configuration.
    :type camera_configuration: str
    :param detector_node_configuration: Advanced detector node configuration.
    :type detector_node_configuration: str
    :param enable_face_mask_classifier: Enables face mask detection.
    :type enable_face_mask_classifier: str
    :param zones: Required. The list of zones with optional events.
    :type zones:
     list[~azure.media.videoanalyzer.edge.models.SpatialAnalysisPersonDistanceZoneEvents]
    """

    _validation = {
        'type': {'required': True},
        'zones': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'debug': {'key': 'debug', 'type': 'str'},
        'camera_configuration': {'key': 'cameraConfiguration', 'type': 'str'},
        'detector_node_configuration': {'key': 'detectorNodeConfiguration', 'type': 'str'},
        'enable_face_mask_classifier': {'key': 'enableFaceMaskClassifier', 'type': 'str'},
        'zones': {'key': 'zones', 'type': '[SpatialAnalysisPersonDistanceZoneEvents]'},
    }

    def __init__(
        self,
        *,
        zones: List["SpatialAnalysisPersonDistanceZoneEvents"],
        debug: Optional[str] = None,
        camera_configuration: Optional[str] = None,
        detector_node_configuration: Optional[str] = None,
        enable_face_mask_classifier: Optional[str] = None,
        **kwargs
    ):
        super(SpatialAnalysisPersonDistanceOperation, self).__init__(debug=debug, camera_configuration=camera_configuration, detector_node_configuration=detector_node_configuration, enable_face_mask_classifier=enable_face_mask_classifier, **kwargs)
        self.type = '#Microsoft.VideoAnalyzer.SpatialAnalysisPersonDistanceOperation'  # type: str
        self.zones = zones


class SpatialAnalysisPersonDistanceZoneEvents(msrest.serialization.Model):
    """SpatialAnalysisPersonDistanceZoneEvents.

    All required parameters must be populated in order to send to Azure.

    :param zone: Required. The named zone.
    :type zone: ~azure.media.videoanalyzer.edge.models.NamedPolygonBase
    :param events: The event configuration.
    :type events: list[~azure.media.videoanalyzer.edge.models.SpatialAnalysisPersonDistanceEvent]
    """

    _validation = {
        'zone': {'required': True},
    }

    _attribute_map = {
        'zone': {'key': 'zone', 'type': 'NamedPolygonBase'},
        'events': {'key': 'events', 'type': '[SpatialAnalysisPersonDistanceEvent]'},
    }

    def __init__(
        self,
        *,
        zone: "NamedPolygonBase",
        events: Optional[List["SpatialAnalysisPersonDistanceEvent"]] = None,
        **kwargs
    ):
        super(SpatialAnalysisPersonDistanceZoneEvents, self).__init__(**kwargs)
        self.zone = zone
        self.events = events


class SpatialAnalysisPersonLineCrossingEvent(SpatialAnalysisOperationEventBase):
    """Defines a Spatial Analysis Person Line Crossing operation eventing configuration.

    :param threshold: The event threshold.
    :type threshold: str
    :param focus: The operation focus type. Possible values include: "center", "bottomCenter",
     "footprint".
    :type focus: str or ~azure.media.videoanalyzer.edge.models.SpatialAnalysisOperationFocus
    """

    _attribute_map = {
        'threshold': {'key': 'threshold', 'type': 'str'},
        'focus': {'key': 'focus', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        threshold: Optional[str] = None,
        focus: Optional[Union[str, "SpatialAnalysisOperationFocus"]] = None,
        **kwargs
    ):
        super(SpatialAnalysisPersonLineCrossingEvent, self).__init__(threshold=threshold, focus=focus, **kwargs)


class SpatialAnalysisPersonLineCrossingLineEvents(msrest.serialization.Model):
    """SpatialAnalysisPersonLineCrossingLineEvents.

    All required parameters must be populated in order to send to Azure.

    :param line: Required. The named line.
    :type line: ~azure.media.videoanalyzer.edge.models.NamedLineBase
    :param events: The event configuration.
    :type events:
     list[~azure.media.videoanalyzer.edge.models.SpatialAnalysisPersonLineCrossingEvent]
    """

    _validation = {
        'line': {'required': True},
    }

    _attribute_map = {
        'line': {'key': 'line', 'type': 'NamedLineBase'},
        'events': {'key': 'events', 'type': '[SpatialAnalysisPersonLineCrossingEvent]'},
    }

    def __init__(
        self,
        *,
        line: "NamedLineBase",
        events: Optional[List["SpatialAnalysisPersonLineCrossingEvent"]] = None,
        **kwargs
    ):
        super(SpatialAnalysisPersonLineCrossingLineEvents, self).__init__(**kwargs)
        self.line = line
        self.events = events


class SpatialAnalysisPersonLineCrossingOperation(SpatialAnalysisTypedOperationBase):
    """Defines a Spatial Analysis Person Line Crossing operation to be used in the Cognitive Services Vision processor.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param debug: Enables debugging for the Spatial Analysis operation.
    :type debug: str
    :param camera_configuration: Advanced camera configuration.
    :type camera_configuration: str
    :param detector_node_configuration: Advanced detector node configuration.
    :type detector_node_configuration: str
    :param enable_face_mask_classifier: Enables face mask detection.
    :type enable_face_mask_classifier: str
    :param lines: Required. The list of lines with optional events.
    :type lines:
     list[~azure.media.videoanalyzer.edge.models.SpatialAnalysisPersonLineCrossingLineEvents]
    """

    _validation = {
        'type': {'required': True},
        'lines': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'debug': {'key': 'debug', 'type': 'str'},
        'camera_configuration': {'key': 'cameraConfiguration', 'type': 'str'},
        'detector_node_configuration': {'key': 'detectorNodeConfiguration', 'type': 'str'},
        'enable_face_mask_classifier': {'key': 'enableFaceMaskClassifier', 'type': 'str'},
        'lines': {'key': 'lines', 'type': '[SpatialAnalysisPersonLineCrossingLineEvents]'},
    }

    def __init__(
        self,
        *,
        lines: List["SpatialAnalysisPersonLineCrossingLineEvents"],
        debug: Optional[str] = None,
        camera_configuration: Optional[str] = None,
        detector_node_configuration: Optional[str] = None,
        enable_face_mask_classifier: Optional[str] = None,
        **kwargs
    ):
        super(SpatialAnalysisPersonLineCrossingOperation, self).__init__(debug=debug, camera_configuration=camera_configuration, detector_node_configuration=detector_node_configuration, enable_face_mask_classifier=enable_face_mask_classifier, **kwargs)
        self.type = '#Microsoft.VideoAnalyzer.SpatialAnalysisPersonLineCrossingOperation'  # type: str
        self.lines = lines


class SpatialAnalysisPersonZoneCrossingEvent(SpatialAnalysisOperationEventBase):
    """Defines a Spatial Analysis Person Crossing Zone operation eventing configuration.

    :param threshold: The event threshold.
    :type threshold: str
    :param focus: The operation focus type. Possible values include: "center", "bottomCenter",
     "footprint".
    :type focus: str or ~azure.media.videoanalyzer.edge.models.SpatialAnalysisOperationFocus
    :param event_type: The event type. Possible values include: "zoneCrossing", "zoneDwellTime".
    :type event_type: str or
     ~azure.media.videoanalyzer.edge.models.SpatialAnalysisPersonZoneCrossingEventType
    """

    _attribute_map = {
        'threshold': {'key': 'threshold', 'type': 'str'},
        'focus': {'key': 'focus', 'type': 'str'},
        'event_type': {'key': 'eventType', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        threshold: Optional[str] = None,
        focus: Optional[Union[str, "SpatialAnalysisOperationFocus"]] = None,
        event_type: Optional[Union[str, "SpatialAnalysisPersonZoneCrossingEventType"]] = None,
        **kwargs
    ):
        super(SpatialAnalysisPersonZoneCrossingEvent, self).__init__(threshold=threshold, focus=focus, **kwargs)
        self.event_type = event_type


class SpatialAnalysisPersonZoneCrossingOperation(SpatialAnalysisTypedOperationBase):
    """Defines a Spatial Analysis Person Zone Crossing operation to be used in the Cognitive Services Vision processor.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param debug: Enables debugging for the Spatial Analysis operation.
    :type debug: str
    :param camera_configuration: Advanced camera configuration.
    :type camera_configuration: str
    :param detector_node_configuration: Advanced detector node configuration.
    :type detector_node_configuration: str
    :param enable_face_mask_classifier: Enables face mask detection.
    :type enable_face_mask_classifier: str
    :param zones: Required. The list of zones with optional events.
    :type zones:
     list[~azure.media.videoanalyzer.edge.models.SpatialAnalysisPersonZoneCrossingZoneEvents]
    """

    _validation = {
        'type': {'required': True},
        'zones': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'debug': {'key': 'debug', 'type': 'str'},
        'camera_configuration': {'key': 'cameraConfiguration', 'type': 'str'},
        'detector_node_configuration': {'key': 'detectorNodeConfiguration', 'type': 'str'},
        'enable_face_mask_classifier': {'key': 'enableFaceMaskClassifier', 'type': 'str'},
        'zones': {'key': 'zones', 'type': '[SpatialAnalysisPersonZoneCrossingZoneEvents]'},
    }

    def __init__(
        self,
        *,
        zones: List["SpatialAnalysisPersonZoneCrossingZoneEvents"],
        debug: Optional[str] = None,
        camera_configuration: Optional[str] = None,
        detector_node_configuration: Optional[str] = None,
        enable_face_mask_classifier: Optional[str] = None,
        **kwargs
    ):
        super(SpatialAnalysisPersonZoneCrossingOperation, self).__init__(debug=debug, camera_configuration=camera_configuration, detector_node_configuration=detector_node_configuration, enable_face_mask_classifier=enable_face_mask_classifier, **kwargs)
        self.type = '#Microsoft.VideoAnalyzer.SpatialAnalysisPersonZoneCrossingOperation'  # type: str
        self.zones = zones


class SpatialAnalysisPersonZoneCrossingZoneEvents(msrest.serialization.Model):
    """SpatialAnalysisPersonZoneCrossingZoneEvents.

    All required parameters must be populated in order to send to Azure.

    :param zone: Required. The named zone.
    :type zone: ~azure.media.videoanalyzer.edge.models.NamedPolygonBase
    :param events: The event configuration.
    :type events:
     list[~azure.media.videoanalyzer.edge.models.SpatialAnalysisPersonZoneCrossingEvent]
    """

    _validation = {
        'zone': {'required': True},
    }

    _attribute_map = {
        'zone': {'key': 'zone', 'type': 'NamedPolygonBase'},
        'events': {'key': 'events', 'type': '[SpatialAnalysisPersonZoneCrossingEvent]'},
    }

    def __init__(
        self,
        *,
        zone: "NamedPolygonBase",
        events: Optional[List["SpatialAnalysisPersonZoneCrossingEvent"]] = None,
        **kwargs
    ):
        super(SpatialAnalysisPersonZoneCrossingZoneEvents, self).__init__(**kwargs)
        self.zone = zone
        self.events = events


class SymmetricKeyCredentials(CredentialsBase):
    """Symmetric key credential.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param key: Required. Symmetric key credential.
    :type key: str
    """

    _validation = {
        'type': {'required': True},
        'key': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'key': {'key': 'key', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        key: str,
        **kwargs
    ):
        super(SymmetricKeyCredentials, self).__init__(**kwargs)
        self.type = '#Microsoft.VideoAnalyzer.SymmetricKeyCredentials'  # type: str
        self.key = key


class SystemData(msrest.serialization.Model):
    """The system data for a resource. This is used by both pipeline topologies and live pipelines.

    :param created_at: The timestamp of resource creation (UTC).
    :type created_at: ~datetime.datetime
    :param last_modified_at: The timestamp of resource last modification (UTC).
    :type last_modified_at: ~datetime.datetime
    """

    _attribute_map = {
        'created_at': {'key': 'createdAt', 'type': 'iso-8601'},
        'last_modified_at': {'key': 'lastModifiedAt', 'type': 'iso-8601'},
    }

    def __init__(
        self,
        *,
        created_at: Optional[datetime.datetime] = None,
        last_modified_at: Optional[datetime.datetime] = None,
        **kwargs
    ):
        super(SystemData, self).__init__(**kwargs)
        self.created_at = created_at
        self.last_modified_at = last_modified_at


class TlsEndpoint(EndpointBase):
    """A TLS endpoint for pipeline topology external connections.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param credentials: Polymorphic credentials to be presented to the endpoint.
    :type credentials: ~azure.media.videoanalyzer.edge.models.CredentialsBase
    :param url: Required. Url for the endpoint.
    :type url: str
    :param trusted_certificates: Trusted certificates when authenticating a TLS connection. Null
     designates that Azure Media Service's source of trust should be used.
    :type trusted_certificates: ~azure.media.videoanalyzer.edge.models.CertificateSource
    :param validation_options: Validation options to use when authenticating a TLS connection. By
     default, strict validation is used.
    :type validation_options: ~azure.media.videoanalyzer.edge.models.TlsValidationOptions
    """

    _validation = {
        'type': {'required': True},
        'url': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'credentials': {'key': 'credentials', 'type': 'CredentialsBase'},
        'url': {'key': 'url', 'type': 'str'},
        'trusted_certificates': {'key': 'trustedCertificates', 'type': 'CertificateSource'},
        'validation_options': {'key': 'validationOptions', 'type': 'TlsValidationOptions'},
    }

    def __init__(
        self,
        *,
        url: str,
        credentials: Optional["CredentialsBase"] = None,
        trusted_certificates: Optional["CertificateSource"] = None,
        validation_options: Optional["TlsValidationOptions"] = None,
        **kwargs
    ):
        super(TlsEndpoint, self).__init__(credentials=credentials, url=url, **kwargs)
        self.type = '#Microsoft.VideoAnalyzer.TlsEndpoint'  # type: str
        self.trusted_certificates = trusted_certificates
        self.validation_options = validation_options


class TlsValidationOptions(msrest.serialization.Model):
    """Options for controlling the authentication of TLS endpoints.

    :param ignore_hostname: Boolean value ignoring the host name (common name) during validation.
    :type ignore_hostname: str
    :param ignore_signature: Boolean value ignoring the integrity of the certificate chain at the
     current time.
    :type ignore_signature: str
    """

    _attribute_map = {
        'ignore_hostname': {'key': 'ignoreHostname', 'type': 'str'},
        'ignore_signature': {'key': 'ignoreSignature', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        ignore_hostname: Optional[str] = None,
        ignore_signature: Optional[str] = None,
        **kwargs
    ):
        super(TlsValidationOptions, self).__init__(**kwargs)
        self.ignore_hostname = ignore_hostname
        self.ignore_signature = ignore_signature


class UnsecuredEndpoint(EndpointBase):
    """An endpoint that the pipeline topology can connect to, with no encryption in transit.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param credentials: Polymorphic credentials to be presented to the endpoint.
    :type credentials: ~azure.media.videoanalyzer.edge.models.CredentialsBase
    :param url: Required. Url for the endpoint.
    :type url: str
    """

    _validation = {
        'type': {'required': True},
        'url': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'credentials': {'key': 'credentials', 'type': 'CredentialsBase'},
        'url': {'key': 'url', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        url: str,
        credentials: Optional["CredentialsBase"] = None,
        **kwargs
    ):
        super(UnsecuredEndpoint, self).__init__(credentials=credentials, url=url, **kwargs)
        self.type = '#Microsoft.VideoAnalyzer.UnsecuredEndpoint'  # type: str


class UsernamePasswordCredentials(CredentialsBase):
    """Username/password credential pair.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param username: Required. Username for a username/password pair.
    :type username: str
    :param password: Required. Password for a username/password pair. Please use a parameter so
     that the actual value is not returned on PUT or GET requests.
    :type password: str
    """

    _validation = {
        'type': {'required': True},
        'username': {'required': True},
        'password': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'username': {'key': 'username', 'type': 'str'},
        'password': {'key': 'password', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        username: str,
        password: str,
        **kwargs
    ):
        super(UsernamePasswordCredentials, self).__init__(**kwargs)
        self.type = '#Microsoft.VideoAnalyzer.UsernamePasswordCredentials'  # type: str
        self.username = username
        self.password = password


class VideoCreationProperties(msrest.serialization.Model):
    """Properties which will be used only if a video is being created.

    :param title: An optional title for the video.
    :type title: str
    :param description: An optional description for the video.
    :type description: str
    :param segment_length: When writing media to video, wait until at least this duration of media
     has been accumulated on the Edge. Expressed in increments of 30 seconds, with a minimum of 30
     seconds and a recommended maximum of 5 minutes.
    :type segment_length: str
    """

    _attribute_map = {
        'title': {'key': 'title', 'type': 'str'},
        'description': {'key': 'description', 'type': 'str'},
        'segment_length': {'key': 'segmentLength', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        title: Optional[str] = None,
        description: Optional[str] = None,
        segment_length: Optional[str] = None,
        **kwargs
    ):
        super(VideoCreationProperties, self).__init__(**kwargs)
        self.title = title
        self.description = description
        self.segment_length = segment_length


class VideoSink(SinkNodeBase):
    """Enables a pipeline topology to record media to an Azure Video Analyzer video for subsequent playback.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The discriminator for derived types.Constant filled by server.
    :type type: str
    :param name: Required. The name to be used for the topology sink.
    :type name: str
    :param inputs: Required. An array of the names of the other nodes in the pipeline topology, the
     outputs of which are used as input for this sink node.
    :type inputs: list[~azure.media.videoanalyzer.edge.models.NodeInput]
    :param video_name: Required. Name of a new or existing Video Analyzer video entity to use as
     media output.
    :type video_name: str
    :param video_creation_properties: Optional properties which will be used only if a video is
     being created.
    :type video_creation_properties: ~azure.media.videoanalyzer.edge.models.VideoCreationProperties
    :param local_media_cache_path: Required. Path to a local file system directory for temporary
     caching of media before writing to a video. This local cache will grow if the connection to
     Azure is not stable.
    :type local_media_cache_path: str
    :param local_media_cache_maximum_size_mi_b: Required. Maximum amount of disk space that can be
     used for temporary caching of media.
    :type local_media_cache_maximum_size_mi_b: str
    """

    _validation = {
        'type': {'required': True},
        'name': {'required': True},
        'inputs': {'required': True},
        'video_name': {'required': True},
        'local_media_cache_path': {'required': True},
        'local_media_cache_maximum_size_mi_b': {'required': True},
    }

    _attribute_map = {
        'type': {'key': '@type', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'inputs': {'key': 'inputs', 'type': '[NodeInput]'},
        'video_name': {'key': 'videoName', 'type': 'str'},
        'video_creation_properties': {'key': 'videoCreationProperties', 'type': 'VideoCreationProperties'},
        'local_media_cache_path': {'key': 'localMediaCachePath', 'type': 'str'},
        'local_media_cache_maximum_size_mi_b': {'key': 'localMediaCacheMaximumSizeMiB', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        name: str,
        inputs: List["NodeInput"],
        video_name: str,
        local_media_cache_path: str,
        local_media_cache_maximum_size_mi_b: str,
        video_creation_properties: Optional["VideoCreationProperties"] = None,
        **kwargs
    ):
        super(VideoSink, self).__init__(name=name, inputs=inputs, **kwargs)
        self.type = '#Microsoft.VideoAnalyzer.VideoSink'  # type: str
        self.video_name = video_name
        self.video_creation_properties = video_creation_properties
        self.local_media_cache_path = local_media_cache_path
        self.local_media_cache_maximum_size_mi_b = local_media_cache_maximum_size_mi_b
