type: pipeline

description: 'submit a spark job using component sdk'

inputs:
  iris_data:
    type: uri_file
    path: ./dataset/iris.csv
    mode: direct

outputs:
  output:
    type: uri_folder
    mode: direct

jobs:
  spark_job:
    type: spark

    inputs:
      file_input1: ${{parent.inputs.iris_data}}
      file_input2: ${{parent.inputs.iris_data}}

    outputs:
      output: ${{parent.outputs.output}}

    code: ./src

    entry:
      file: entry.py # file path of the entry file relative to the code root folder

    py_files:
      - utils.zip
    jars:
      - scalaproj.jar

    files:
      - my_files.txt
    #archives:
    #  - my_archive.tar

    args: >-
      --file_input1 ${{inputs.file_input1}}
      --file_input2 ${{inputs.file_input2}}
      --output ${{outputs.output}}

    compute: azureml:spark31
    conf:
      spark.driver.cores: 2
      spark.driver.memory: "1g"
      spark.executor.cores: 1
      spark.executor.memory: "1g"
      spark.executor.instances: 1
