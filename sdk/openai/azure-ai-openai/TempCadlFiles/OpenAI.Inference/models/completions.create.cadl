import "@cadl-lang/rest";

using Cadl.Rest;
using Cadl.Http;

namespace Azure.OpenAI.Completions;

@doc("Post body schema to create a prompt completion from a deployment")
model CompletionsOptions {
    @doc("""
An optional prompt to complete from, encoded as a string, a list of strings, or
a list of token lists. Defaults to <|endoftext|>. The prompt to complete from.
If you would like to provide multiple prompts, use the POST variant of this
method. Note that <|endoftext|> is the document separator that the model sees
during training, so if a prompt is not specified the model will generate as if
from the beginning of a new document. Maximum allowed size of string list is
2048.
""")
    prompt?: CompletionsPrompt;

    @doc("The maximum number of tokens to generate. Has minimum of 0.")
    max_tokens?: int32;

    @doc("""
What sampling temperature to use. Higher values means the model will take more
risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones
with a well-defined answer.
We generally recommend using this or `top_p` but
not both.
Minimum of 0 and maximum of 2 allowed.

""")
    temperature?: float32;

    @doc("""
An alternative to sampling with temperature, called nucleus sampling, where the
model considers the results of the tokens with top_p probability mass. So 0.1
means only the tokens comprising the top 10% probability mass are
considered.
We generally recommend using this or `temperature` but not
both.
Minimum of 0 and maximum of 1 allowed.

""")
    top_p?: float32;

    @doc("""
Defaults to null. Modify the likelihood of specified tokens appearing in the
completion. Accepts a json object that maps tokens (specified by their token ID
in the GPT tokenizer) to an associated bias value from -100 to 100. You can use
this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to
token IDs. Mathematically, the bias is added to the logits generated by the
model prior to sampling. The exact effect will vary per model, but values
between -1 and 1 should decrease or increase likelihood of selection; values
like -100 or 100 should result in a ban or exclusive selection of the relevant
token. As an example, you can pass {\"50256\" &#58; -100} to prevent the
<|endoftext|> token from being generated.
""")
    logit_bias?: Record<int32>;

    @doc("The ID of the end-user, for use in tracking and rate-limiting.")
    user?: string;

    @doc("""
How many snippets to generate for each prompt. Minimum of 1 and maximum of 128
allowed.
""")
    n?: int32;

// stream mode requires some API update that we cannot generate nbow
//     @doc("""
// Whether to enable streaming for this endpoint. If set, tokens will be sent as
// server-sent events as they become available.
// """)
//     stream?: boolean;

    @doc("""
Include the log probabilities on the `logprobs` most likely tokens, as well the
chosen tokens. So for example, if `logprobs` is 10, the API will return a list
of the 10 most likely tokens. If `logprobs` is 0, only the chosen tokens will
have logprobs returned. Minimum of 0 and maximum of 100 allowed.
""")
    logprobs?: int32;

    @doc("The name of the model to use")
    "model"?: string;

    @doc("Echo back the prompt in addition to the completion")
    echo?: boolean;

    @doc("A sequence which indicates the end of the current document.")
    stop?: CompletionsStop;

    @doc("Completion configuration")
    completion_config?: string;

    @doc("""
can be used to disable any server-side caching, 0=no cache, 1=prompt prefix
enabled, 2=full cache
""")
    cache_level?: int32;

    @doc("""
How much to penalize new tokens based on their existing frequency in the text
so far. Decreases the model's likelihood to repeat the same line verbatim. Has
minimum of -2 and maximum of 2.
""")
    presence_penalty?: float32;

    @doc("""
How much to penalize new tokens based on whether they appear in the text so
far. Increases the model's likelihood to talk about new topics.
""")
    frequency_penalty?: float32;

    @doc("""
How many generations to create server side, and display only the best. Will not
stream intermediate progress if best_of > 1. Has maximum value of 128.
""")
    best_of?: int32;
};

alias CompletionsPrompt = string[];

alias CompletionsStop = string[];

@doc("Expected response schema to completion request")
model Completions {
    @doc("Request ID for troubleshooting purposes")
    @header "apim-request-id": string;
    @doc("Id for completion response")
    id?: string;
    @doc("Object for completion response")
    object: "text_completion";
    @doc("Created time for completion response")
    created?: int32;
    @doc("Model used for completion response")
    "model"?: string;
    @doc("Array of choices returned containing text completions to prompts sent")
    choices?: Choice[];
}

@doc("Choice model within completion response")
model Choice {
    @doc("Generated text for given completion prompt")
    text?: string;
    @doc("Index")
    index?: int32;
    @doc("Log Prob Model")
    logprobs?: CompletionsLogProbsModel;
    @doc("Reason for finishing")
    finish_reason?: string;
}

@doc("LogProbs model within completion choice")
model CompletionsLogProbsModel {
    @doc("Tokens")
    tokens?: string[];
    @doc("LogProbs of Tokens")
    token_logprobs?: float32[];
    @doc("Top LogProbs")
    top_logprobs?: Record<float32>[];
    @doc("Text offset")
    text_offset?: int32[];
}
