# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for
# license information.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.serialization import Model


class BaseJobParameters(Model):
    """Data Lake Analytics Job Parameters base class for build and submit.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The job type of the current job (Hive, USql, or
     Scope (for internal use only)). Possible values include: 'USql', 'Hive',
     'Scope'
    :type type: str or ~azure.mgmt.datalake.analytics.job.models.JobType
    :param properties: Required. The job specific properties.
    :type properties:
     ~azure.mgmt.datalake.analytics.job.models.CreateJobProperties
    """

    _validation = {
        'type': {'required': True},
        'properties': {'required': True},
    }

    _attribute_map = {
        'type': {'key': 'type', 'type': 'JobType'},
        'properties': {'key': 'properties', 'type': 'CreateJobProperties'},
    }

    def __init__(self, **kwargs):
        super(BaseJobParameters, self).__init__(**kwargs)
        self.type = kwargs.get('type', None)
        self.properties = kwargs.get('properties', None)


class BuildJobParameters(BaseJobParameters):
    """The parameters used to build a new Data Lake Analytics job.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The job type of the current job (Hive, USql, or
     Scope (for internal use only)). Possible values include: 'USql', 'Hive',
     'Scope'
    :type type: str or ~azure.mgmt.datalake.analytics.job.models.JobType
    :param properties: Required. The job specific properties.
    :type properties:
     ~azure.mgmt.datalake.analytics.job.models.CreateJobProperties
    :param name: The friendly name of the job to build.
    :type name: str
    """

    _validation = {
        'type': {'required': True},
        'properties': {'required': True},
    }

    _attribute_map = {
        'type': {'key': 'type', 'type': 'JobType'},
        'properties': {'key': 'properties', 'type': 'CreateJobProperties'},
        'name': {'key': 'name', 'type': 'str'},
    }

    def __init__(self, **kwargs):
        super(BuildJobParameters, self).__init__(**kwargs)
        self.name = kwargs.get('name', None)


class CloudError(Model):
    """CloudError.
    """

    _attribute_map = {
    }


class CreateJobParameters(BaseJobParameters):
    """The parameters used to submit a new Data Lake Analytics job.

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The job type of the current job (Hive, USql, or
     Scope (for internal use only)). Possible values include: 'USql', 'Hive',
     'Scope'
    :type type: str or ~azure.mgmt.datalake.analytics.job.models.JobType
    :param properties: Required. The job specific properties.
    :type properties:
     ~azure.mgmt.datalake.analytics.job.models.CreateJobProperties
    :param name: Required. The friendly name of the job to submit.
    :type name: str
    :param degree_of_parallelism: The degree of parallelism to use for this
     job. At most one of degreeOfParallelism and degreeOfParallelismPercent
     should be specified. If none, a default value of 1 will be used for
     degreeOfParallelism. Default value: 1 .
    :type degree_of_parallelism: int
    :param degree_of_parallelism_percent: the degree of parallelism in
     percentage used for this job. At most one of degreeOfParallelism and
     degreeOfParallelismPercent should be specified. If none, a default value
     of 1 will be used for degreeOfParallelism.
    :type degree_of_parallelism_percent: float
    :param priority: The priority value to use for the current job. Lower
     numbers have a higher priority. By default, a job has a priority of 1000.
     This must be greater than 0.
    :type priority: int
    :param log_file_patterns: The list of log file name patterns to find in
     the logFolder. '*' is the only matching character allowed. Example format:
     jobExecution*.log or *mylog*.txt
    :type log_file_patterns: list[str]
    :param related: The recurring job relationship information properties.
    :type related:
     ~azure.mgmt.datalake.analytics.job.models.JobRelationshipProperties
    """

    _validation = {
        'type': {'required': True},
        'properties': {'required': True},
        'name': {'required': True},
    }

    _attribute_map = {
        'type': {'key': 'type', 'type': 'JobType'},
        'properties': {'key': 'properties', 'type': 'CreateJobProperties'},
        'name': {'key': 'name', 'type': 'str'},
        'degree_of_parallelism': {'key': 'degreeOfParallelism', 'type': 'int'},
        'degree_of_parallelism_percent': {'key': 'degreeOfParallelismPercent', 'type': 'float'},
        'priority': {'key': 'priority', 'type': 'int'},
        'log_file_patterns': {'key': 'logFilePatterns', 'type': '[str]'},
        'related': {'key': 'related', 'type': 'JobRelationshipProperties'},
    }

    def __init__(self, **kwargs):
        super(CreateJobParameters, self).__init__(**kwargs)
        self.name = kwargs.get('name', None)
        self.degree_of_parallelism = kwargs.get('degree_of_parallelism', 1)
        self.degree_of_parallelism_percent = kwargs.get('degree_of_parallelism_percent', None)
        self.priority = kwargs.get('priority', None)
        self.log_file_patterns = kwargs.get('log_file_patterns', None)
        self.related = kwargs.get('related', None)


class CreateJobProperties(Model):
    """The common Data Lake Analytics job properties for job submission.

    You probably want to use the sub-classes and not this class directly. Known
    sub-classes are: CreateUSqlJobProperties, CreateScopeJobProperties

    All required parameters must be populated in order to send to Azure.

    :param runtime_version: The runtime version of the Data Lake Analytics
     engine to use for the specific type of job being run.
    :type runtime_version: str
    :param script: Required. The script to run. Please note that the maximum
     script size is 3 MB.
    :type script: str
    :param type: Required. Constant filled by server.
    :type type: str
    """

    _validation = {
        'script': {'required': True},
        'type': {'required': True},
    }

    _attribute_map = {
        'runtime_version': {'key': 'runtimeVersion', 'type': 'str'},
        'script': {'key': 'script', 'type': 'str'},
        'type': {'key': 'type', 'type': 'str'},
    }

    _subtype_map = {
        'type': {'USql': 'CreateUSqlJobProperties', 'Scope': 'CreateScopeJobProperties'}
    }

    def __init__(self, **kwargs):
        super(CreateJobProperties, self).__init__(**kwargs)
        self.runtime_version = kwargs.get('runtime_version', None)
        self.script = kwargs.get('script', None)
        self.type = None


class CreateScopeJobParameters(CreateJobParameters):
    """The parameters used to submit a new Data Lake Analytics Scope job. (Only
    for use internally with Scope job type.).

    All required parameters must be populated in order to send to Azure.

    :param type: Required. The job type of the current job (Hive, USql, or
     Scope (for internal use only)). Possible values include: 'USql', 'Hive',
     'Scope'
    :type type: str or ~azure.mgmt.datalake.analytics.job.models.JobType
    :param properties: Required. The job specific properties.
    :type properties:
     ~azure.mgmt.datalake.analytics.job.models.CreateJobProperties
    :param name: Required. The friendly name of the job to submit.
    :type name: str
    :param degree_of_parallelism: The degree of parallelism to use for this
     job. At most one of degreeOfParallelism and degreeOfParallelismPercent
     should be specified. If none, a default value of 1 will be used for
     degreeOfParallelism. Default value: 1 .
    :type degree_of_parallelism: int
    :param degree_of_parallelism_percent: the degree of parallelism in
     percentage used for this job. At most one of degreeOfParallelism and
     degreeOfParallelismPercent should be specified. If none, a default value
     of 1 will be used for degreeOfParallelism.
    :type degree_of_parallelism_percent: float
    :param priority: The priority value to use for the current job. Lower
     numbers have a higher priority. By default, a job has a priority of 1000.
     This must be greater than 0.
    :type priority: int
    :param log_file_patterns: The list of log file name patterns to find in
     the logFolder. '*' is the only matching character allowed. Example format:
     jobExecution*.log or *mylog*.txt
    :type log_file_patterns: list[str]
    :param related: The recurring job relationship information properties.
    :type related:
     ~azure.mgmt.datalake.analytics.job.models.JobRelationshipProperties
    :param tags: The key-value pairs used to add additional metadata to the
     job information.
    :type tags: dict[str, str]
    """

    _validation = {
        'type': {'required': True},
        'properties': {'required': True},
        'name': {'required': True},
    }

    _attribute_map = {
        'type': {'key': 'type', 'type': 'JobType'},
        'properties': {'key': 'properties', 'type': 'CreateJobProperties'},
        'name': {'key': 'name', 'type': 'str'},
        'degree_of_parallelism': {'key': 'degreeOfParallelism', 'type': 'int'},
        'degree_of_parallelism_percent': {'key': 'degreeOfParallelismPercent', 'type': 'float'},
        'priority': {'key': 'priority', 'type': 'int'},
        'log_file_patterns': {'key': 'logFilePatterns', 'type': '[str]'},
        'related': {'key': 'related', 'type': 'JobRelationshipProperties'},
        'tags': {'key': 'tags', 'type': '{str}'},
    }

    def __init__(self, **kwargs):
        super(CreateScopeJobParameters, self).__init__(**kwargs)
        self.tags = kwargs.get('tags', None)


class CreateScopeJobProperties(CreateJobProperties):
    """Scope job properties used when submitting Scope jobs. (Only for use
    internally with Scope job type.).

    All required parameters must be populated in order to send to Azure.

    :param runtime_version: The runtime version of the Data Lake Analytics
     engine to use for the specific type of job being run.
    :type runtime_version: str
    :param script: Required. The script to run. Please note that the maximum
     script size is 3 MB.
    :type script: str
    :param type: Required. Constant filled by server.
    :type type: str
    :param resources: The list of resources that are required by the job.
    :type resources:
     list[~azure.mgmt.datalake.analytics.job.models.ScopeJobResource]
    :param notifier: The list of email addresses, separated by semi-colons, to
     notify when the job reaches a terminal state.
    :type notifier: str
    """

    _validation = {
        'script': {'required': True},
        'type': {'required': True},
    }

    _attribute_map = {
        'runtime_version': {'key': 'runtimeVersion', 'type': 'str'},
        'script': {'key': 'script', 'type': 'str'},
        'type': {'key': 'type', 'type': 'str'},
        'resources': {'key': 'resources', 'type': '[ScopeJobResource]'},
        'notifier': {'key': 'notifier', 'type': 'str'},
    }

    def __init__(self, **kwargs):
        super(CreateScopeJobProperties, self).__init__(**kwargs)
        self.resources = kwargs.get('resources', None)
        self.notifier = kwargs.get('notifier', None)
        self.type = 'Scope'


class CreateUSqlJobProperties(CreateJobProperties):
    """U-SQL job properties used when submitting U-SQL jobs.

    All required parameters must be populated in order to send to Azure.

    :param runtime_version: The runtime version of the Data Lake Analytics
     engine to use for the specific type of job being run.
    :type runtime_version: str
    :param script: Required. The script to run. Please note that the maximum
     script size is 3 MB.
    :type script: str
    :param type: Required. Constant filled by server.
    :type type: str
    :param compile_mode: The specific compilation mode for the job used during
     execution. If this is not specified during submission, the server will
     determine the optimal compilation mode. Possible values include:
     'Semantic', 'Full', 'SingleBox'
    :type compile_mode: str or
     ~azure.mgmt.datalake.analytics.job.models.CompileMode
    """

    _validation = {
        'script': {'required': True},
        'type': {'required': True},
    }

    _attribute_map = {
        'runtime_version': {'key': 'runtimeVersion', 'type': 'str'},
        'script': {'key': 'script', 'type': 'str'},
        'type': {'key': 'type', 'type': 'str'},
        'compile_mode': {'key': 'compileMode', 'type': 'CompileMode'},
    }

    def __init__(self, **kwargs):
        super(CreateUSqlJobProperties, self).__init__(**kwargs)
        self.compile_mode = kwargs.get('compile_mode', None)
        self.type = 'USql'


class Diagnostics(Model):
    """Error diagnostic information for failed jobs.

    Variables are only populated by the server, and will be ignored when
    sending a request.

    :ivar message: The error message.
    :vartype message: str
    :ivar severity: The severity of the error. Possible values include:
     'Warning', 'Error', 'Info', 'SevereWarning', 'Deprecated', 'UserWarning'
    :vartype severity: str or
     ~azure.mgmt.datalake.analytics.job.models.SeverityTypes
    :ivar line_number: The line number the error occurred on.
    :vartype line_number: int
    :ivar column_number: The column where the error occurred.
    :vartype column_number: int
    :ivar start: The starting index of the error.
    :vartype start: int
    :ivar end: The ending index of the error.
    :vartype end: int
    """

    _validation = {
        'message': {'readonly': True},
        'severity': {'readonly': True},
        'line_number': {'readonly': True},
        'column_number': {'readonly': True},
        'start': {'readonly': True},
        'end': {'readonly': True},
    }

    _attribute_map = {
        'message': {'key': 'message', 'type': 'str'},
        'severity': {'key': 'severity', 'type': 'SeverityTypes'},
        'line_number': {'key': 'lineNumber', 'type': 'int'},
        'column_number': {'key': 'columnNumber', 'type': 'int'},
        'start': {'key': 'start', 'type': 'int'},
        'end': {'key': 'end', 'type': 'int'},
    }

    def __init__(self, **kwargs):
        super(Diagnostics, self).__init__(**kwargs)
        self.message = None
        self.severity = None
        self.line_number = None
        self.column_number = None
        self.start = None
        self.end = None


class JobProperties(Model):
    """The common Data Lake Analytics job properties.

    You probably want to use the sub-classes and not this class directly. Known
    sub-classes are: USqlJobProperties, HiveJobProperties, ScopeJobProperties

    All required parameters must be populated in order to send to Azure.

    :param runtime_version: The runtime version of the Data Lake Analytics
     engine to use for the specific type of job being run.
    :type runtime_version: str
    :param script: Required. The script to run. Please note that the maximum
     script size is 3 MB.
    :type script: str
    :param type: Required. Constant filled by server.
    :type type: str
    """

    _validation = {
        'script': {'required': True},
        'type': {'required': True},
    }

    _attribute_map = {
        'runtime_version': {'key': 'runtimeVersion', 'type': 'str'},
        'script': {'key': 'script', 'type': 'str'},
        'type': {'key': 'type', 'type': 'str'},
    }

    _subtype_map = {
        'type': {'USql': 'USqlJobProperties', 'Hive': 'HiveJobProperties', 'Scope': 'ScopeJobProperties'}
    }

    def __init__(self, **kwargs):
        super(JobProperties, self).__init__(**kwargs)
        self.runtime_version = kwargs.get('runtime_version', None)
        self.script = kwargs.get('script', None)
        self.type = None


class HiveJobProperties(JobProperties):
    """Hive job properties used when retrieving Hive jobs.

    Variables are only populated by the server, and will be ignored when
    sending a request.

    All required parameters must be populated in order to send to Azure.

    :param runtime_version: The runtime version of the Data Lake Analytics
     engine to use for the specific type of job being run.
    :type runtime_version: str
    :param script: Required. The script to run. Please note that the maximum
     script size is 3 MB.
    :type script: str
    :param type: Required. Constant filled by server.
    :type type: str
    :ivar logs_location: The Hive logs location.
    :vartype logs_location: str
    :ivar output_location: The location of Hive job output files (both
     execution output and results).
    :vartype output_location: str
    :ivar statement_count: The number of statements that will be run based on
     the script.
    :vartype statement_count: int
    :ivar executed_statement_count: The number of statements that have been
     run based on the script.
    :vartype executed_statement_count: int
    """

    _validation = {
        'script': {'required': True},
        'type': {'required': True},
        'logs_location': {'readonly': True},
        'output_location': {'readonly': True},
        'statement_count': {'readonly': True},
        'executed_statement_count': {'readonly': True},
    }

    _attribute_map = {
        'runtime_version': {'key': 'runtimeVersion', 'type': 'str'},
        'script': {'key': 'script', 'type': 'str'},
        'type': {'key': 'type', 'type': 'str'},
        'logs_location': {'key': 'logsLocation', 'type': 'str'},
        'output_location': {'key': 'outputLocation', 'type': 'str'},
        'statement_count': {'key': 'statementCount', 'type': 'int'},
        'executed_statement_count': {'key': 'executedStatementCount', 'type': 'int'},
    }

    def __init__(self, **kwargs):
        super(HiveJobProperties, self).__init__(**kwargs)
        self.logs_location = None
        self.output_location = None
        self.statement_count = None
        self.executed_statement_count = None
        self.type = 'Hive'


class JobDataPath(Model):
    """A Data Lake Analytics job data path item.

    Variables are only populated by the server, and will be ignored when
    sending a request.

    :ivar job_id: The ID of the job this data is for.
    :vartype job_id: str
    :ivar command: The command that this job data relates to.
    :vartype command: str
    :ivar paths: The list of paths to all of the job data.
    :vartype paths: list[str]
    """

    _validation = {
        'job_id': {'readonly': True},
        'command': {'readonly': True},
        'paths': {'readonly': True},
    }

    _attribute_map = {
        'job_id': {'key': 'jobId', 'type': 'str'},
        'command': {'key': 'command', 'type': 'str'},
        'paths': {'key': 'paths', 'type': '[str]'},
    }

    def __init__(self, **kwargs):
        super(JobDataPath, self).__init__(**kwargs)
        self.job_id = None
        self.command = None
        self.paths = None


class JobErrorDetails(Model):
    """The Data Lake Analytics job error details.

    Variables are only populated by the server, and will be ignored when
    sending a request.

    :ivar error_id: The specific identifier for the type of error encountered
     in the job.
    :vartype error_id: str
    :ivar severity: The severity level of the failure. Possible values
     include: 'Warning', 'Error', 'Info', 'SevereWarning', 'Deprecated',
     'UserWarning'
    :vartype severity: str or
     ~azure.mgmt.datalake.analytics.job.models.SeverityTypes
    :ivar source: The ultimate source of the failure (usually either SYSTEM or
     USER).
    :vartype source: str
    :ivar message: The user friendly error message for the failure.
    :vartype message: str
    :ivar description: The error message description.
    :vartype description: str
    :ivar details: The details of the error message.
    :vartype details: str
    :ivar line_number: The specific line number in the job where the error
     occurred.
    :vartype line_number: int
    :ivar start_offset: The start offset in the job where the error was found
    :vartype start_offset: int
    :ivar end_offset: The end offset in the job where the error was found.
    :vartype end_offset: int
    :ivar resolution: The recommended resolution for the failure, if any.
    :vartype resolution: str
    :ivar file_path: The path to any supplemental error files, if any.
    :vartype file_path: str
    :ivar help_link: The link to MSDN or Azure help for this type of error, if
     any.
    :vartype help_link: str
    :ivar internal_diagnostics: The internal diagnostic stack trace if the
     user requesting the job error details has sufficient permissions it will
     be retrieved, otherwise it will be empty.
    :vartype internal_diagnostics: str
    :ivar inner_error: The inner error of this specific job error message, if
     any.
    :vartype inner_error:
     ~azure.mgmt.datalake.analytics.job.models.JobInnerError
    """

    _validation = {
        'error_id': {'readonly': True},
        'severity': {'readonly': True},
        'source': {'readonly': True},
        'message': {'readonly': True},
        'description': {'readonly': True},
        'details': {'readonly': True},
        'line_number': {'readonly': True},
        'start_offset': {'readonly': True},
        'end_offset': {'readonly': True},
        'resolution': {'readonly': True},
        'file_path': {'readonly': True},
        'help_link': {'readonly': True},
        'internal_diagnostics': {'readonly': True},
        'inner_error': {'readonly': True},
    }

    _attribute_map = {
        'error_id': {'key': 'errorId', 'type': 'str'},
        'severity': {'key': 'severity', 'type': 'SeverityTypes'},
        'source': {'key': 'source', 'type': 'str'},
        'message': {'key': 'message', 'type': 'str'},
        'description': {'key': 'description', 'type': 'str'},
        'details': {'key': 'details', 'type': 'str'},
        'line_number': {'key': 'lineNumber', 'type': 'int'},
        'start_offset': {'key': 'startOffset', 'type': 'int'},
        'end_offset': {'key': 'endOffset', 'type': 'int'},
        'resolution': {'key': 'resolution', 'type': 'str'},
        'file_path': {'key': 'filePath', 'type': 'str'},
        'help_link': {'key': 'helpLink', 'type': 'str'},
        'internal_diagnostics': {'key': 'internalDiagnostics', 'type': 'str'},
        'inner_error': {'key': 'innerError', 'type': 'JobInnerError'},
    }

    def __init__(self, **kwargs):
        super(JobErrorDetails, self).__init__(**kwargs)
        self.error_id = None
        self.severity = None
        self.source = None
        self.message = None
        self.description = None
        self.details = None
        self.line_number = None
        self.start_offset = None
        self.end_offset = None
        self.resolution = None
        self.file_path = None
        self.help_link = None
        self.internal_diagnostics = None
        self.inner_error = None


class JobInformationBasic(Model):
    """The common Data Lake Analytics job information properties.

    Variables are only populated by the server, and will be ignored when
    sending a request.

    All required parameters must be populated in order to send to Azure.

    :ivar job_id: The job's unique identifier (a GUID).
    :vartype job_id: str
    :param name: Required. The friendly name of the job.
    :type name: str
    :param type: Required. The job type of the current job (Hive, USql, or
     Scope (for internal use only)). Possible values include: 'USql', 'Hive',
     'Scope'
    :type type: str or ~azure.mgmt.datalake.analytics.job.models.JobType
    :ivar submitter: The user or account that submitted the job.
    :vartype submitter: str
    :param degree_of_parallelism: The degree of parallelism used for this job.
     Default value: 1 .
    :type degree_of_parallelism: int
    :ivar degree_of_parallelism_percent: the degree of parallelism in
     percentage used for this job.
    :vartype degree_of_parallelism_percent: float
    :param priority: The priority value for the current job. Lower numbers
     have a higher priority. By default, a job has a priority of 1000. This
     must be greater than 0.
    :type priority: int
    :ivar submit_time: The time the job was submitted to the service.
    :vartype submit_time: datetime
    :ivar start_time: The start time of the job.
    :vartype start_time: datetime
    :ivar end_time: The completion time of the job.
    :vartype end_time: datetime
    :ivar state: The job state. When the job is in the Ended state, refer to
     Result and ErrorMessage for details. Possible values include: 'Accepted',
     'Compiling', 'Ended', 'New', 'Queued', 'Running', 'Scheduling',
     'Starting', 'Paused', 'WaitingForCapacity', 'Yielded', 'Finalizing'
    :vartype state: str or ~azure.mgmt.datalake.analytics.job.models.JobState
    :ivar result: The result of job execution or the current result of the
     running job. Possible values include: 'None', 'Succeeded', 'Cancelled',
     'Failed'
    :vartype result: str or
     ~azure.mgmt.datalake.analytics.job.models.JobResult
    :ivar log_folder: The log folder path to use in the following format:
     adl://<accountName>.azuredatalakestore.net/system/jobservice/jobs/Usql/2016/03/13/17/18/5fe51957-93bc-4de0-8ddc-c5a4753b068b/logs/.
    :vartype log_folder: str
    :param log_file_patterns: The list of log file name patterns to find in
     the logFolder. '*' is the only matching character allowed. Example format:
     jobExecution*.log or *mylog*.txt
    :type log_file_patterns: list[str]
    :param related: The recurring job relationship information properties.
    :type related:
     ~azure.mgmt.datalake.analytics.job.models.JobRelationshipProperties
    :param tags: The key-value pairs used to add additional metadata to the
     job information. (Only for use internally with Scope job type.)
    :type tags: dict[str, str]
    :ivar hierarchy_queue_node: the name of hierarchy queue node this job is
     assigned to, Null if job has not been assigned yet or the account doesn't
     have hierarchy queue.
    :vartype hierarchy_queue_node: str
    """

    _validation = {
        'job_id': {'readonly': True},
        'name': {'required': True},
        'type': {'required': True},
        'submitter': {'readonly': True},
        'degree_of_parallelism_percent': {'readonly': True},
        'submit_time': {'readonly': True},
        'start_time': {'readonly': True},
        'end_time': {'readonly': True},
        'state': {'readonly': True},
        'result': {'readonly': True},
        'log_folder': {'readonly': True},
        'hierarchy_queue_node': {'readonly': True},
    }

    _attribute_map = {
        'job_id': {'key': 'jobId', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'type': {'key': 'type', 'type': 'JobType'},
        'submitter': {'key': 'submitter', 'type': 'str'},
        'degree_of_parallelism': {'key': 'degreeOfParallelism', 'type': 'int'},
        'degree_of_parallelism_percent': {'key': 'degreeOfParallelismPercent', 'type': 'float'},
        'priority': {'key': 'priority', 'type': 'int'},
        'submit_time': {'key': 'submitTime', 'type': 'iso-8601'},
        'start_time': {'key': 'startTime', 'type': 'iso-8601'},
        'end_time': {'key': 'endTime', 'type': 'iso-8601'},
        'state': {'key': 'state', 'type': 'JobState'},
        'result': {'key': 'result', 'type': 'JobResult'},
        'log_folder': {'key': 'logFolder', 'type': 'str'},
        'log_file_patterns': {'key': 'logFilePatterns', 'type': '[str]'},
        'related': {'key': 'related', 'type': 'JobRelationshipProperties'},
        'tags': {'key': 'tags', 'type': '{str}'},
        'hierarchy_queue_node': {'key': 'hierarchyQueueNode', 'type': 'str'},
    }

    def __init__(self, **kwargs):
        super(JobInformationBasic, self).__init__(**kwargs)
        self.job_id = None
        self.name = kwargs.get('name', None)
        self.type = kwargs.get('type', None)
        self.submitter = None
        self.degree_of_parallelism = kwargs.get('degree_of_parallelism', 1)
        self.degree_of_parallelism_percent = None
        self.priority = kwargs.get('priority', None)
        self.submit_time = None
        self.start_time = None
        self.end_time = None
        self.state = None
        self.result = None
        self.log_folder = None
        self.log_file_patterns = kwargs.get('log_file_patterns', None)
        self.related = kwargs.get('related', None)
        self.tags = kwargs.get('tags', None)
        self.hierarchy_queue_node = None


class JobInformation(JobInformationBasic):
    """The extended Data Lake Analytics job information properties returned when
    retrieving a specific job.

    Variables are only populated by the server, and will be ignored when
    sending a request.

    All required parameters must be populated in order to send to Azure.

    :ivar job_id: The job's unique identifier (a GUID).
    :vartype job_id: str
    :param name: Required. The friendly name of the job.
    :type name: str
    :param type: Required. The job type of the current job (Hive, USql, or
     Scope (for internal use only)). Possible values include: 'USql', 'Hive',
     'Scope'
    :type type: str or ~azure.mgmt.datalake.analytics.job.models.JobType
    :ivar submitter: The user or account that submitted the job.
    :vartype submitter: str
    :param degree_of_parallelism: The degree of parallelism used for this job.
     Default value: 1 .
    :type degree_of_parallelism: int
    :ivar degree_of_parallelism_percent: the degree of parallelism in
     percentage used for this job.
    :vartype degree_of_parallelism_percent: float
    :param priority: The priority value for the current job. Lower numbers
     have a higher priority. By default, a job has a priority of 1000. This
     must be greater than 0.
    :type priority: int
    :ivar submit_time: The time the job was submitted to the service.
    :vartype submit_time: datetime
    :ivar start_time: The start time of the job.
    :vartype start_time: datetime
    :ivar end_time: The completion time of the job.
    :vartype end_time: datetime
    :ivar state: The job state. When the job is in the Ended state, refer to
     Result and ErrorMessage for details. Possible values include: 'Accepted',
     'Compiling', 'Ended', 'New', 'Queued', 'Running', 'Scheduling',
     'Starting', 'Paused', 'WaitingForCapacity', 'Yielded', 'Finalizing'
    :vartype state: str or ~azure.mgmt.datalake.analytics.job.models.JobState
    :ivar result: The result of job execution or the current result of the
     running job. Possible values include: 'None', 'Succeeded', 'Cancelled',
     'Failed'
    :vartype result: str or
     ~azure.mgmt.datalake.analytics.job.models.JobResult
    :ivar log_folder: The log folder path to use in the following format:
     adl://<accountName>.azuredatalakestore.net/system/jobservice/jobs/Usql/2016/03/13/17/18/5fe51957-93bc-4de0-8ddc-c5a4753b068b/logs/.
    :vartype log_folder: str
    :param log_file_patterns: The list of log file name patterns to find in
     the logFolder. '*' is the only matching character allowed. Example format:
     jobExecution*.log or *mylog*.txt
    :type log_file_patterns: list[str]
    :param related: The recurring job relationship information properties.
    :type related:
     ~azure.mgmt.datalake.analytics.job.models.JobRelationshipProperties
    :param tags: The key-value pairs used to add additional metadata to the
     job information. (Only for use internally with Scope job type.)
    :type tags: dict[str, str]
    :ivar hierarchy_queue_node: the name of hierarchy queue node this job is
     assigned to, Null if job has not been assigned yet or the account doesn't
     have hierarchy queue.
    :vartype hierarchy_queue_node: str
    :ivar error_message: The error message details for the job, if the job
     failed.
    :vartype error_message:
     list[~azure.mgmt.datalake.analytics.job.models.JobErrorDetails]
    :ivar state_audit_records: The job state audit records, indicating when
     various operations have been performed on this job.
    :vartype state_audit_records:
     list[~azure.mgmt.datalake.analytics.job.models.JobStateAuditRecord]
    :param properties: Required. The job specific properties.
    :type properties: ~azure.mgmt.datalake.analytics.job.models.JobProperties
    """

    _validation = {
        'job_id': {'readonly': True},
        'name': {'required': True},
        'type': {'required': True},
        'submitter': {'readonly': True},
        'degree_of_parallelism_percent': {'readonly': True},
        'submit_time': {'readonly': True},
        'start_time': {'readonly': True},
        'end_time': {'readonly': True},
        'state': {'readonly': True},
        'result': {'readonly': True},
        'log_folder': {'readonly': True},
        'hierarchy_queue_node': {'readonly': True},
        'error_message': {'readonly': True},
        'state_audit_records': {'readonly': True},
        'properties': {'required': True},
    }

    _attribute_map = {
        'job_id': {'key': 'jobId', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'type': {'key': 'type', 'type': 'JobType'},
        'submitter': {'key': 'submitter', 'type': 'str'},
        'degree_of_parallelism': {'key': 'degreeOfParallelism', 'type': 'int'},
        'degree_of_parallelism_percent': {'key': 'degreeOfParallelismPercent', 'type': 'float'},
        'priority': {'key': 'priority', 'type': 'int'},
        'submit_time': {'key': 'submitTime', 'type': 'iso-8601'},
        'start_time': {'key': 'startTime', 'type': 'iso-8601'},
        'end_time': {'key': 'endTime', 'type': 'iso-8601'},
        'state': {'key': 'state', 'type': 'JobState'},
        'result': {'key': 'result', 'type': 'JobResult'},
        'log_folder': {'key': 'logFolder', 'type': 'str'},
        'log_file_patterns': {'key': 'logFilePatterns', 'type': '[str]'},
        'related': {'key': 'related', 'type': 'JobRelationshipProperties'},
        'tags': {'key': 'tags', 'type': '{str}'},
        'hierarchy_queue_node': {'key': 'hierarchyQueueNode', 'type': 'str'},
        'error_message': {'key': 'errorMessage', 'type': '[JobErrorDetails]'},
        'state_audit_records': {'key': 'stateAuditRecords', 'type': '[JobStateAuditRecord]'},
        'properties': {'key': 'properties', 'type': 'JobProperties'},
    }

    def __init__(self, **kwargs):
        super(JobInformation, self).__init__(**kwargs)
        self.error_message = None
        self.state_audit_records = None
        self.properties = kwargs.get('properties', None)


class JobInnerError(Model):
    """The Data Lake Analytics job error details.

    Variables are only populated by the server, and will be ignored when
    sending a request.

    :ivar error_id: The specific identifier for the type of error encountered
     in the job.
    :vartype error_id: str
    :ivar severity: The severity level of the failure. Possible values
     include: 'Warning', 'Error', 'Info', 'SevereWarning', 'Deprecated',
     'UserWarning'
    :vartype severity: str or
     ~azure.mgmt.datalake.analytics.job.models.SeverityTypes
    :ivar source: The ultimate source of the failure (usually either SYSTEM or
     USER).
    :vartype source: str
    :ivar message: The user friendly error message for the failure.
    :vartype message: str
    :ivar description: The error message description.
    :vartype description: str
    :ivar details: The details of the error message.
    :vartype details: str
    :ivar diagnostic_code: The diagnostic error code.
    :vartype diagnostic_code: int
    :ivar component: The component that failed.
    :vartype component: str
    :ivar resolution: The recommended resolution for the failure, if any.
    :vartype resolution: str
    :ivar help_link: The link to MSDN or Azure help for this type of error, if
     any.
    :vartype help_link: str
    :ivar internal_diagnostics: The internal diagnostic stack trace if the
     user requesting the job error details has sufficient permissions it will
     be retrieved, otherwise it will be empty.
    :vartype internal_diagnostics: str
    :ivar inner_error: The inner error of this specific job error message, if
     any.
    :vartype inner_error:
     ~azure.mgmt.datalake.analytics.job.models.JobInnerError
    """

    _validation = {
        'error_id': {'readonly': True},
        'severity': {'readonly': True},
        'source': {'readonly': True},
        'message': {'readonly': True},
        'description': {'readonly': True},
        'details': {'readonly': True},
        'diagnostic_code': {'readonly': True},
        'component': {'readonly': True},
        'resolution': {'readonly': True},
        'help_link': {'readonly': True},
        'internal_diagnostics': {'readonly': True},
        'inner_error': {'readonly': True},
    }

    _attribute_map = {
        'error_id': {'key': 'errorId', 'type': 'str'},
        'severity': {'key': 'severity', 'type': 'SeverityTypes'},
        'source': {'key': 'source', 'type': 'str'},
        'message': {'key': 'message', 'type': 'str'},
        'description': {'key': 'description', 'type': 'str'},
        'details': {'key': 'details', 'type': 'str'},
        'diagnostic_code': {'key': 'diagnosticCode', 'type': 'int'},
        'component': {'key': 'component', 'type': 'str'},
        'resolution': {'key': 'resolution', 'type': 'str'},
        'help_link': {'key': 'helpLink', 'type': 'str'},
        'internal_diagnostics': {'key': 'internalDiagnostics', 'type': 'str'},
        'inner_error': {'key': 'innerError', 'type': 'JobInnerError'},
    }

    def __init__(self, **kwargs):
        super(JobInnerError, self).__init__(**kwargs)
        self.error_id = None
        self.severity = None
        self.source = None
        self.message = None
        self.description = None
        self.details = None
        self.diagnostic_code = None
        self.component = None
        self.resolution = None
        self.help_link = None
        self.internal_diagnostics = None
        self.inner_error = None


class JobPipelineInformation(Model):
    """Job Pipeline Information, showing the relationship of jobs and recurrences
    of those jobs in a pipeline.

    Variables are only populated by the server, and will be ignored when
    sending a request.

    :ivar pipeline_id: The job relationship pipeline identifier (a GUID).
    :vartype pipeline_id: str
    :ivar pipeline_name: The friendly name of the job relationship pipeline,
     which does not need to be unique.
    :vartype pipeline_name: str
    :ivar pipeline_uri: The pipeline uri, unique, links to the originating
     service for this pipeline.
    :vartype pipeline_uri: str
    :ivar num_jobs_failed: The number of jobs in this pipeline that have
     failed.
    :vartype num_jobs_failed: int
    :ivar num_jobs_canceled: The number of jobs in this pipeline that have
     been canceled.
    :vartype num_jobs_canceled: int
    :ivar num_jobs_succeeded: The number of jobs in this pipeline that have
     succeeded.
    :vartype num_jobs_succeeded: int
    :ivar au_hours_failed: The number of job execution hours that resulted in
     failed jobs.
    :vartype au_hours_failed: float
    :ivar au_hours_canceled: The number of job execution hours that resulted
     in canceled jobs.
    :vartype au_hours_canceled: float
    :ivar au_hours_succeeded: The number of job execution hours that resulted
     in successful jobs.
    :vartype au_hours_succeeded: float
    :ivar last_submit_time: The last time a job in this pipeline was
     submitted.
    :vartype last_submit_time: datetime
    :ivar runs: The list of recurrence identifiers representing each run of
     this pipeline.
    :vartype runs:
     list[~azure.mgmt.datalake.analytics.job.models.JobPipelineRunInformation]
    :ivar recurrences: The list of recurrence identifiers representing each
     run of this pipeline.
    :vartype recurrences: list[str]
    """

    _validation = {
        'pipeline_id': {'readonly': True},
        'pipeline_name': {'readonly': True, 'max_length': 260},
        'pipeline_uri': {'readonly': True},
        'num_jobs_failed': {'readonly': True},
        'num_jobs_canceled': {'readonly': True},
        'num_jobs_succeeded': {'readonly': True},
        'au_hours_failed': {'readonly': True},
        'au_hours_canceled': {'readonly': True},
        'au_hours_succeeded': {'readonly': True},
        'last_submit_time': {'readonly': True},
        'runs': {'readonly': True},
        'recurrences': {'readonly': True},
    }

    _attribute_map = {
        'pipeline_id': {'key': 'pipelineId', 'type': 'str'},
        'pipeline_name': {'key': 'pipelineName', 'type': 'str'},
        'pipeline_uri': {'key': 'pipelineUri', 'type': 'str'},
        'num_jobs_failed': {'key': 'numJobsFailed', 'type': 'int'},
        'num_jobs_canceled': {'key': 'numJobsCanceled', 'type': 'int'},
        'num_jobs_succeeded': {'key': 'numJobsSucceeded', 'type': 'int'},
        'au_hours_failed': {'key': 'auHoursFailed', 'type': 'float'},
        'au_hours_canceled': {'key': 'auHoursCanceled', 'type': 'float'},
        'au_hours_succeeded': {'key': 'auHoursSucceeded', 'type': 'float'},
        'last_submit_time': {'key': 'lastSubmitTime', 'type': 'iso-8601'},
        'runs': {'key': 'runs', 'type': '[JobPipelineRunInformation]'},
        'recurrences': {'key': 'recurrences', 'type': '[str]'},
    }

    def __init__(self, **kwargs):
        super(JobPipelineInformation, self).__init__(**kwargs)
        self.pipeline_id = None
        self.pipeline_name = None
        self.pipeline_uri = None
        self.num_jobs_failed = None
        self.num_jobs_canceled = None
        self.num_jobs_succeeded = None
        self.au_hours_failed = None
        self.au_hours_canceled = None
        self.au_hours_succeeded = None
        self.last_submit_time = None
        self.runs = None
        self.recurrences = None


class JobPipelineRunInformation(Model):
    """Run info for a specific job pipeline.

    Variables are only populated by the server, and will be ignored when
    sending a request.

    :ivar run_id: The run identifier of an instance of pipeline executions (a
     GUID).
    :vartype run_id: str
    :ivar last_submit_time: The time this instance was last submitted.
    :vartype last_submit_time: datetime
    """

    _validation = {
        'run_id': {'readonly': True},
        'last_submit_time': {'readonly': True},
    }

    _attribute_map = {
        'run_id': {'key': 'runId', 'type': 'str'},
        'last_submit_time': {'key': 'lastSubmitTime', 'type': 'iso-8601'},
    }

    def __init__(self, **kwargs):
        super(JobPipelineRunInformation, self).__init__(**kwargs)
        self.run_id = None
        self.last_submit_time = None


class JobRecurrenceInformation(Model):
    """Recurrence job information for a specific recurrence.

    Variables are only populated by the server, and will be ignored when
    sending a request.

    :ivar recurrence_id: The recurrence identifier (a GUID), unique per
     activity/script, regardless of iterations. This is something to link
     different occurrences of the same job together.
    :vartype recurrence_id: str
    :ivar recurrence_name: The recurrence name, user friendly name for the
     correlation between jobs.
    :vartype recurrence_name: str
    :ivar num_jobs_failed: The number of jobs in this recurrence that have
     failed.
    :vartype num_jobs_failed: int
    :ivar num_jobs_canceled: The number of jobs in this recurrence that have
     been canceled.
    :vartype num_jobs_canceled: int
    :ivar num_jobs_succeeded: The number of jobs in this recurrence that have
     succeeded.
    :vartype num_jobs_succeeded: int
    :ivar au_hours_failed: The number of job execution hours that resulted in
     failed jobs.
    :vartype au_hours_failed: float
    :ivar au_hours_canceled: The number of job execution hours that resulted
     in canceled jobs.
    :vartype au_hours_canceled: float
    :ivar au_hours_succeeded: The number of job execution hours that resulted
     in successful jobs.
    :vartype au_hours_succeeded: float
    :ivar last_submit_time: The last time a job in this recurrence was
     submitted.
    :vartype last_submit_time: datetime
    """

    _validation = {
        'recurrence_id': {'readonly': True},
        'recurrence_name': {'readonly': True},
        'num_jobs_failed': {'readonly': True},
        'num_jobs_canceled': {'readonly': True},
        'num_jobs_succeeded': {'readonly': True},
        'au_hours_failed': {'readonly': True},
        'au_hours_canceled': {'readonly': True},
        'au_hours_succeeded': {'readonly': True},
        'last_submit_time': {'readonly': True},
    }

    _attribute_map = {
        'recurrence_id': {'key': 'recurrenceId', 'type': 'str'},
        'recurrence_name': {'key': 'recurrenceName', 'type': 'str'},
        'num_jobs_failed': {'key': 'numJobsFailed', 'type': 'int'},
        'num_jobs_canceled': {'key': 'numJobsCanceled', 'type': 'int'},
        'num_jobs_succeeded': {'key': 'numJobsSucceeded', 'type': 'int'},
        'au_hours_failed': {'key': 'auHoursFailed', 'type': 'float'},
        'au_hours_canceled': {'key': 'auHoursCanceled', 'type': 'float'},
        'au_hours_succeeded': {'key': 'auHoursSucceeded', 'type': 'float'},
        'last_submit_time': {'key': 'lastSubmitTime', 'type': 'iso-8601'},
    }

    def __init__(self, **kwargs):
        super(JobRecurrenceInformation, self).__init__(**kwargs)
        self.recurrence_id = None
        self.recurrence_name = None
        self.num_jobs_failed = None
        self.num_jobs_canceled = None
        self.num_jobs_succeeded = None
        self.au_hours_failed = None
        self.au_hours_canceled = None
        self.au_hours_succeeded = None
        self.last_submit_time = None


class JobRelationshipProperties(Model):
    """Job relationship information properties including pipeline information,
    correlation information, etc.

    All required parameters must be populated in order to send to Azure.

    :param pipeline_id: The job relationship pipeline identifier (a GUID).
    :type pipeline_id: str
    :param pipeline_name: The friendly name of the job relationship pipeline,
     which does not need to be unique.
    :type pipeline_name: str
    :param pipeline_uri: The pipeline uri, unique, links to the originating
     service for this pipeline.
    :type pipeline_uri: str
    :param run_id: The run identifier (a GUID), unique identifier of the
     iteration of this pipeline.
    :type run_id: str
    :param recurrence_id: Required. The recurrence identifier (a GUID), unique
     per activity/script, regardless of iterations. This is something to link
     different occurrences of the same job together.
    :type recurrence_id: str
    :param recurrence_name: The recurrence name, user friendly name for the
     correlation between jobs.
    :type recurrence_name: str
    """

    _validation = {
        'pipeline_name': {'max_length': 260},
        'recurrence_id': {'required': True},
        'recurrence_name': {'max_length': 260},
    }

    _attribute_map = {
        'pipeline_id': {'key': 'pipelineId', 'type': 'str'},
        'pipeline_name': {'key': 'pipelineName', 'type': 'str'},
        'pipeline_uri': {'key': 'pipelineUri', 'type': 'str'},
        'run_id': {'key': 'runId', 'type': 'str'},
        'recurrence_id': {'key': 'recurrenceId', 'type': 'str'},
        'recurrence_name': {'key': 'recurrenceName', 'type': 'str'},
    }

    def __init__(self, **kwargs):
        super(JobRelationshipProperties, self).__init__(**kwargs)
        self.pipeline_id = kwargs.get('pipeline_id', None)
        self.pipeline_name = kwargs.get('pipeline_name', None)
        self.pipeline_uri = kwargs.get('pipeline_uri', None)
        self.run_id = kwargs.get('run_id', None)
        self.recurrence_id = kwargs.get('recurrence_id', None)
        self.recurrence_name = kwargs.get('recurrence_name', None)


class JobResource(Model):
    """The Data Lake Analytics job resources.

    :param name: The name of the resource.
    :type name: str
    :param resource_path: The path to the resource.
    :type resource_path: str
    :param type: The job resource type. Possible values include:
     'VertexResource', 'JobManagerResource', 'StatisticsResource',
     'VertexResourceInUserFolder', 'JobManagerResourceInUserFolder',
     'StatisticsResourceInUserFolder'
    :type type: str or
     ~azure.mgmt.datalake.analytics.job.models.JobResourceType
    """

    _attribute_map = {
        'name': {'key': 'name', 'type': 'str'},
        'resource_path': {'key': 'resourcePath', 'type': 'str'},
        'type': {'key': 'type', 'type': 'JobResourceType'},
    }

    def __init__(self, **kwargs):
        super(JobResource, self).__init__(**kwargs)
        self.name = kwargs.get('name', None)
        self.resource_path = kwargs.get('resource_path', None)
        self.type = kwargs.get('type', None)


class JobStateAuditRecord(Model):
    """The Data Lake Analytics job state audit records for tracking the lifecycle
    of a job.

    Variables are only populated by the server, and will be ignored when
    sending a request.

    :ivar new_state: The new state the job is in.
    :vartype new_state: str
    :ivar time_stamp: The time stamp that the state change took place.
    :vartype time_stamp: datetime
    :ivar requested_by_user: The user who requests the change.
    :vartype requested_by_user: str
    :ivar details: The details of the audit log.
    :vartype details: str
    """

    _validation = {
        'new_state': {'readonly': True},
        'time_stamp': {'readonly': True},
        'requested_by_user': {'readonly': True},
        'details': {'readonly': True},
    }

    _attribute_map = {
        'new_state': {'key': 'newState', 'type': 'str'},
        'time_stamp': {'key': 'timeStamp', 'type': 'iso-8601'},
        'requested_by_user': {'key': 'requestedByUser', 'type': 'str'},
        'details': {'key': 'details', 'type': 'str'},
    }

    def __init__(self, **kwargs):
        super(JobStateAuditRecord, self).__init__(**kwargs)
        self.new_state = None
        self.time_stamp = None
        self.requested_by_user = None
        self.details = None


class JobStatistics(Model):
    """The Data Lake Analytics job execution statistics.

    Variables are only populated by the server, and will be ignored when
    sending a request.

    :ivar last_update_time_utc: The last update time for the statistics.
    :vartype last_update_time_utc: datetime
    :ivar finalizing_time_utc: The job finalizing start time.
    :vartype finalizing_time_utc: datetime
    :ivar stages: The list of stages for the job.
    :vartype stages:
     list[~azure.mgmt.datalake.analytics.job.models.JobStatisticsVertexStage]
    """

    _validation = {
        'last_update_time_utc': {'readonly': True},
        'finalizing_time_utc': {'readonly': True},
        'stages': {'readonly': True},
    }

    _attribute_map = {
        'last_update_time_utc': {'key': 'lastUpdateTimeUtc', 'type': 'iso-8601'},
        'finalizing_time_utc': {'key': 'finalizingTimeUtc', 'type': 'iso-8601'},
        'stages': {'key': 'stages', 'type': '[JobStatisticsVertexStage]'},
    }

    def __init__(self, **kwargs):
        super(JobStatistics, self).__init__(**kwargs)
        self.last_update_time_utc = None
        self.finalizing_time_utc = None
        self.stages = None


class JobStatisticsVertex(Model):
    """The detailed information for a vertex.

    Variables are only populated by the server, and will be ignored when
    sending a request.

    :ivar name: The name of the vertex.
    :vartype name: str
    :ivar vertex_id: The id of the vertex.
    :vartype vertex_id: str
    :ivar execution_time: The amount of execution time of the vertex.
    :vartype execution_time: timedelta
    :ivar data_read: The amount of data read of the vertex, in bytes.
    :vartype data_read: long
    :ivar peak_mem_usage: The amount of peak memory usage of the vertex, in
     bytes.
    :vartype peak_mem_usage: long
    """

    _validation = {
        'name': {'readonly': True},
        'vertex_id': {'readonly': True},
        'execution_time': {'readonly': True},
        'data_read': {'readonly': True},
        'peak_mem_usage': {'readonly': True},
    }

    _attribute_map = {
        'name': {'key': 'name', 'type': 'str'},
        'vertex_id': {'key': 'vertexId', 'type': 'str'},
        'execution_time': {'key': 'executionTime', 'type': 'duration'},
        'data_read': {'key': 'dataRead', 'type': 'long'},
        'peak_mem_usage': {'key': 'peakMemUsage', 'type': 'long'},
    }

    def __init__(self, **kwargs):
        super(JobStatisticsVertex, self).__init__(**kwargs)
        self.name = None
        self.vertex_id = None
        self.execution_time = None
        self.data_read = None
        self.peak_mem_usage = None


class JobStatisticsVertexStage(Model):
    """The Data Lake Analytics job statistics vertex stage information.

    Variables are only populated by the server, and will be ignored when
    sending a request.

    :ivar data_read: The amount of data read, in bytes.
    :vartype data_read: long
    :ivar data_read_cross_pod: The amount of data read across multiple pods,
     in bytes.
    :vartype data_read_cross_pod: long
    :ivar data_read_intra_pod: The amount of data read in one pod, in bytes.
    :vartype data_read_intra_pod: long
    :ivar data_to_read: The amount of data remaining to be read, in bytes.
    :vartype data_to_read: long
    :ivar data_written: The amount of data written, in bytes.
    :vartype data_written: long
    :ivar duplicate_discard_count: The number of duplicates that were
     discarded.
    :vartype duplicate_discard_count: int
    :ivar failed_count: The number of failures that occurred in this stage.
    :vartype failed_count: int
    :ivar max_vertex_data_read: The maximum amount of data read in a single
     vertex, in bytes.
    :vartype max_vertex_data_read: long
    :ivar min_vertex_data_read: The minimum amount of data read in a single
     vertex, in bytes.
    :vartype min_vertex_data_read: long
    :ivar read_failure_count: The number of read failures in this stage.
    :vartype read_failure_count: int
    :ivar revocation_count: The number of vertices that were revoked during
     this stage.
    :vartype revocation_count: int
    :ivar running_count: The number of currently running vertices in this
     stage.
    :vartype running_count: int
    :ivar scheduled_count: The number of currently scheduled vertices in this
     stage.
    :vartype scheduled_count: int
    :ivar stage_name: The name of this stage in job execution.
    :vartype stage_name: str
    :ivar succeeded_count: The number of vertices that succeeded in this
     stage.
    :vartype succeeded_count: int
    :ivar temp_data_written: The amount of temporary data written, in bytes.
    :vartype temp_data_written: long
    :ivar total_count: The total vertex count for this stage.
    :vartype total_count: int
    :ivar total_failed_time: The amount of time that failed vertices took up
     in this stage.
    :vartype total_failed_time: timedelta
    :ivar total_progress: The current progress of this stage, as a percentage.
    :vartype total_progress: int
    :ivar total_succeeded_time: The amount of time all successful vertices
     took in this stage.
    :vartype total_succeeded_time: timedelta
    :ivar total_peak_mem_usage: The sum of the peak memory usage of all the
     vertices in the stage, in bytes.
    :vartype total_peak_mem_usage: long
    :ivar total_execution_time: The sum of the total execution time of all the
     vertices in the stage.
    :vartype total_execution_time: timedelta
    :param max_data_read_vertex: the vertex with the maximum amount of data
     read.
    :type max_data_read_vertex:
     ~azure.mgmt.datalake.analytics.job.models.JobStatisticsVertex
    :param max_execution_time_vertex: the vertex with the maximum execution
     time.
    :type max_execution_time_vertex:
     ~azure.mgmt.datalake.analytics.job.models.JobStatisticsVertex
    :param max_peak_mem_usage_vertex: the vertex with the maximum peak memory
     usage.
    :type max_peak_mem_usage_vertex:
     ~azure.mgmt.datalake.analytics.job.models.JobStatisticsVertex
    :ivar estimated_vertex_cpu_core_count: The estimated vertex CPU core
     count.
    :vartype estimated_vertex_cpu_core_count: int
    :ivar estimated_vertex_peak_cpu_core_count: The estimated vertex peak CPU
     core count.
    :vartype estimated_vertex_peak_cpu_core_count: int
    :ivar estimated_vertex_mem_size: The estimated vertex memory size, in
     bytes.
    :vartype estimated_vertex_mem_size: long
    :param allocated_container_cpu_core_count: The statistics information for
     the allocated container CPU core count.
    :type allocated_container_cpu_core_count:
     ~azure.mgmt.datalake.analytics.job.models.ResourceUsageStatistics
    :param allocated_container_mem_size: The statistics information for the
     allocated container memory size.
    :type allocated_container_mem_size:
     ~azure.mgmt.datalake.analytics.job.models.ResourceUsageStatistics
    :param used_vertex_cpu_core_count: The statistics information for the used
     vertex CPU core count.
    :type used_vertex_cpu_core_count:
     ~azure.mgmt.datalake.analytics.job.models.ResourceUsageStatistics
    :param used_vertex_peak_mem_size: The statistics information for the used
     vertex peak memory size.
    :type used_vertex_peak_mem_size:
     ~azure.mgmt.datalake.analytics.job.models.ResourceUsageStatistics
    """

    _validation = {
        'data_read': {'readonly': True},
        'data_read_cross_pod': {'readonly': True},
        'data_read_intra_pod': {'readonly': True},
        'data_to_read': {'readonly': True},
        'data_written': {'readonly': True},
        'duplicate_discard_count': {'readonly': True},
        'failed_count': {'readonly': True},
        'max_vertex_data_read': {'readonly': True},
        'min_vertex_data_read': {'readonly': True},
        'read_failure_count': {'readonly': True},
        'revocation_count': {'readonly': True},
        'running_count': {'readonly': True},
        'scheduled_count': {'readonly': True},
        'stage_name': {'readonly': True},
        'succeeded_count': {'readonly': True},
        'temp_data_written': {'readonly': True},
        'total_count': {'readonly': True},
        'total_failed_time': {'readonly': True},
        'total_progress': {'readonly': True},
        'total_succeeded_time': {'readonly': True},
        'total_peak_mem_usage': {'readonly': True},
        'total_execution_time': {'readonly': True},
        'estimated_vertex_cpu_core_count': {'readonly': True},
        'estimated_vertex_peak_cpu_core_count': {'readonly': True},
        'estimated_vertex_mem_size': {'readonly': True},
    }

    _attribute_map = {
        'data_read': {'key': 'dataRead', 'type': 'long'},
        'data_read_cross_pod': {'key': 'dataReadCrossPod', 'type': 'long'},
        'data_read_intra_pod': {'key': 'dataReadIntraPod', 'type': 'long'},
        'data_to_read': {'key': 'dataToRead', 'type': 'long'},
        'data_written': {'key': 'dataWritten', 'type': 'long'},
        'duplicate_discard_count': {'key': 'duplicateDiscardCount', 'type': 'int'},
        'failed_count': {'key': 'failedCount', 'type': 'int'},
        'max_vertex_data_read': {'key': 'maxVertexDataRead', 'type': 'long'},
        'min_vertex_data_read': {'key': 'minVertexDataRead', 'type': 'long'},
        'read_failure_count': {'key': 'readFailureCount', 'type': 'int'},
        'revocation_count': {'key': 'revocationCount', 'type': 'int'},
        'running_count': {'key': 'runningCount', 'type': 'int'},
        'scheduled_count': {'key': 'scheduledCount', 'type': 'int'},
        'stage_name': {'key': 'stageName', 'type': 'str'},
        'succeeded_count': {'key': 'succeededCount', 'type': 'int'},
        'temp_data_written': {'key': 'tempDataWritten', 'type': 'long'},
        'total_count': {'key': 'totalCount', 'type': 'int'},
        'total_failed_time': {'key': 'totalFailedTime', 'type': 'duration'},
        'total_progress': {'key': 'totalProgress', 'type': 'int'},
        'total_succeeded_time': {'key': 'totalSucceededTime', 'type': 'duration'},
        'total_peak_mem_usage': {'key': 'totalPeakMemUsage', 'type': 'long'},
        'total_execution_time': {'key': 'totalExecutionTime', 'type': 'duration'},
        'max_data_read_vertex': {'key': 'maxDataReadVertex', 'type': 'JobStatisticsVertex'},
        'max_execution_time_vertex': {'key': 'maxExecutionTimeVertex', 'type': 'JobStatisticsVertex'},
        'max_peak_mem_usage_vertex': {'key': 'maxPeakMemUsageVertex', 'type': 'JobStatisticsVertex'},
        'estimated_vertex_cpu_core_count': {'key': 'estimatedVertexCpuCoreCount', 'type': 'int'},
        'estimated_vertex_peak_cpu_core_count': {'key': 'estimatedVertexPeakCpuCoreCount', 'type': 'int'},
        'estimated_vertex_mem_size': {'key': 'estimatedVertexMemSize', 'type': 'long'},
        'allocated_container_cpu_core_count': {'key': 'allocatedContainerCpuCoreCount', 'type': 'ResourceUsageStatistics'},
        'allocated_container_mem_size': {'key': 'allocatedContainerMemSize', 'type': 'ResourceUsageStatistics'},
        'used_vertex_cpu_core_count': {'key': 'usedVertexCpuCoreCount', 'type': 'ResourceUsageStatistics'},
        'used_vertex_peak_mem_size': {'key': 'usedVertexPeakMemSize', 'type': 'ResourceUsageStatistics'},
    }

    def __init__(self, **kwargs):
        super(JobStatisticsVertexStage, self).__init__(**kwargs)
        self.data_read = None
        self.data_read_cross_pod = None
        self.data_read_intra_pod = None
        self.data_to_read = None
        self.data_written = None
        self.duplicate_discard_count = None
        self.failed_count = None
        self.max_vertex_data_read = None
        self.min_vertex_data_read = None
        self.read_failure_count = None
        self.revocation_count = None
        self.running_count = None
        self.scheduled_count = None
        self.stage_name = None
        self.succeeded_count = None
        self.temp_data_written = None
        self.total_count = None
        self.total_failed_time = None
        self.total_progress = None
        self.total_succeeded_time = None
        self.total_peak_mem_usage = None
        self.total_execution_time = None
        self.max_data_read_vertex = kwargs.get('max_data_read_vertex', None)
        self.max_execution_time_vertex = kwargs.get('max_execution_time_vertex', None)
        self.max_peak_mem_usage_vertex = kwargs.get('max_peak_mem_usage_vertex', None)
        self.estimated_vertex_cpu_core_count = None
        self.estimated_vertex_peak_cpu_core_count = None
        self.estimated_vertex_mem_size = None
        self.allocated_container_cpu_core_count = kwargs.get('allocated_container_cpu_core_count', None)
        self.allocated_container_mem_size = kwargs.get('allocated_container_mem_size', None)
        self.used_vertex_cpu_core_count = kwargs.get('used_vertex_cpu_core_count', None)
        self.used_vertex_peak_mem_size = kwargs.get('used_vertex_peak_mem_size', None)


class ResourceUsageStatistics(Model):
    """The statistics information for resource usage.

    Variables are only populated by the server, and will be ignored when
    sending a request.

    :ivar average: The average value.
    :vartype average: float
    :ivar minimum: The minimum value.
    :vartype minimum: long
    :ivar maximum: The maximum value.
    :vartype maximum: long
    """

    _validation = {
        'average': {'readonly': True},
        'minimum': {'readonly': True},
        'maximum': {'readonly': True},
    }

    _attribute_map = {
        'average': {'key': 'average', 'type': 'float'},
        'minimum': {'key': 'minimum', 'type': 'long'},
        'maximum': {'key': 'maximum', 'type': 'long'},
    }

    def __init__(self, **kwargs):
        super(ResourceUsageStatistics, self).__init__(**kwargs)
        self.average = None
        self.minimum = None
        self.maximum = None


class ScopeJobProperties(JobProperties):
    """Scope job properties used when submitting and retrieving Scope jobs. (Only
    for use internally with Scope job type.).

    Variables are only populated by the server, and will be ignored when
    sending a request.

    All required parameters must be populated in order to send to Azure.

    :param runtime_version: The runtime version of the Data Lake Analytics
     engine to use for the specific type of job being run.
    :type runtime_version: str
    :param script: Required. The script to run. Please note that the maximum
     script size is 3 MB.
    :type script: str
    :param type: Required. Constant filled by server.
    :type type: str
    :ivar resources: The list of resources that are required by the job.
    :vartype resources:
     list[~azure.mgmt.datalake.analytics.job.models.ScopeJobResource]
    :ivar user_algebra_path: The algebra file path after the job has
     completed.
    :vartype user_algebra_path: str
    :param notifier: The list of email addresses, separated by semi-colons, to
     notify when the job reaches a terminal state.
    :type notifier: str
    :ivar total_compilation_time: The total time this job spent compiling.
     This value should not be set by the user and will be ignored if it is.
    :vartype total_compilation_time: timedelta
    :ivar total_queued_time: The total time this job spent queued. This value
     should not be set by the user and will be ignored if it is.
    :vartype total_queued_time: timedelta
    :ivar total_running_time: The total time this job spent executing. This
     value should not be set by the user and will be ignored if it is.
    :vartype total_running_time: timedelta
    :ivar total_paused_time: The total time this job spent paused. This value
     should not be set by the user and will be ignored if it is.
    :vartype total_paused_time: timedelta
    :ivar root_process_node_id: The ID used to identify the job manager
     coordinating job execution. This value should not be set by the user and
     will be ignored if it is.
    :vartype root_process_node_id: str
    :ivar yarn_application_id: The ID used to identify the yarn application
     executing the job. This value should not be set by the user and will be
     ignored if it is.
    :vartype yarn_application_id: str
    """

    _validation = {
        'script': {'required': True},
        'type': {'required': True},
        'resources': {'readonly': True},
        'user_algebra_path': {'readonly': True},
        'total_compilation_time': {'readonly': True},
        'total_queued_time': {'readonly': True},
        'total_running_time': {'readonly': True},
        'total_paused_time': {'readonly': True},
        'root_process_node_id': {'readonly': True},
        'yarn_application_id': {'readonly': True},
    }

    _attribute_map = {
        'runtime_version': {'key': 'runtimeVersion', 'type': 'str'},
        'script': {'key': 'script', 'type': 'str'},
        'type': {'key': 'type', 'type': 'str'},
        'resources': {'key': 'resources', 'type': '[ScopeJobResource]'},
        'user_algebra_path': {'key': 'userAlgebraPath', 'type': 'str'},
        'notifier': {'key': 'notifier', 'type': 'str'},
        'total_compilation_time': {'key': 'totalCompilationTime', 'type': 'duration'},
        'total_queued_time': {'key': 'totalQueuedTime', 'type': 'duration'},
        'total_running_time': {'key': 'totalRunningTime', 'type': 'duration'},
        'total_paused_time': {'key': 'totalPausedTime', 'type': 'duration'},
        'root_process_node_id': {'key': 'rootProcessNodeId', 'type': 'str'},
        'yarn_application_id': {'key': 'yarnApplicationId', 'type': 'str'},
    }

    def __init__(self, **kwargs):
        super(ScopeJobProperties, self).__init__(**kwargs)
        self.resources = None
        self.user_algebra_path = None
        self.notifier = kwargs.get('notifier', None)
        self.total_compilation_time = None
        self.total_queued_time = None
        self.total_running_time = None
        self.total_paused_time = None
        self.root_process_node_id = None
        self.yarn_application_id = None
        self.type = 'Scope'


class ScopeJobResource(Model):
    """The Scope job resources. (Only for use internally with Scope job type.).

    :param name: The name of the resource.
    :type name: str
    :param path: The path to the resource.
    :type path: str
    """

    _attribute_map = {
        'name': {'key': 'name', 'type': 'str'},
        'path': {'key': 'path', 'type': 'str'},
    }

    def __init__(self, **kwargs):
        super(ScopeJobResource, self).__init__(**kwargs)
        self.name = kwargs.get('name', None)
        self.path = kwargs.get('path', None)


class UpdateJobParameters(Model):
    """The parameters that can be used to update existing Data Lake Analytics job
    information properties. (Only for use internally with Scope job type.).

    :param degree_of_parallelism: The degree of parallelism used for this job.
    :type degree_of_parallelism: int
    :param degree_of_parallelism_percent: the degree of parallelism in
     percentage used for this job.
    :type degree_of_parallelism_percent: float
    :param priority: The priority value for the current job. Lower numbers
     have a higher priority. By default, a job has a priority of 1000. This
     must be greater than 0.
    :type priority: int
    :param tags: The key-value pairs used to add additional metadata to the
     job information.
    :type tags: dict[str, str]
    """

    _attribute_map = {
        'degree_of_parallelism': {'key': 'degreeOfParallelism', 'type': 'int'},
        'degree_of_parallelism_percent': {'key': 'degreeOfParallelismPercent', 'type': 'float'},
        'priority': {'key': 'priority', 'type': 'int'},
        'tags': {'key': 'tags', 'type': '{str}'},
    }

    def __init__(self, **kwargs):
        super(UpdateJobParameters, self).__init__(**kwargs)
        self.degree_of_parallelism = kwargs.get('degree_of_parallelism', None)
        self.degree_of_parallelism_percent = kwargs.get('degree_of_parallelism_percent', None)
        self.priority = kwargs.get('priority', None)
        self.tags = kwargs.get('tags', None)


class USqlJobProperties(JobProperties):
    """U-SQL job properties used when retrieving U-SQL jobs.

    Variables are only populated by the server, and will be ignored when
    sending a request.

    All required parameters must be populated in order to send to Azure.

    :param runtime_version: The runtime version of the Data Lake Analytics
     engine to use for the specific type of job being run.
    :type runtime_version: str
    :param script: Required. The script to run. Please note that the maximum
     script size is 3 MB.
    :type script: str
    :param type: Required. Constant filled by server.
    :type type: str
    :ivar resources: The list of resources that are required by the job.
    :vartype resources:
     list[~azure.mgmt.datalake.analytics.job.models.JobResource]
    :param statistics: The job specific statistics.
    :type statistics: ~azure.mgmt.datalake.analytics.job.models.JobStatistics
    :param debug_data: The job specific debug data locations.
    :type debug_data: ~azure.mgmt.datalake.analytics.job.models.JobDataPath
    :ivar diagnostics: The diagnostics for the job.
    :vartype diagnostics:
     list[~azure.mgmt.datalake.analytics.job.models.Diagnostics]
    :ivar algebra_file_path: The algebra file path after the job has
     completed.
    :vartype algebra_file_path: str
    :ivar total_compilation_time: The total time this job spent compiling.
     This value should not be set by the user and will be ignored if it is.
    :vartype total_compilation_time: timedelta
    :ivar total_queued_time: The total time this job spent queued. This value
     should not be set by the user and will be ignored if it is.
    :vartype total_queued_time: timedelta
    :ivar total_running_time: The total time this job spent executing. This
     value should not be set by the user and will be ignored if it is.
    :vartype total_running_time: timedelta
    :ivar total_paused_time: The total time this job spent paused. This value
     should not be set by the user and will be ignored if it is.
    :vartype total_paused_time: timedelta
    :ivar root_process_node_id: The ID used to identify the job manager
     coordinating job execution. This value should not be set by the user and
     will be ignored if it is.
    :vartype root_process_node_id: str
    :ivar yarn_application_id: The ID used to identify the yarn application
     executing the job. This value should not be set by the user and will be
     ignored if it is.
    :vartype yarn_application_id: str
    :ivar yarn_application_time_stamp: The timestamp (in ticks) for the yarn
     application executing the job. This value should not be set by the user
     and will be ignored if it is.
    :vartype yarn_application_time_stamp: long
    :ivar compile_mode: The specific compilation mode for the job used during
     execution. If this is not specified during submission, the server will
     determine the optimal compilation mode. Possible values include:
     'Semantic', 'Full', 'SingleBox'
    :vartype compile_mode: str or
     ~azure.mgmt.datalake.analytics.job.models.CompileMode
    """

    _validation = {
        'script': {'required': True},
        'type': {'required': True},
        'resources': {'readonly': True},
        'diagnostics': {'readonly': True},
        'algebra_file_path': {'readonly': True},
        'total_compilation_time': {'readonly': True},
        'total_queued_time': {'readonly': True},
        'total_running_time': {'readonly': True},
        'total_paused_time': {'readonly': True},
        'root_process_node_id': {'readonly': True},
        'yarn_application_id': {'readonly': True},
        'yarn_application_time_stamp': {'readonly': True},
        'compile_mode': {'readonly': True},
    }

    _attribute_map = {
        'runtime_version': {'key': 'runtimeVersion', 'type': 'str'},
        'script': {'key': 'script', 'type': 'str'},
        'type': {'key': 'type', 'type': 'str'},
        'resources': {'key': 'resources', 'type': '[JobResource]'},
        'statistics': {'key': 'statistics', 'type': 'JobStatistics'},
        'debug_data': {'key': 'debugData', 'type': 'JobDataPath'},
        'diagnostics': {'key': 'diagnostics', 'type': '[Diagnostics]'},
        'algebra_file_path': {'key': 'algebraFilePath', 'type': 'str'},
        'total_compilation_time': {'key': 'totalCompilationTime', 'type': 'duration'},
        'total_queued_time': {'key': 'totalQueuedTime', 'type': 'duration'},
        'total_running_time': {'key': 'totalRunningTime', 'type': 'duration'},
        'total_paused_time': {'key': 'totalPausedTime', 'type': 'duration'},
        'root_process_node_id': {'key': 'rootProcessNodeId', 'type': 'str'},
        'yarn_application_id': {'key': 'yarnApplicationId', 'type': 'str'},
        'yarn_application_time_stamp': {'key': 'yarnApplicationTimeStamp', 'type': 'long'},
        'compile_mode': {'key': 'compileMode', 'type': 'CompileMode'},
    }

    def __init__(self, **kwargs):
        super(USqlJobProperties, self).__init__(**kwargs)
        self.resources = None
        self.statistics = kwargs.get('statistics', None)
        self.debug_data = kwargs.get('debug_data', None)
        self.diagnostics = None
        self.algebra_file_path = None
        self.total_compilation_time = None
        self.total_queued_time = None
        self.total_running_time = None
        self.total_paused_time = None
        self.root_process_node_id = None
        self.yarn_application_id = None
        self.yarn_application_time_stamp = None
        self.compile_mode = None
        self.type = 'USql'
