# pylint: disable=too-many-lines,too-many-statements
# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for license information.
# Code generated by Microsoft (R) Python Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------
from io import IOBase
import json
import sys
from typing import Any, Callable, Dict, IO, List, Optional, Type, TypeVar, Union, overload

from azure.core.exceptions import (
    ClientAuthenticationError,
    HttpResponseError,
    ResourceExistsError,
    ResourceNotFoundError,
    ResourceNotModifiedError,
    StreamClosedError,
    StreamConsumedError,
    map_error,
)
from azure.core.pipeline import PipelineResponse
from azure.core.rest import AsyncHttpResponse, HttpRequest
from azure.core.tracing.decorator_async import distributed_trace_async
from azure.core.utils import case_insensitive_dict

from ... import _model_base, models as _models
from ..._model_base import SdkJSONEncoder, _deserialize
from ..._operations._operations import (
    build_face_detect_from_url_request,
    build_face_detect_request,
    build_face_find_similar_request,
    build_face_group_request,
    build_face_session_create_liveness_session_request,
    build_face_session_create_liveness_with_verify_session_request,
    build_face_session_create_liveness_with_verify_session_with_verify_image_request,
    build_face_session_delete_liveness_session_request,
    build_face_session_delete_liveness_with_verify_session_request,
    build_face_session_get_liveness_session_audit_entries_request,
    build_face_session_get_liveness_session_result_request,
    build_face_session_get_liveness_sessions_request,
    build_face_session_get_liveness_with_verify_session_audit_entries_request,
    build_face_session_get_liveness_with_verify_session_result_request,
    build_face_session_get_liveness_with_verify_sessions_request,
    build_face_verify_face_to_face_request,
)
from ..._vendor import FileType, prepare_multipart_form_data
from .._vendor import FaceClientMixinABC, FaceSessionClientMixinABC

if sys.version_info >= (3, 9):
    from collections.abc import MutableMapping
else:
    from typing import MutableMapping  # type: ignore  # pylint: disable=ungrouped-imports
JSON = MutableMapping[str, Any]  # pylint: disable=unsubscriptable-object
_Unset: Any = object()
T = TypeVar("T")
ClsType = Optional[Callable[[PipelineResponse[HttpRequest, AsyncHttpResponse], T, Dict[str, Any]], Any]]


class FaceClientOperationsMixin(FaceClientMixinABC):

    @overload
    async def _detect_from_url(
        self,
        body: JSON,
        *,
        content_type: str = "application/json",
        detection_model: Optional[Union[str, _models.FaceDetectionModel]] = None,
        recognition_model: Optional[Union[str, _models.FaceRecognitionModel]] = None,
        return_face_id: Optional[bool] = None,
        return_face_attributes: Optional[List[Union[str, _models.FaceAttributeType]]] = None,
        return_face_landmarks: Optional[bool] = None,
        return_recognition_model: Optional[bool] = None,
        face_id_time_to_live: Optional[int] = None,
        **kwargs: Any
    ) -> List[_models.FaceDetectionResult]: ...
    @overload
    async def _detect_from_url(
        self,
        *,
        url: str,
        content_type: str = "application/json",
        detection_model: Optional[Union[str, _models.FaceDetectionModel]] = None,
        recognition_model: Optional[Union[str, _models.FaceRecognitionModel]] = None,
        return_face_id: Optional[bool] = None,
        return_face_attributes: Optional[List[Union[str, _models.FaceAttributeType]]] = None,
        return_face_landmarks: Optional[bool] = None,
        return_recognition_model: Optional[bool] = None,
        face_id_time_to_live: Optional[int] = None,
        **kwargs: Any
    ) -> List[_models.FaceDetectionResult]: ...
    @overload
    async def _detect_from_url(
        self,
        body: IO[bytes],
        *,
        content_type: str = "application/json",
        detection_model: Optional[Union[str, _models.FaceDetectionModel]] = None,
        recognition_model: Optional[Union[str, _models.FaceRecognitionModel]] = None,
        return_face_id: Optional[bool] = None,
        return_face_attributes: Optional[List[Union[str, _models.FaceAttributeType]]] = None,
        return_face_landmarks: Optional[bool] = None,
        return_recognition_model: Optional[bool] = None,
        face_id_time_to_live: Optional[int] = None,
        **kwargs: Any
    ) -> List[_models.FaceDetectionResult]: ...

    @distributed_trace_async
    async def _detect_from_url(
        self,
        body: Union[JSON, IO[bytes]] = _Unset,
        *,
        url: str = _Unset,
        detection_model: Optional[Union[str, _models.FaceDetectionModel]] = None,
        recognition_model: Optional[Union[str, _models.FaceRecognitionModel]] = None,
        return_face_id: Optional[bool] = None,
        return_face_attributes: Optional[List[Union[str, _models.FaceAttributeType]]] = None,
        return_face_landmarks: Optional[bool] = None,
        return_recognition_model: Optional[bool] = None,
        face_id_time_to_live: Optional[int] = None,
        **kwargs: Any
    ) -> List[_models.FaceDetectionResult]:
        """Detect human faces in an image, return face rectangles, and optionally with faceIds, landmarks,
        and attributes.

        ..

           [!IMPORTANT]
           To mitigate potential misuse that can subject people to stereotyping, discrimination, or
        unfair denial of services, we are retiring Face API attributes that predict emotion, gender,
        age, smile, facial hair, hair, and makeup. Read more about this decision
        https://azure.microsoft.com/blog/responsible-ai-investments-and-safeguards-for-facial-recognition/.


        *


        * No image will be stored. Only the extracted face feature(s) will be stored on server. The
        faceId is an identifier of the face feature and will be used in "Identify", "Verify", and "Find
        Similar". The stored face features will expire and be deleted at the time specified by
        faceIdTimeToLive after the original detection call.
        * Optional parameters include faceId, landmarks, and attributes. Attributes include headPose,
        glasses, occlusion, accessories, blur, exposure, noise, mask, and qualityForRecognition. Some
        of the results returned for specific attributes may not be highly accurate.
        * JPEG, PNG, GIF (the first frame), and BMP format are supported. The allowed image file size
        is from 1KB to 6MB.
        * The minimum detectable face size is 36x36 pixels in an image no larger than 1920x1080 pixels.
        Images with dimensions higher than 1920x1080 pixels will need a proportionally larger minimum
        face size.
        * Up to 100 faces can be returned for an image. Faces are ranked by face rectangle size from
        large to small.
        * For optimal results when querying "Identify", "Verify", and "Find Similar" ('returnFaceId' is
        true), please use faces that are: frontal, clear, and with a minimum size of 200x200 pixels
        (100 pixels between eyes).
        * Different 'detectionModel' values can be provided. To use and compare different detection
        models, please refer to
        https://learn.microsoft.com/azure/ai-services/computer-vision/how-to/specify-detection-model

          * 'detection_02': Face attributes and landmarks are disabled if you choose this detection
        model.
          * 'detection_03': Face attributes (mask, blur, and headPose) and landmarks are supported if
        you choose this detection model.

        * Different 'recognitionModel' values are provided. If follow-up operations like "Verify",
        "Identify", "Find Similar" are needed, please specify the recognition model with
        'recognitionModel' parameter. The default value for 'recognitionModel' is 'recognition_01', if
        latest model needed, please explicitly specify the model you need in this parameter. Once
        specified, the detected faceIds will be associated with the specified recognition model. More
        details, please refer to
        https://learn.microsoft.com/azure/ai-services/computer-vision/how-to/specify-recognition-model.

        :param body: Is either a JSON type or a IO[bytes] type. Required.
        :type body: JSON or IO[bytes]
        :keyword url: URL of input image. Required.
        :paramtype url: str
        :keyword detection_model: The 'detectionModel' associated with the detected faceIds. Supported
         'detectionModel' values include 'detection_01', 'detection_02' and 'detection_03'. The default
         value is 'detection_01'. Known values are: "detection_01", "detection_02", and "detection_03".
         Default value is None.
        :paramtype detection_model: str or ~azure.ai.vision.face.models.FaceDetectionModel
        :keyword recognition_model: The 'recognitionModel' associated with the detected faceIds.
         Supported 'recognitionModel' values include 'recognition_01', 'recognition_02',
         'recognition_03' or 'recognition_04'. The default value is 'recognition_01'. 'recognition_04'
         is recommended since its accuracy is improved on faces wearing masks compared with
         'recognition_03', and its overall accuracy is improved compared with 'recognition_01' and
         'recognition_02'. Known values are: "recognition_01", "recognition_02", "recognition_03", and
         "recognition_04". Default value is None.
        :paramtype recognition_model: str or ~azure.ai.vision.face.models.FaceRecognitionModel
        :keyword return_face_id: Return faceIds of the detected faces or not. The default value is
         true. Default value is None.
        :paramtype return_face_id: bool
        :keyword return_face_attributes: Analyze and return the one or more specified face attributes
         in the comma-separated string like 'returnFaceAttributes=headPose,glasses'. Face attribute
         analysis has additional computational and time cost. Default value is None.
        :paramtype return_face_attributes: list[str or ~azure.ai.vision.face.models.FaceAttributeType]
        :keyword return_face_landmarks: Return face landmarks of the detected faces or not. The default
         value is false. Default value is None.
        :paramtype return_face_landmarks: bool
        :keyword return_recognition_model: Return 'recognitionModel' or not. The default value is
         false. This is only applicable when returnFaceId = true. Default value is None.
        :paramtype return_recognition_model: bool
        :keyword face_id_time_to_live: The number of seconds for the face ID being cached. Supported
         range from 60 seconds up to 86400 seconds. The default value is 86400 (24 hours). Default value
         is None.
        :paramtype face_id_time_to_live: int
        :return: list of FaceDetectionResult
        :rtype: list[~azure.ai.vision.face.models.FaceDetectionResult]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("content-type", None))
        cls: ClsType[List[_models.FaceDetectionResult]] = kwargs.pop("cls", None)

        if body is _Unset:
            if url is _Unset:
                raise TypeError("missing required argument: url")
            body = {"url": url}
            body = {k: v for k, v in body.items() if v is not None}
        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IOBase, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_face_detect_from_url_request(
            detection_model=detection_model,
            recognition_model=recognition_model,
            return_face_id=return_face_id,
            return_face_attributes=return_face_attributes,
            return_face_landmarks=return_face_landmarks,
            return_recognition_model=return_recognition_model,
            face_id_time_to_live=face_id_time_to_live,
            content_type=content_type,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
            "apiVersion": self._serialize.url("self._config.api_version", self._config.api_version, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _deserialize(_models.FaceErrorResponse, response.json())
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(List[_models.FaceDetectionResult], response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def _detect(
        self,
        image_content: bytes,
        *,
        detection_model: Optional[Union[str, _models.FaceDetectionModel]] = None,
        recognition_model: Optional[Union[str, _models.FaceRecognitionModel]] = None,
        return_face_id: Optional[bool] = None,
        return_face_attributes: Optional[List[Union[str, _models.FaceAttributeType]]] = None,
        return_face_landmarks: Optional[bool] = None,
        return_recognition_model: Optional[bool] = None,
        face_id_time_to_live: Optional[int] = None,
        **kwargs: Any
    ) -> List[_models.FaceDetectionResult]:
        """Detect human faces in an image, return face rectangles, and optionally with faceIds, landmarks,
        and attributes.

        ..

           [!IMPORTANT]
           To mitigate potential misuse that can subject people to stereotyping, discrimination, or
        unfair denial of services, we are retiring Face API attributes that predict emotion, gender,
        age, smile, facial hair, hair, and makeup. Read more about this decision
        https://azure.microsoft.com/blog/responsible-ai-investments-and-safeguards-for-facial-recognition/.


        *


        * No image will be stored. Only the extracted face feature(s) will be stored on server. The
        faceId is an identifier of the face feature and will be used in "Identify", "Verify", and "Find
        Similar". The stored face features will expire and be deleted at the time specified by
        faceIdTimeToLive after the original detection call.
        * Optional parameters include faceId, landmarks, and attributes. Attributes include headPose,
        glasses, occlusion, accessories, blur, exposure, noise, mask, and qualityForRecognition. Some
        of the results returned for specific attributes may not be highly accurate.
        * JPEG, PNG, GIF (the first frame), and BMP format are supported. The allowed image file size
        is from 1KB to 6MB.
        * The minimum detectable face size is 36x36 pixels in an image no larger than 1920x1080 pixels.
        Images with dimensions higher than 1920x1080 pixels will need a proportionally larger minimum
        face size.
        * Up to 100 faces can be returned for an image. Faces are ranked by face rectangle size from
        large to small.
        * For optimal results when querying "Identify", "Verify", and "Find Similar" ('returnFaceId' is
        true), please use faces that are: frontal, clear, and with a minimum size of 200x200 pixels
        (100 pixels between eyes).
        * Different 'detectionModel' values can be provided. To use and compare different detection
        models, please refer to
        https://learn.microsoft.com/azure/ai-services/computer-vision/how-to/specify-detection-model

          * 'detection_02': Face attributes and landmarks are disabled if you choose this detection
        model.
          * 'detection_03': Face attributes (mask, blur, and headPose) and landmarks are supported if
        you choose this detection model.

        * Different 'recognitionModel' values are provided. If follow-up operations like "Verify",
        "Identify", "Find Similar" are needed, please specify the recognition model with
        'recognitionModel' parameter. The default value for 'recognitionModel' is 'recognition_01', if
        latest model needed, please explicitly specify the model you need in this parameter. Once
        specified, the detected faceIds will be associated with the specified recognition model. More
        details, please refer to
        https://learn.microsoft.com/azure/ai-services/computer-vision/how-to/specify-recognition-model.

        :param image_content: The input image binary. Required.
        :type image_content: bytes
        :keyword detection_model: The 'detectionModel' associated with the detected faceIds. Supported
         'detectionModel' values include 'detection_01', 'detection_02' and 'detection_03'. The default
         value is 'detection_01'. Known values are: "detection_01", "detection_02", and "detection_03".
         Default value is None.
        :paramtype detection_model: str or ~azure.ai.vision.face.models.FaceDetectionModel
        :keyword recognition_model: The 'recognitionModel' associated with the detected faceIds.
         Supported 'recognitionModel' values include 'recognition_01', 'recognition_02',
         'recognition_03' or 'recognition_04'. The default value is 'recognition_01'. 'recognition_04'
         is recommended since its accuracy is improved on faces wearing masks compared with
         'recognition_03', and its overall accuracy is improved compared with 'recognition_01' and
         'recognition_02'. Known values are: "recognition_01", "recognition_02", "recognition_03", and
         "recognition_04". Default value is None.
        :paramtype recognition_model: str or ~azure.ai.vision.face.models.FaceRecognitionModel
        :keyword return_face_id: Return faceIds of the detected faces or not. The default value is
         true. Default value is None.
        :paramtype return_face_id: bool
        :keyword return_face_attributes: Analyze and return the one or more specified face attributes
         in the comma-separated string like 'returnFaceAttributes=headPose,glasses'. Face attribute
         analysis has additional computational and time cost. Default value is None.
        :paramtype return_face_attributes: list[str or ~azure.ai.vision.face.models.FaceAttributeType]
        :keyword return_face_landmarks: Return face landmarks of the detected faces or not. The default
         value is false. Default value is None.
        :paramtype return_face_landmarks: bool
        :keyword return_recognition_model: Return 'recognitionModel' or not. The default value is
         false. This is only applicable when returnFaceId = true. Default value is None.
        :paramtype return_recognition_model: bool
        :keyword face_id_time_to_live: The number of seconds for the face ID being cached. Supported
         range from 60 seconds up to 86400 seconds. The default value is 86400 (24 hours). Default value
         is None.
        :paramtype face_id_time_to_live: int
        :return: list of FaceDetectionResult
        :rtype: list[~azure.ai.vision.face.models.FaceDetectionResult]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: str = kwargs.pop("content_type", _headers.pop("content-type", "application/octet-stream"))
        cls: ClsType[List[_models.FaceDetectionResult]] = kwargs.pop("cls", None)

        _content = image_content

        _request = build_face_detect_request(
            detection_model=detection_model,
            recognition_model=recognition_model,
            return_face_id=return_face_id,
            return_face_attributes=return_face_attributes,
            return_face_landmarks=return_face_landmarks,
            return_recognition_model=return_recognition_model,
            face_id_time_to_live=face_id_time_to_live,
            content_type=content_type,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
            "apiVersion": self._serialize.url("self._config.api_version", self._config.api_version, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _deserialize(_models.FaceErrorResponse, response.json())
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(List[_models.FaceDetectionResult], response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def find_similar(
        self, body: JSON, *, content_type: str = "application/json", **kwargs: Any
    ) -> List[_models.FaceFindSimilarResult]:
        """Given query face's faceId, to search the similar-looking faces from a faceId array. A faceId
        array contains the faces created by Detect.

        Depending on the input the returned similar faces list contains faceIds or persistedFaceIds
        ranked by similarity.

        Find similar has two working modes, "matchPerson" and "matchFace". "matchPerson" is the default
        mode that it tries to find faces of the same person as possible by using internal same-person
        thresholds. It is useful to find a known person's other photos. Note that an empty list will be
        returned if no faces pass the internal thresholds. "matchFace" mode ignores same-person
        thresholds and returns ranked similar faces anyway, even the similarity is low. It can be used
        in the cases like searching celebrity-looking faces.

        The 'recognitionModel' associated with the query faceId should be the same as the
        'recognitionModel' used by the target faceId array.

        :param body: Required.
        :type body: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: list of FaceFindSimilarResult
        :rtype: list[~azure.ai.vision.face.models.FaceFindSimilarResult]
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def find_similar(
        self,
        *,
        face_id: str,
        face_ids: List[str],
        content_type: str = "application/json",
        max_num_of_candidates_returned: Optional[int] = None,
        mode: Optional[Union[str, _models.FindSimilarMatchMode]] = None,
        **kwargs: Any
    ) -> List[_models.FaceFindSimilarResult]:
        """Given query face's faceId, to search the similar-looking faces from a faceId array. A faceId
        array contains the faces created by Detect.

        Depending on the input the returned similar faces list contains faceIds or persistedFaceIds
        ranked by similarity.

        Find similar has two working modes, "matchPerson" and "matchFace". "matchPerson" is the default
        mode that it tries to find faces of the same person as possible by using internal same-person
        thresholds. It is useful to find a known person's other photos. Note that an empty list will be
        returned if no faces pass the internal thresholds. "matchFace" mode ignores same-person
        thresholds and returns ranked similar faces anyway, even the similarity is low. It can be used
        in the cases like searching celebrity-looking faces.

        The 'recognitionModel' associated with the query faceId should be the same as the
        'recognitionModel' used by the target faceId array.

        :keyword face_id: faceId of the query face. User needs to call "Detect" first to get a valid
         faceId. Note that this faceId is not persisted and will expire 24 hours after the detection
         call. Required.
        :paramtype face_id: str
        :keyword face_ids: An array of candidate faceIds. All of them are created by "Detect" and the
         faceIds will expire 24 hours after the detection call. The number of faceIds is limited to
         1000. Required.
        :paramtype face_ids: list[str]
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :keyword max_num_of_candidates_returned: The number of top similar faces returned. The valid
         range is [1, 1000]. Default value is 20. Default value is None.
        :paramtype max_num_of_candidates_returned: int
        :keyword mode: Similar face searching mode. It can be 'matchPerson' or 'matchFace'. Default
         value is 'matchPerson'. Known values are: "matchPerson" and "matchFace". Default value is None.
        :paramtype mode: str or ~azure.ai.vision.face.models.FindSimilarMatchMode
        :return: list of FaceFindSimilarResult
        :rtype: list[~azure.ai.vision.face.models.FaceFindSimilarResult]
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def find_similar(
        self, body: IO[bytes], *, content_type: str = "application/json", **kwargs: Any
    ) -> List[_models.FaceFindSimilarResult]:
        """Given query face's faceId, to search the similar-looking faces from a faceId array. A faceId
        array contains the faces created by Detect.

        Depending on the input the returned similar faces list contains faceIds or persistedFaceIds
        ranked by similarity.

        Find similar has two working modes, "matchPerson" and "matchFace". "matchPerson" is the default
        mode that it tries to find faces of the same person as possible by using internal same-person
        thresholds. It is useful to find a known person's other photos. Note that an empty list will be
        returned if no faces pass the internal thresholds. "matchFace" mode ignores same-person
        thresholds and returns ranked similar faces anyway, even the similarity is low. It can be used
        in the cases like searching celebrity-looking faces.

        The 'recognitionModel' associated with the query faceId should be the same as the
        'recognitionModel' used by the target faceId array.

        :param body: Required.
        :type body: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: list of FaceFindSimilarResult
        :rtype: list[~azure.ai.vision.face.models.FaceFindSimilarResult]
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    async def find_similar(
        self,
        body: Union[JSON, IO[bytes]] = _Unset,
        *,
        face_id: str = _Unset,
        face_ids: List[str] = _Unset,
        max_num_of_candidates_returned: Optional[int] = None,
        mode: Optional[Union[str, _models.FindSimilarMatchMode]] = None,
        **kwargs: Any
    ) -> List[_models.FaceFindSimilarResult]:
        """Given query face's faceId, to search the similar-looking faces from a faceId array. A faceId
        array contains the faces created by Detect.

        Depending on the input the returned similar faces list contains faceIds or persistedFaceIds
        ranked by similarity.

        Find similar has two working modes, "matchPerson" and "matchFace". "matchPerson" is the default
        mode that it tries to find faces of the same person as possible by using internal same-person
        thresholds. It is useful to find a known person's other photos. Note that an empty list will be
        returned if no faces pass the internal thresholds. "matchFace" mode ignores same-person
        thresholds and returns ranked similar faces anyway, even the similarity is low. It can be used
        in the cases like searching celebrity-looking faces.

        The 'recognitionModel' associated with the query faceId should be the same as the
        'recognitionModel' used by the target faceId array.

        :param body: Is either a JSON type or a IO[bytes] type. Required.
        :type body: JSON or IO[bytes]
        :keyword face_id: faceId of the query face. User needs to call "Detect" first to get a valid
         faceId. Note that this faceId is not persisted and will expire 24 hours after the detection
         call. Required.
        :paramtype face_id: str
        :keyword face_ids: An array of candidate faceIds. All of them are created by "Detect" and the
         faceIds will expire 24 hours after the detection call. The number of faceIds is limited to
         1000. Required.
        :paramtype face_ids: list[str]
        :keyword max_num_of_candidates_returned: The number of top similar faces returned. The valid
         range is [1, 1000]. Default value is 20. Default value is None.
        :paramtype max_num_of_candidates_returned: int
        :keyword mode: Similar face searching mode. It can be 'matchPerson' or 'matchFace'. Default
         value is 'matchPerson'. Known values are: "matchPerson" and "matchFace". Default value is None.
        :paramtype mode: str or ~azure.ai.vision.face.models.FindSimilarMatchMode
        :return: list of FaceFindSimilarResult
        :rtype: list[~azure.ai.vision.face.models.FaceFindSimilarResult]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[List[_models.FaceFindSimilarResult]] = kwargs.pop("cls", None)

        if body is _Unset:
            if face_id is _Unset:
                raise TypeError("missing required argument: face_id")
            if face_ids is _Unset:
                raise TypeError("missing required argument: face_ids")
            body = {
                "faceId": face_id,
                "faceIds": face_ids,
                "maxNumOfCandidatesReturned": max_num_of_candidates_returned,
                "mode": mode,
            }
            body = {k: v for k, v in body.items() if v is not None}
        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IOBase, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_face_find_similar_request(
            content_type=content_type,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
            "apiVersion": self._serialize.url("self._config.api_version", self._config.api_version, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _deserialize(_models.FaceErrorResponse, response.json())
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(List[_models.FaceFindSimilarResult], response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def verify_face_to_face(
        self, body: JSON, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.FaceVerificationResult:
        """Verify whether two faces belong to a same person.

        ..

           [!NOTE]

           *


           * Higher face image quality means better identification precision. Please consider
        high-quality faces: frontal, clear, and face size is 200x200 pixels (100 pixels between eyes)
        or bigger.
           * For the scenarios that are sensitive to accuracy please make your own judgment.
           * The 'recognitionModel' associated with the both faces should be the same.

        :param body: Required.
        :type body: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: FaceVerificationResult. The FaceVerificationResult is compatible with MutableMapping
        :rtype: ~azure.ai.vision.face.models.FaceVerificationResult
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def verify_face_to_face(
        self, *, face_id1: str, face_id2: str, content_type: str = "application/json", **kwargs: Any
    ) -> _models.FaceVerificationResult:
        """Verify whether two faces belong to a same person.

        ..

           [!NOTE]

           *


           * Higher face image quality means better identification precision. Please consider
        high-quality faces: frontal, clear, and face size is 200x200 pixels (100 pixels between eyes)
        or bigger.
           * For the scenarios that are sensitive to accuracy please make your own judgment.
           * The 'recognitionModel' associated with the both faces should be the same.

        :keyword face_id1: The faceId of one face, come from "Detect". Required.
        :paramtype face_id1: str
        :keyword face_id2: The faceId of another face, come from "Detect". Required.
        :paramtype face_id2: str
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: FaceVerificationResult. The FaceVerificationResult is compatible with MutableMapping
        :rtype: ~azure.ai.vision.face.models.FaceVerificationResult
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def verify_face_to_face(
        self, body: IO[bytes], *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.FaceVerificationResult:
        """Verify whether two faces belong to a same person.

        ..

           [!NOTE]

           *


           * Higher face image quality means better identification precision. Please consider
        high-quality faces: frontal, clear, and face size is 200x200 pixels (100 pixels between eyes)
        or bigger.
           * For the scenarios that are sensitive to accuracy please make your own judgment.
           * The 'recognitionModel' associated with the both faces should be the same.

        :param body: Required.
        :type body: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: FaceVerificationResult. The FaceVerificationResult is compatible with MutableMapping
        :rtype: ~azure.ai.vision.face.models.FaceVerificationResult
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    async def verify_face_to_face(
        self, body: Union[JSON, IO[bytes]] = _Unset, *, face_id1: str = _Unset, face_id2: str = _Unset, **kwargs: Any
    ) -> _models.FaceVerificationResult:
        """Verify whether two faces belong to a same person.

        ..

           [!NOTE]

           *


           * Higher face image quality means better identification precision. Please consider
        high-quality faces: frontal, clear, and face size is 200x200 pixels (100 pixels between eyes)
        or bigger.
           * For the scenarios that are sensitive to accuracy please make your own judgment.
           * The 'recognitionModel' associated with the both faces should be the same.

        :param body: Is either a JSON type or a IO[bytes] type. Required.
        :type body: JSON or IO[bytes]
        :keyword face_id1: The faceId of one face, come from "Detect". Required.
        :paramtype face_id1: str
        :keyword face_id2: The faceId of another face, come from "Detect". Required.
        :paramtype face_id2: str
        :return: FaceVerificationResult. The FaceVerificationResult is compatible with MutableMapping
        :rtype: ~azure.ai.vision.face.models.FaceVerificationResult
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.FaceVerificationResult] = kwargs.pop("cls", None)

        if body is _Unset:
            if face_id1 is _Unset:
                raise TypeError("missing required argument: face_id1")
            if face_id2 is _Unset:
                raise TypeError("missing required argument: face_id2")
            body = {"faceId1": face_id1, "faceId2": face_id2}
            body = {k: v for k, v in body.items() if v is not None}
        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IOBase, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_face_verify_face_to_face_request(
            content_type=content_type,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
            "apiVersion": self._serialize.url("self._config.api_version", self._config.api_version, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _deserialize(_models.FaceErrorResponse, response.json())
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.FaceVerificationResult, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def group(
        self, body: JSON, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.FaceGroupingResult:
        """Divide candidate faces into groups based on face similarity.

        >
        *


        * The output is one or more disjointed face groups and a messyGroup. A face group contains
        faces that have similar looking, often of the same person. Face groups are ranked by group
        size, i.e. number of faces. Notice that faces belonging to a same person might be split into
        several groups in the result.
        * MessyGroup is a special face group containing faces that cannot find any similar counterpart
        face from original faces. The messyGroup will not appear in the result if all faces found their
        counterparts.
        * Group API needs at least 2 candidate faces and 1000 at most. We suggest to try "Verify Face
        To Face" when you only have 2 candidate faces.
        * The 'recognitionModel' associated with the query faces' faceIds should be the same.

        :param body: Required.
        :type body: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: FaceGroupingResult. The FaceGroupingResult is compatible with MutableMapping
        :rtype: ~azure.ai.vision.face.models.FaceGroupingResult
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def group(
        self, *, face_ids: List[str], content_type: str = "application/json", **kwargs: Any
    ) -> _models.FaceGroupingResult:
        """Divide candidate faces into groups based on face similarity.

        >
        *


        * The output is one or more disjointed face groups and a messyGroup. A face group contains
        faces that have similar looking, often of the same person. Face groups are ranked by group
        size, i.e. number of faces. Notice that faces belonging to a same person might be split into
        several groups in the result.
        * MessyGroup is a special face group containing faces that cannot find any similar counterpart
        face from original faces. The messyGroup will not appear in the result if all faces found their
        counterparts.
        * Group API needs at least 2 candidate faces and 1000 at most. We suggest to try "Verify Face
        To Face" when you only have 2 candidate faces.
        * The 'recognitionModel' associated with the query faces' faceIds should be the same.

        :keyword face_ids: Array of candidate faceIds created by "Detect". The maximum is 1000 faces.
         Required.
        :paramtype face_ids: list[str]
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: FaceGroupingResult. The FaceGroupingResult is compatible with MutableMapping
        :rtype: ~azure.ai.vision.face.models.FaceGroupingResult
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def group(
        self, body: IO[bytes], *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.FaceGroupingResult:
        """Divide candidate faces into groups based on face similarity.

        >
        *


        * The output is one or more disjointed face groups and a messyGroup. A face group contains
        faces that have similar looking, often of the same person. Face groups are ranked by group
        size, i.e. number of faces. Notice that faces belonging to a same person might be split into
        several groups in the result.
        * MessyGroup is a special face group containing faces that cannot find any similar counterpart
        face from original faces. The messyGroup will not appear in the result if all faces found their
        counterparts.
        * Group API needs at least 2 candidate faces and 1000 at most. We suggest to try "Verify Face
        To Face" when you only have 2 candidate faces.
        * The 'recognitionModel' associated with the query faces' faceIds should be the same.

        :param body: Required.
        :type body: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: FaceGroupingResult. The FaceGroupingResult is compatible with MutableMapping
        :rtype: ~azure.ai.vision.face.models.FaceGroupingResult
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    async def group(
        self, body: Union[JSON, IO[bytes]] = _Unset, *, face_ids: List[str] = _Unset, **kwargs: Any
    ) -> _models.FaceGroupingResult:
        """Divide candidate faces into groups based on face similarity.

        >
        *


        * The output is one or more disjointed face groups and a messyGroup. A face group contains
        faces that have similar looking, often of the same person. Face groups are ranked by group
        size, i.e. number of faces. Notice that faces belonging to a same person might be split into
        several groups in the result.
        * MessyGroup is a special face group containing faces that cannot find any similar counterpart
        face from original faces. The messyGroup will not appear in the result if all faces found their
        counterparts.
        * Group API needs at least 2 candidate faces and 1000 at most. We suggest to try "Verify Face
        To Face" when you only have 2 candidate faces.
        * The 'recognitionModel' associated with the query faces' faceIds should be the same.

        :param body: Is either a JSON type or a IO[bytes] type. Required.
        :type body: JSON or IO[bytes]
        :keyword face_ids: Array of candidate faceIds created by "Detect". The maximum is 1000 faces.
         Required.
        :paramtype face_ids: list[str]
        :return: FaceGroupingResult. The FaceGroupingResult is compatible with MutableMapping
        :rtype: ~azure.ai.vision.face.models.FaceGroupingResult
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.FaceGroupingResult] = kwargs.pop("cls", None)

        if body is _Unset:
            if face_ids is _Unset:
                raise TypeError("missing required argument: face_ids")
            body = {"faceIds": face_ids}
            body = {k: v for k, v in body.items() if v is not None}
        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IOBase, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_face_group_request(
            content_type=content_type,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
            "apiVersion": self._serialize.url("self._config.api_version", self._config.api_version, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _deserialize(_models.FaceErrorResponse, response.json())
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.FaceGroupingResult, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore


class FaceSessionClientOperationsMixin(FaceSessionClientMixinABC):

    @overload
    async def create_liveness_session(
        self, body: JSON, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.CreateLivenessSessionResult:
        """Create a new detect liveness session.

        A session is best for client device scenarios where developers want to authorize a client
        device to perform only a liveness detection without granting full access to their resource.
        Created sessions have a limited life span and only authorize clients to perform the desired
        action before access is expired.

        Permissions includes...
        >
        *


        * Ability to call /detectLiveness/singleModal for up to 3 retries.
        * A token lifetime of 10 minutes.

        ..

           [!NOTE]
           Client access can be revoked by deleting the session using the Delete Liveness Session
        operation. To retrieve a result, use the Get Liveness Session. To audit the individual requests
        that a client has made to your resource, use the List Liveness Session Audit Entries.

        :param body: Required.
        :type body: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: CreateLivenessSessionResult. The CreateLivenessSessionResult is compatible with
         MutableMapping
        :rtype: ~azure.ai.vision.face.models.CreateLivenessSessionResult
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def create_liveness_session(
        self,
        *,
        liveness_operation_mode: Union[str, _models.LivenessOperationMode],
        content_type: str = "application/json",
        send_results_to_client: Optional[bool] = None,
        device_correlation_id_set_in_client: Optional[bool] = None,
        device_correlation_id: Optional[str] = None,
        auth_token_time_to_live_in_seconds: Optional[int] = None,
        **kwargs: Any
    ) -> _models.CreateLivenessSessionResult:
        """Create a new detect liveness session.

        A session is best for client device scenarios where developers want to authorize a client
        device to perform only a liveness detection without granting full access to their resource.
        Created sessions have a limited life span and only authorize clients to perform the desired
        action before access is expired.

        Permissions includes...
        >
        *


        * Ability to call /detectLiveness/singleModal for up to 3 retries.
        * A token lifetime of 10 minutes.

        ..

           [!NOTE]
           Client access can be revoked by deleting the session using the Delete Liveness Session
        operation. To retrieve a result, use the Get Liveness Session. To audit the individual requests
        that a client has made to your resource, use the List Liveness Session Audit Entries.

        :keyword liveness_operation_mode: Type of liveness mode the client should follow. Known values
         are: "Passive" and "PassiveActive". Required.
        :paramtype liveness_operation_mode: str or ~azure.ai.vision.face.models.LivenessOperationMode
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :keyword send_results_to_client: Whether or not to allow a '200 - Success' response body to be
         sent to the client, which may be undesirable for security reasons. Default is false, clients
         will receive a '204 - NoContent' empty body response. Regardless of selection, calling Session
         GetResult will always contain a response body enabling business logic to be implemented.
         Default value is None.
        :paramtype send_results_to_client: bool
        :keyword device_correlation_id_set_in_client: Whether or not to allow client to set their own
         'deviceCorrelationId' via the Vision SDK. Default is false, and 'deviceCorrelationId' must be
         set in this request body. Default value is None.
        :paramtype device_correlation_id_set_in_client: bool
        :keyword device_correlation_id: Unique Guid per each end-user device. This is to provide rate
         limiting and anti-hammering. If 'deviceCorrelationIdSetInClient' is true in this request, this
         'deviceCorrelationId' must be null. Default value is None.
        :paramtype device_correlation_id: str
        :keyword auth_token_time_to_live_in_seconds: Seconds the session should last for. Range is 60
         to 86400 seconds. Default value is 600. Default value is None.
        :paramtype auth_token_time_to_live_in_seconds: int
        :return: CreateLivenessSessionResult. The CreateLivenessSessionResult is compatible with
         MutableMapping
        :rtype: ~azure.ai.vision.face.models.CreateLivenessSessionResult
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def create_liveness_session(
        self, body: IO[bytes], *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.CreateLivenessSessionResult:
        """Create a new detect liveness session.

        A session is best for client device scenarios where developers want to authorize a client
        device to perform only a liveness detection without granting full access to their resource.
        Created sessions have a limited life span and only authorize clients to perform the desired
        action before access is expired.

        Permissions includes...
        >
        *


        * Ability to call /detectLiveness/singleModal for up to 3 retries.
        * A token lifetime of 10 minutes.

        ..

           [!NOTE]
           Client access can be revoked by deleting the session using the Delete Liveness Session
        operation. To retrieve a result, use the Get Liveness Session. To audit the individual requests
        that a client has made to your resource, use the List Liveness Session Audit Entries.

        :param body: Required.
        :type body: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: CreateLivenessSessionResult. The CreateLivenessSessionResult is compatible with
         MutableMapping
        :rtype: ~azure.ai.vision.face.models.CreateLivenessSessionResult
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    async def create_liveness_session(
        self,
        body: Union[JSON, IO[bytes]] = _Unset,
        *,
        liveness_operation_mode: Union[str, _models.LivenessOperationMode] = _Unset,
        send_results_to_client: Optional[bool] = None,
        device_correlation_id_set_in_client: Optional[bool] = None,
        device_correlation_id: Optional[str] = None,
        auth_token_time_to_live_in_seconds: Optional[int] = None,
        **kwargs: Any
    ) -> _models.CreateLivenessSessionResult:
        """Create a new detect liveness session.

        A session is best for client device scenarios where developers want to authorize a client
        device to perform only a liveness detection without granting full access to their resource.
        Created sessions have a limited life span and only authorize clients to perform the desired
        action before access is expired.

        Permissions includes...
        >
        *


        * Ability to call /detectLiveness/singleModal for up to 3 retries.
        * A token lifetime of 10 minutes.

        ..

           [!NOTE]
           Client access can be revoked by deleting the session using the Delete Liveness Session
        operation. To retrieve a result, use the Get Liveness Session. To audit the individual requests
        that a client has made to your resource, use the List Liveness Session Audit Entries.

        :param body: Is either a JSON type or a IO[bytes] type. Required.
        :type body: JSON or IO[bytes]
        :keyword liveness_operation_mode: Type of liveness mode the client should follow. Known values
         are: "Passive" and "PassiveActive". Required.
        :paramtype liveness_operation_mode: str or ~azure.ai.vision.face.models.LivenessOperationMode
        :keyword send_results_to_client: Whether or not to allow a '200 - Success' response body to be
         sent to the client, which may be undesirable for security reasons. Default is false, clients
         will receive a '204 - NoContent' empty body response. Regardless of selection, calling Session
         GetResult will always contain a response body enabling business logic to be implemented.
         Default value is None.
        :paramtype send_results_to_client: bool
        :keyword device_correlation_id_set_in_client: Whether or not to allow client to set their own
         'deviceCorrelationId' via the Vision SDK. Default is false, and 'deviceCorrelationId' must be
         set in this request body. Default value is None.
        :paramtype device_correlation_id_set_in_client: bool
        :keyword device_correlation_id: Unique Guid per each end-user device. This is to provide rate
         limiting and anti-hammering. If 'deviceCorrelationIdSetInClient' is true in this request, this
         'deviceCorrelationId' must be null. Default value is None.
        :paramtype device_correlation_id: str
        :keyword auth_token_time_to_live_in_seconds: Seconds the session should last for. Range is 60
         to 86400 seconds. Default value is 600. Default value is None.
        :paramtype auth_token_time_to_live_in_seconds: int
        :return: CreateLivenessSessionResult. The CreateLivenessSessionResult is compatible with
         MutableMapping
        :rtype: ~azure.ai.vision.face.models.CreateLivenessSessionResult
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.CreateLivenessSessionResult] = kwargs.pop("cls", None)

        if body is _Unset:
            if liveness_operation_mode is _Unset:
                raise TypeError("missing required argument: liveness_operation_mode")
            body = {
                "authTokenTimeToLiveInSeconds": auth_token_time_to_live_in_seconds,
                "deviceCorrelationId": device_correlation_id,
                "deviceCorrelationIdSetInClient": device_correlation_id_set_in_client,
                "livenessOperationMode": liveness_operation_mode,
                "sendResultsToClient": send_results_to_client,
            }
            body = {k: v for k, v in body.items() if v is not None}
        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IOBase, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_face_session_create_liveness_session_request(
            content_type=content_type,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
            "apiVersion": self._serialize.url("self._config.api_version", self._config.api_version, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _deserialize(_models.FaceErrorResponse, response.json())
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.CreateLivenessSessionResult, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def delete_liveness_session(  # pylint: disable=inconsistent-return-statements
        self, session_id: str, **kwargs: Any
    ) -> None:
        """Delete all session related information for matching the specified session id.

        ..

           [!NOTE]
           Deleting a session deactivates the Session Auth Token by blocking future API calls made with
        that Auth Token. While this can be used to remove any access for that token, those requests
        will still count towards overall resource rate limits. It's best to leverage TokenTTL to limit
        length of tokens in the case that it is misused.

        :param session_id: The unique ID to reference this session. Required.
        :type session_id: str
        :return: None
        :rtype: None
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[None] = kwargs.pop("cls", None)

        _request = build_face_session_delete_liveness_session_request(
            session_id=session_id,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
            "apiVersion": self._serialize.url("self._config.api_version", self._config.api_version, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = False
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _deserialize(_models.FaceErrorResponse, response.json())
            raise HttpResponseError(response=response, model=error)

        if cls:
            return cls(pipeline_response, None, {})  # type: ignore

    @distributed_trace_async
    async def get_liveness_session_result(self, session_id: str, **kwargs: Any) -> _models.LivenessSession:
        """Get session result of detectLiveness/singleModal call.

        :param session_id: The unique ID to reference this session. Required.
        :type session_id: str
        :return: LivenessSession. The LivenessSession is compatible with MutableMapping
        :rtype: ~azure.ai.vision.face.models.LivenessSession
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.LivenessSession] = kwargs.pop("cls", None)

        _request = build_face_session_get_liveness_session_result_request(
            session_id=session_id,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
            "apiVersion": self._serialize.url("self._config.api_version", self._config.api_version, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _deserialize(_models.FaceErrorResponse, response.json())
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.LivenessSession, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def get_liveness_sessions(
        self, *, start: Optional[str] = None, top: Optional[int] = None, **kwargs: Any
    ) -> List[_models.LivenessSessionItem]:
        """Lists sessions for /detectLiveness/SingleModal.

        List sessions from the last sessionId greater than the 'start'.

        The result should be ordered by sessionId in ascending order.

        :keyword start: List resources greater than the "start". It contains no more than 64
         characters. Default is empty. Default value is None.
        :paramtype start: str
        :keyword top: The number of items to list, ranging in [1, 1000]. Default is 1000. Default value
         is None.
        :paramtype top: int
        :return: list of LivenessSessionItem
        :rtype: list[~azure.ai.vision.face.models.LivenessSessionItem]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[List[_models.LivenessSessionItem]] = kwargs.pop("cls", None)

        _request = build_face_session_get_liveness_sessions_request(
            start=start,
            top=top,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
            "apiVersion": self._serialize.url("self._config.api_version", self._config.api_version, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _deserialize(_models.FaceErrorResponse, response.json())
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(List[_models.LivenessSessionItem], response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def get_liveness_session_audit_entries(
        self, session_id: str, *, start: Optional[str] = None, top: Optional[int] = None, **kwargs: Any
    ) -> List[_models.LivenessSessionAuditEntry]:
        """Gets session requests and response body for the session.

        :param session_id: The unique ID to reference this session. Required.
        :type session_id: str
        :keyword start: List resources greater than the "start". It contains no more than 64
         characters. Default is empty. Default value is None.
        :paramtype start: str
        :keyword top: The number of items to list, ranging in [1, 1000]. Default is 1000. Default value
         is None.
        :paramtype top: int
        :return: list of LivenessSessionAuditEntry
        :rtype: list[~azure.ai.vision.face.models.LivenessSessionAuditEntry]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[List[_models.LivenessSessionAuditEntry]] = kwargs.pop("cls", None)

        _request = build_face_session_get_liveness_session_audit_entries_request(
            session_id=session_id,
            start=start,
            top=top,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
            "apiVersion": self._serialize.url("self._config.api_version", self._config.api_version, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _deserialize(_models.FaceErrorResponse, response.json())
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(List[_models.LivenessSessionAuditEntry], response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def _create_liveness_with_verify_session(
        self, body: JSON, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.CreateLivenessWithVerifySessionResult: ...
    @overload
    async def _create_liveness_with_verify_session(
        self,
        *,
        liveness_operation_mode: Union[str, _models.LivenessOperationMode],
        content_type: str = "application/json",
        send_results_to_client: Optional[bool] = None,
        device_correlation_id_set_in_client: Optional[bool] = None,
        device_correlation_id: Optional[str] = None,
        auth_token_time_to_live_in_seconds: Optional[int] = None,
        **kwargs: Any
    ) -> _models.CreateLivenessWithVerifySessionResult: ...
    @overload
    async def _create_liveness_with_verify_session(
        self, body: IO[bytes], *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.CreateLivenessWithVerifySessionResult: ...

    @distributed_trace_async
    async def _create_liveness_with_verify_session(
        self,
        body: Union[JSON, IO[bytes]] = _Unset,
        *,
        liveness_operation_mode: Union[str, _models.LivenessOperationMode] = _Unset,
        send_results_to_client: Optional[bool] = None,
        device_correlation_id_set_in_client: Optional[bool] = None,
        device_correlation_id: Optional[str] = None,
        auth_token_time_to_live_in_seconds: Optional[int] = None,
        **kwargs: Any
    ) -> _models.CreateLivenessWithVerifySessionResult:
        """Create a new liveness session with verify. Client device submits VerifyImage during the
        /detectLivenessWithVerify/singleModal call.

        A session is best for client device scenarios where developers want to authorize a client
        device to perform only a liveness detection without granting full access to their resource.
        Created sessions have a limited life span and only authorize clients to perform the desired
        action before access is expired.

        Permissions includes...
        >
        *


        * Ability to call /detectLivenessWithVerify/singleModal for up to 3 retries.
        * A token lifetime of 10 minutes.

        ..

           [!NOTE]

           *


           * Client access can be revoked by deleting the session using the Delete Liveness With Verify
        Session operation.
           * To retrieve a result, use the Get Liveness With Verify Session.
           * To audit the individual requests that a client has made to your resource, use the List
        Liveness With Verify Session Audit Entries.


        Alternative Option: Client device submits VerifyImage during the
        /detectLivenessWithVerify/singleModal call.

        ..

           [!NOTE]
           Extra measures should be taken to validate that the client is sending the expected
        VerifyImage.

        :param body: Is either a JSON type or a IO[bytes] type. Required.
        :type body: JSON or IO[bytes]
        :keyword liveness_operation_mode: Type of liveness mode the client should follow. Known values
         are: "Passive" and "PassiveActive". Required.
        :paramtype liveness_operation_mode: str or ~azure.ai.vision.face.models.LivenessOperationMode
        :keyword send_results_to_client: Whether or not to allow a '200 - Success' response body to be
         sent to the client, which may be undesirable for security reasons. Default is false, clients
         will receive a '204 - NoContent' empty body response. Regardless of selection, calling Session
         GetResult will always contain a response body enabling business logic to be implemented.
         Default value is None.
        :paramtype send_results_to_client: bool
        :keyword device_correlation_id_set_in_client: Whether or not to allow client to set their own
         'deviceCorrelationId' via the Vision SDK. Default is false, and 'deviceCorrelationId' must be
         set in this request body. Default value is None.
        :paramtype device_correlation_id_set_in_client: bool
        :keyword device_correlation_id: Unique Guid per each end-user device. This is to provide rate
         limiting and anti-hammering. If 'deviceCorrelationIdSetInClient' is true in this request, this
         'deviceCorrelationId' must be null. Default value is None.
        :paramtype device_correlation_id: str
        :keyword auth_token_time_to_live_in_seconds: Seconds the session should last for. Range is 60
         to 86400 seconds. Default value is 600. Default value is None.
        :paramtype auth_token_time_to_live_in_seconds: int
        :return: CreateLivenessWithVerifySessionResult. The CreateLivenessWithVerifySessionResult is
         compatible with MutableMapping
        :rtype: ~azure.ai.vision.face.models.CreateLivenessWithVerifySessionResult
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.CreateLivenessWithVerifySessionResult] = kwargs.pop("cls", None)

        if body is _Unset:
            if liveness_operation_mode is _Unset:
                raise TypeError("missing required argument: liveness_operation_mode")
            body = {
                "authTokenTimeToLiveInSeconds": auth_token_time_to_live_in_seconds,
                "deviceCorrelationId": device_correlation_id,
                "deviceCorrelationIdSetInClient": device_correlation_id_set_in_client,
                "livenessOperationMode": liveness_operation_mode,
                "sendResultsToClient": send_results_to_client,
            }
            body = {k: v for k, v in body.items() if v is not None}
        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IOBase, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_face_session_create_liveness_with_verify_session_request(
            content_type=content_type,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
            "apiVersion": self._serialize.url("self._config.api_version", self._config.api_version, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _deserialize(_models.FaceErrorResponse, response.json())
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.CreateLivenessWithVerifySessionResult, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def _create_liveness_with_verify_session_with_verify_image(  # pylint: disable=name-too-long
        self, body: JSON, **kwargs: Any
    ) -> _models.CreateLivenessWithVerifySessionResult: ...
    @overload
    async def _create_liveness_with_verify_session_with_verify_image(  # pylint: disable=name-too-long
        self, *, parameters: _models._models.CreateLivenessSessionContent, verify_image: FileType, **kwargs: Any
    ) -> _models.CreateLivenessWithVerifySessionResult: ...

    @distributed_trace_async
    async def _create_liveness_with_verify_session_with_verify_image(  # pylint: disable=name-too-long
        self,
        body: JSON = _Unset,
        *,
        parameters: _models._models.CreateLivenessSessionContent = _Unset,
        verify_image: FileType = _Unset,
        **kwargs: Any
    ) -> _models.CreateLivenessWithVerifySessionResult:
        """Create a new liveness session with verify. Provide the verify image during session creation.

        A session is best for client device scenarios where developers want to authorize a client
        device to perform only a liveness detection without granting full access to their resource.
        Created sessions have a limited life span and only authorize clients to perform the desired
        action before access is expired.

        Permissions includes...
        >
        *


        * Ability to call /detectLivenessWithVerify/singleModal for up to 3 retries.
        * A token lifetime of 10 minutes.

        ..

           [!NOTE]

           *


           * Client access can be revoked by deleting the session using the Delete Liveness With Verify
        Session operation.
           * To retrieve a result, use the Get Liveness With Verify Session.
           * To audit the individual requests that a client has made to your resource, use the List
        Liveness With Verify Session Audit Entries.


        Recommended Option: VerifyImage is provided during session creation.

        :param body: Is one of the following types: JSON Required.
        :type body: JSON
        :keyword parameters: The parameters for creating session. Required.
        :paramtype parameters: ~azure.ai.vision.face.models._models.CreateLivenessSessionContent
        :keyword verify_image: The image stream for verify. Content-Disposition header field for this
         part must have filename. Required.
        :paramtype verify_image: ~azure.ai.vision.face._vendor.FileType
        :return: CreateLivenessWithVerifySessionResult. The CreateLivenessWithVerifySessionResult is
         compatible with MutableMapping
        :rtype: ~azure.ai.vision.face.models.CreateLivenessWithVerifySessionResult
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.CreateLivenessWithVerifySessionResult] = kwargs.pop("cls", None)

        if body is _Unset:
            if parameters is _Unset:
                raise TypeError("missing required argument: parameters")
            if verify_image is _Unset:
                raise TypeError("missing required argument: verify_image")
            body = {"Parameters": parameters, "VerifyImage": verify_image}
            body = {k: v for k, v in body.items() if v is not None}
        _body = body.as_dict() if isinstance(body, _model_base.Model) else body
        _file_fields: List[str] = ["VerifyImage"]
        _data_fields: List[str] = ["Parameters"]
        _files, _data = prepare_multipart_form_data(_body, _file_fields, _data_fields)

        _request = build_face_session_create_liveness_with_verify_session_with_verify_image_request(
            files=_files,
            data=_data,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
            "apiVersion": self._serialize.url("self._config.api_version", self._config.api_version, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _deserialize(_models.FaceErrorResponse, response.json())
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.CreateLivenessWithVerifySessionResult, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def delete_liveness_with_verify_session(  # pylint: disable=inconsistent-return-statements
        self, session_id: str, **kwargs: Any
    ) -> None:
        """Delete all session related information for matching the specified session id.

        ..

           [!NOTE]
           Deleting a session deactivates the Session Auth Token by blocking future API calls made with
        that Auth Token. While this can be used to remove any access for that token, those requests
        will still count towards overall resource rate limits. It's best to leverage TokenTTL to limit
        length of tokens in the case that it is misused.

        :param session_id: The unique ID to reference this session. Required.
        :type session_id: str
        :return: None
        :rtype: None
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[None] = kwargs.pop("cls", None)

        _request = build_face_session_delete_liveness_with_verify_session_request(
            session_id=session_id,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
            "apiVersion": self._serialize.url("self._config.api_version", self._config.api_version, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = False
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _deserialize(_models.FaceErrorResponse, response.json())
            raise HttpResponseError(response=response, model=error)

        if cls:
            return cls(pipeline_response, None, {})  # type: ignore

    @distributed_trace_async
    async def get_liveness_with_verify_session_result(
        self, session_id: str, **kwargs: Any
    ) -> _models.LivenessWithVerifySession:
        """Get session result of detectLivenessWithVerify/singleModal call.

        :param session_id: The unique ID to reference this session. Required.
        :type session_id: str
        :return: LivenessWithVerifySession. The LivenessWithVerifySession is compatible with
         MutableMapping
        :rtype: ~azure.ai.vision.face.models.LivenessWithVerifySession
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.LivenessWithVerifySession] = kwargs.pop("cls", None)

        _request = build_face_session_get_liveness_with_verify_session_result_request(
            session_id=session_id,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
            "apiVersion": self._serialize.url("self._config.api_version", self._config.api_version, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _deserialize(_models.FaceErrorResponse, response.json())
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.LivenessWithVerifySession, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def get_liveness_with_verify_sessions(
        self, *, start: Optional[str] = None, top: Optional[int] = None, **kwargs: Any
    ) -> List[_models.LivenessSessionItem]:
        """Lists sessions for /detectLivenessWithVerify/SingleModal.

        List sessions from the last sessionId greater than the "start".

        The result should be ordered by sessionId in ascending order.

        :keyword start: List resources greater than the "start". It contains no more than 64
         characters. Default is empty. Default value is None.
        :paramtype start: str
        :keyword top: The number of items to list, ranging in [1, 1000]. Default is 1000. Default value
         is None.
        :paramtype top: int
        :return: list of LivenessSessionItem
        :rtype: list[~azure.ai.vision.face.models.LivenessSessionItem]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[List[_models.LivenessSessionItem]] = kwargs.pop("cls", None)

        _request = build_face_session_get_liveness_with_verify_sessions_request(
            start=start,
            top=top,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
            "apiVersion": self._serialize.url("self._config.api_version", self._config.api_version, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _deserialize(_models.FaceErrorResponse, response.json())
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(List[_models.LivenessSessionItem], response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def get_liveness_with_verify_session_audit_entries(  # pylint: disable=name-too-long
        self, session_id: str, *, start: Optional[str] = None, top: Optional[int] = None, **kwargs: Any
    ) -> List[_models.LivenessSessionAuditEntry]:
        """Gets session requests and response body for the session.

        :param session_id: The unique ID to reference this session. Required.
        :type session_id: str
        :keyword start: List resources greater than the "start". It contains no more than 64
         characters. Default is empty. Default value is None.
        :paramtype start: str
        :keyword top: The number of items to list, ranging in [1, 1000]. Default is 1000. Default value
         is None.
        :paramtype top: int
        :return: list of LivenessSessionAuditEntry
        :rtype: list[~azure.ai.vision.face.models.LivenessSessionAuditEntry]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping[int, Type[HttpResponseError]] = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[List[_models.LivenessSessionAuditEntry]] = kwargs.pop("cls", None)

        _request = build_face_session_get_liveness_with_verify_session_audit_entries_request(
            session_id=session_id,
            start=start,
            top=top,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
            "apiVersion": self._serialize.url("self._config.api_version", self._config.api_version, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _deserialize(_models.FaceErrorResponse, response.json())
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(List[_models.LivenessSessionAuditEntry], response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore
