# pylint: disable=line-too-long,useless-suppression
# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for license information.
# Code generated by Microsoft (R) Python Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------
import pytest
import os
import re
from typing import Tuple, Union, Dict, Any, Optional, List, Set
from devtools_testutils.aio import recorded_by_proxy_async
from testpreparer_async import ContentUnderstandingClientTestBaseAsync, ContentUnderstandingPreparer
from azure.ai.contentunderstanding.models import ContentAnalyzer
from azure.ai.contentunderstanding.aio import ContentUnderstandingClient
from azure.ai.contentunderstanding.models import AnalyzeInput
from test_helpers import (
    generate_analyzer_id,
    new_simple_content_analyzer_object,
    new_marketing_video_analyzer_object,
    assert_poller_properties,
    assert_simple_content_analyzer_result,
    save_analysis_result_to_file,
    save_keyframe_image_to_file,
)
from devtools_testutils import is_live, is_live_and_not_recording


async def create_analyzer_and_assert_async(
    client: ContentUnderstandingClient, analyzer_id: str, resource: Union[ContentAnalyzer, Dict[str, Any]]
) -> Any:
    """Create an analyzer and perform basic assertions (async version).

    Args:
        client: The ContentUnderstandingClient instance
        analyzer_id: The analyzer ID to create
        resource: The analyzer resource (ContentAnalyzer object or dict)

    Returns:
        Any: The poller object

    Raises:
        AssertionError: If the creation fails or assertions fail
    """
    print(f"\nCreating analyzer {analyzer_id}")

    # Start the analyzer creation operation
    poller = await client.begin_create_or_replace(
        analyzer_id=analyzer_id,
        resource=resource,
    )

    # Wait for the operation to complete
    print(f"  Waiting for analyzer {analyzer_id} to be created")
    response = await poller.result()
    assert response is not None
    assert poller.status() == "Succeeded"
    assert poller.done()
    print(f"  Analyzer {analyzer_id} is created successfully")

    # Additional poller assertions
    assert poller is not None
    assert poller.status() is not None
    assert poller.continuation_token() is not None

    return poller


async def delete_analyzer_and_assert(
    client: ContentUnderstandingClient, analyzer_id: str, created_analyzer: bool
) -> None:
    """Delete an analyzer and assert it was deleted successfully.

    Args:
        client: The ContentUnderstandingClient instance
        analyzer_id: The analyzer ID to delete
        created_analyzer: Whether the analyzer was created (to determine if cleanup is needed)

    Raises:
        AssertionError: If the analyzer still exists after deletion
    """
    if created_analyzer:
        print(f"Cleaning up analyzer {analyzer_id}")
        try:
            await client.delete(analyzer_id=analyzer_id)
        except Exception as e:
            # If deletion fails, the test should fail
            raise AssertionError(f"Failed to delete analyzer {analyzer_id}: {e}") from e
    else:
        print(f"Analyzer {analyzer_id} was not created, no cleanup needed")


async def download_keyframes_and_assert_async(
    client: ContentUnderstandingClient,
    analysis_operation_id: str,
    result: Any,
    test_py_file_dir: str,
    identifier: Optional[str] = None,
) -> None:
    """Download keyframes from video analysis result and assert their existence (async version).

    Downloads up to 3 keyframes: first, middle, and last frame to avoid duplicates.

    Args:
        client: The ContentUnderstandingClient instance
        analysis_operation_id: The operation ID from the analysis
        result: The analysis result containing markdown with keyframes
        test_py_file_dir: The directory where pytest files are located
        identifier: Optional unique identifier to avoid conflicts (e.g., analyzer_id)

    Returns:
        None

    Raises:
        AssertionError: If no keyframes are found in the analysis result
    """
    keyframe_ids: Set[str] = set()

    # Iterate over contents to find keyframes from markdown
    for content in result.contents:
        # Extract keyframe IDs from "markdown" if it exists and is a string
        markdown_content = getattr(content, "markdown", "")
        if isinstance(markdown_content, str):
            # Use the same regex pattern as the official sample: (keyFrame\.d+)\.jpg
            keyframe_ids.update(re.findall(r"(keyFrame\.\d+)\.jpg", markdown_content))

    print(f"Found keyframe IDs in markdown: {keyframe_ids}")

    # Assert that keyframe IDs were found in the video analysis
    assert (
        keyframe_ids
    ), "No keyframe IDs were found in the video analysis markdown content. Video analysis should generate keyframes that can be extracted using regex pattern."

    print(f"Successfully extracted {len(keyframe_ids)} keyframe IDs from video analysis")

    # Sort keyframes by frame number to get first, middle, and last
    # Extract numeric part from "keyFrame.22367" format and convert to "keyframes/22367" format
    def extract_frame_number(keyframe_id: str) -> int:
        # Extract number after "keyFrame."
        match = re.search(r"keyFrame\.(\d+)", keyframe_id)
        if match:
            return int(match.group(1))
        return 0

    # Build keyframe paths in the format expected by get_result_file API: "keyframes/{time_ms}"
    keyframe_paths = [f"keyframes/{extract_frame_number(kf)}" for kf in keyframe_ids]

    # Sort by frame number
    sorted_keyframes: List[str] = sorted(keyframe_paths, key=lambda x: int(x.split("/")[-1]))

    # Create a set with first, middle, and last frames (automatically removes duplicates)
    frames_set: Set[str] = {sorted_keyframes[0], sorted_keyframes[-1], sorted_keyframes[len(sorted_keyframes) // 2]}

    # Convert set to list for processing
    frames_to_download: List[str] = list(frames_set)

    print(f"Selected frames to download: {frames_to_download}")

    # Try to retrieve the selected keyframe images using get_result_file API
    files_retrieved: int = 0

    for keyframe_id in frames_to_download:
        print(f"Trying to get result file with path: {keyframe_id}")
        response = await client.get_result_file(
            operation_id=analysis_operation_id,
            path=keyframe_id,  # Use keyframe_id directly as path, no .jpg extension
        )

        # Handle the response - it's an async iterator that needs to be collected
        from collections.abc import AsyncIterator

        assert isinstance(response, AsyncIterator), f"Expected AsyncIterator, got {type(response)}"

        # It's an async iterator, collect all bytes efficiently
        chunks = []
        async for chunk in response:
            chunks.append(chunk)
        result_bytes = b"".join(chunks)

        # Assert that we successfully get a response and it's valid image data
        assert result_bytes is not None, f"Response for path {keyframe_id} should not be None"
        assert isinstance(
            result_bytes, bytes
        ), f"Response for {keyframe_id} should be bytes (image data), got {type(result_bytes)}"
        assert len(result_bytes) > 0, f"Image file content for {keyframe_id} should not be empty"

        # Save the image file using the helper function
        saved_file_path = save_keyframe_image_to_file(
            image_content=result_bytes,
            keyframe_id=keyframe_id,
            test_name="test_content_analyzers_get_result_file",
            test_py_file_dir=test_py_file_dir,
            identifier=identifier,
        )

        # Verify the saved file exists and has content
        assert os.path.exists(saved_file_path), f"Saved image file should exist at {saved_file_path}"
        assert os.path.getsize(saved_file_path) > 0, f"Saved image file should not be empty"

        files_retrieved += 1
        print(f"Successfully downloaded keyframe image: {keyframe_id}")

    # Assert that we successfully downloaded all expected files
    assert files_retrieved == len(
        frames_to_download
    ), f"Expected to download {len(frames_to_download)} files, but only downloaded {files_retrieved}"
    print(f"Successfully completed get_result_file test - downloaded {files_retrieved} keyframe images")


import pytest


class TestContentUnderstandingContentAnalyzersOperationsAsync(ContentUnderstandingClientTestBaseAsync):

    @ContentUnderstandingPreparer()
    @recorded_by_proxy_async
    async def test_content_analyzers_begin_create_with_content_analyzer(
        self, contentunderstanding_endpoint: str
    ) -> None:
        """
        Test Summary:
        - Create analyzer using ContentAnalyzer object
        - Verify analyzer creation and poller properties
        - Clean up created analyzer
        """
        client: ContentUnderstandingClient = self.create_async_client(endpoint=contentunderstanding_endpoint)
        analyzer_id = generate_analyzer_id(client, "create_content_analyzer", is_async=True)
        created_analyzer = False

        content_analyzer = new_simple_content_analyzer_object(
            analyzer_id=analyzer_id, description=f"test analyzer: {analyzer_id}", tags={"tag1_name": "tag1_value"}
        )

        try:
            # Create analyzer using the refactored function
            poller = await create_analyzer_and_assert_async(client, analyzer_id, content_analyzer)
            created_analyzer = True

        finally:
            # Always clean up the created analyzer, even if the test fails
            await delete_analyzer_and_assert(client, analyzer_id, created_analyzer)

    @ContentUnderstandingPreparer()
    @recorded_by_proxy_async
    async def test_content_analyzers_begin_create_with_json(self, contentunderstanding_endpoint: str) -> None:
        """
        Test Summary:
        - Create analyzer using JSON dictionary
        - Verify analyzer creation and poller properties
        - Clean up created analyzer
        """
        client: ContentUnderstandingClient = self.create_async_client(endpoint=contentunderstanding_endpoint)
        analyzer_id = generate_analyzer_id(client, "create_json", is_async=True)
        created_analyzer = False

        try:
            # Create analyzer using the refactored function with JSON resource
            poller = await create_analyzer_and_assert_async(
                client,
                analyzer_id,
                {
                    "analyzerId": analyzer_id,
                    "baseAnalyzerId": "prebuilt-document",
                    "config": {
                        "disableContentFiltering": False,
                        "disableFaceBlurring": False,
                        "enableFace": False,
                        "enableFormula": True,
                        "enableLayout": True,
                        "enableOcr": True,
                        "estimateFieldSourceAndConfidence": True,
                        "returnDetails": True,
                    },
                    "description": f"test analyzer: {analyzer_id}",
                    "fieldSchema": {
                        "fields": {
                            "total_amount": {
                                "description": "Total amount of this table",
                                "method": "extract",
                                "type": "number",
                            }
                        },
                        "description": "schema description here",
                        "name": "schema name here",
                    },
                    "mode": "standard",
                    "processingLocation": "global",
                    "models": {"completion": "gpt-4o"},  # Required when using fieldSchema
                    "tags": {"tag1_name": "tag1_value"},
                },
            )
            created_analyzer = True

        finally:
            # Always clean up the created analyzer, even if the test fails
            await delete_analyzer_and_assert(client, analyzer_id, created_analyzer)

    @ContentUnderstandingPreparer()
    @recorded_by_proxy_async
    async def test_content_analyzers_update(self, contentunderstanding_endpoint: str) -> None:
        """
        Test Summary:
        - Create initial analyzer
        - Get analyzer before update to verify initial state
        - Update analyzer with new description and tags
        - Get analyzer after update to verify changes persisted
        - Clean up created analyzer
        """
        client: ContentUnderstandingClient = self.create_async_client(endpoint=contentunderstanding_endpoint)
        analyzer_id = generate_analyzer_id(client, "update", is_async=True)
        created_analyzer = False

        # Create initial analyzer
        initial_analyzer = new_simple_content_analyzer_object(
            analyzer_id=analyzer_id,
            description=f"Initial analyzer for update test: {analyzer_id}",
            tags={"initial_tag": "initial_value"},
        )

        try:
            # Create the initial analyzer using the refactored function
            poller = await create_analyzer_and_assert_async(client, analyzer_id, initial_analyzer)
            created_analyzer = True

            # Get the analyzer before update to verify initial state
            print(f"Getting analyzer {analyzer_id} before update")
            analyzer_before_update = await client.get(analyzer_id=analyzer_id)
            assert analyzer_before_update is not None
            assert analyzer_before_update.analyzer_id == analyzer_id
            assert analyzer_before_update.description == f"Initial analyzer for update test: {analyzer_id}"
            assert analyzer_before_update.tags == {"initial_tag": "initial_value"}
            print(
                f"Initial analyzer state verified - description: {analyzer_before_update.description}, tags: {analyzer_before_update.tags}"
            )

            # Create updated analyzer with only allowed properties (description and tags)
            # Note: Service requires baseAnalyzerId and models even in PATCH update
            # This is a service bug - TypeSpec says they should not be required in Update
            updated_analyzer = ContentAnalyzer(
                base_analyzer_id=analyzer_before_update.base_analyzer_id,  # <== SERVICE-FIX: Service will return error without this
                models=analyzer_before_update.models,  # <== SERVICE-FIX: Service will return error without this
                description=f"Updated analyzer for update test: {analyzer_id}",
                tags={"initial_tag": "initial_value", "tag1_field": "updated_value"},
            )

            print(f"Updating analyzer {analyzer_id} with new tag and description")

            # Update the analyzer
            response = await client.update(
                analyzer_id=analyzer_id,
                resource=updated_analyzer,
            )

            # Verify the update response
            assert response is not None
            print(f"Update response: {response}")

            # Verify the updated analyzer has the new tag and updated description
            assert response.analyzer_id == analyzer_id
            assert response.tags is not None
            assert "tag1_field" in response.tags
            assert response.tags["tag1_field"] == "updated_value"
            assert response.description == f"Updated analyzer for update test: {analyzer_id}"

            print(f"Successfully updated analyzer {analyzer_id} with new tag and description")

            # Get the analyzer after update to verify the changes persisted
            print(f"Getting analyzer {analyzer_id} after update")
            analyzer_after_update = await client.get(analyzer_id=analyzer_id)
            assert analyzer_after_update is not None
            assert analyzer_after_update.analyzer_id == analyzer_id
            assert analyzer_after_update.description == f"Updated analyzer for update test: {analyzer_id}"
            assert analyzer_after_update.tags == {"initial_tag": "initial_value", "tag1_field": "updated_value"}
            print(
                f"Updated analyzer state verified - description: {analyzer_after_update.description}, tags: {analyzer_after_update.tags}"
            )

        finally:
            # Always clean up the created analyzer, even if the test fails
            await delete_analyzer_and_assert(client, analyzer_id, created_analyzer)

    @ContentUnderstandingPreparer()
    @recorded_by_proxy_async
    async def test_content_analyzers_get(self, contentunderstanding_endpoint: str) -> None:
        """
        Test Summary:
        - Get existing prebuilt analyzer
        - Verify analyzer properties and status
        """
        client: ContentUnderstandingClient = self.create_async_client(endpoint=contentunderstanding_endpoint)
        response = await client.get(
            analyzer_id="prebuilt-documentSearch",
        )
        assert response is not None
        print(response)
        assert response.analyzer_id == "prebuilt-documentSearch"
        assert response.description is not None
        assert len(response.description) > 0
        assert response.status == "ready"
        assert response.created_at is not None
        assert response.config is not None

    @ContentUnderstandingPreparer()
    @recorded_by_proxy_async
    async def test_content_analyzers_delete(self, contentunderstanding_endpoint: str) -> None:
        """
        Test Summary:
        - Create analyzer for deletion test
        - Delete analyzer
        - Clean up if deletion failed
        """
        client: ContentUnderstandingClient = self.create_async_client(endpoint=contentunderstanding_endpoint)
        analyzer_id = generate_analyzer_id(client, "delete", is_async=True)
        created_analyzer = False

        # Create a simple analyzer for deletion test
        content_analyzer = new_simple_content_analyzer_object(
            analyzer_id=analyzer_id,
            description=f"test analyzer for deletion: {analyzer_id}",
            tags={"test_type": "deletion"},
        )

        try:
            # Create analyzer using the refactored function
            poller = await create_analyzer_and_assert_async(client, analyzer_id, content_analyzer)
            created_analyzer = True

            # Delete the analyzer
            print(f"Deleting analyzer {analyzer_id}")
            response = await client.delete(analyzer_id=analyzer_id)

            # Verify the delete response
            assert response is None
            #     client, analyzer_id
            # ), f"Deleted analyzer with ID '{analyzer_id}' was found in the list"
        finally:
            # Clean up if the analyzer was created but deletion failed
            if created_analyzer:
                print(f"Cleaning up analyzer {analyzer_id} that was not properly deleted")
                try:
                    await client.delete(analyzer_id=analyzer_id)
                    # Verify deletion (NOTE: check disabled - list too long to execute)
                    #     client, analyzer_id
                    # ), f"Failed to delete analyzer {analyzer_id} during cleanup"
                    print(f"Analyzer {analyzer_id} is deleted successfully during cleanup")
                except Exception as e:
                    # If cleanup fails, the test should fail
                    raise AssertionError(f"Failed to delete analyzer {analyzer_id} during cleanup: {e}") from e
            elif not created_analyzer:
                print(f"Analyzer {analyzer_id} was not created, no cleanup needed")

    @pytest.mark.skip(reason="TEMPORARILY SKIPPED: List operation is too long - too many analyzers")
    @ContentUnderstandingPreparer()
    @recorded_by_proxy_async
    async def test_content_analyzers_list(self, contentunderstanding_endpoint: str) -> None:
        """
        Test Summary:
        - List all available analyzers
        - Verify list response contains expected prebuilt analyzers
        - Verify each analyzer has required properties
        """
        client: ContentUnderstandingClient = self.create_async_client(endpoint=contentunderstanding_endpoint)
        response = client.list()
        result = [r async for r in response]
        assert len(result) > 0, "Should have at least one analyzer in the list"
        print(f"Found {len(result)} analyzers")
        prebuilt_found = False
        for analyzer in result:
            assert hasattr(analyzer, "analyzer_id"), "Each analyzer should have analyzer_id"
            assert hasattr(analyzer, "description"), "Each analyzer should have description"
            assert hasattr(analyzer, "status"), "Each analyzer should have status"
            assert hasattr(analyzer, "created_at"), "Each analyzer should have created_at"

            if analyzer.analyzer_id == "prebuilt-documentSearch":
                prebuilt_found = True
                assert analyzer.status == "ready", "prebuilt-documentSearch should be ready"
                print(f"Found prebuilt-documentSearch: {analyzer.description}")

        assert prebuilt_found, "prebuilt-documentSearch should be in the list"
        print("List analyzers test completed successfully")

    @ContentUnderstandingPreparer()
    @recorded_by_proxy_async
    async def test_content_analyzers_begin_analyze_url(self, contentunderstanding_endpoint: str) -> None:
        """
        Test Summary:
        - Create simple analyzer for URL analysis
        - Begin analysis operation with URL input
        - Wait for analysis completion
        - Save analysis result to output file
        - Verify fields node exists in first result
        - Verify total_amount field exists and equals 110
        - Clean up created analyzer
        """
        client: ContentUnderstandingClient = self.create_async_client(endpoint=contentunderstanding_endpoint)
        analyzer_id = generate_analyzer_id(client, "analyze_url", is_async=True)
        created_analyzer = False

        # Create a simple analyzer for URL analysis
        content_analyzer = new_simple_content_analyzer_object(
            analyzer_id=analyzer_id,
            description=f"test analyzer for URL analysis: {analyzer_id}",
            tags={"test_type": "url_analysis"},
        )

        try:
            # Create analyzer using the refactored function
            poller = await create_analyzer_and_assert_async(client, analyzer_id, content_analyzer)
            created_analyzer = True

            # Use the provided URL for the invoice PDF
            invoice_url = "https://github.com/Azure-Samples/azure-ai-content-understanding-python/raw/refs/heads/main/data/invoice.pdf"

            print(f"Starting URL analysis with analyzer {analyzer_id}")

            # Begin analysis operation with URL
            analysis_poller = await client.begin_analyze(
                analyzer_id=analyzer_id, inputs=[AnalyzeInput(url=invoice_url)]
            )
            assert_poller_properties(analysis_poller, "Analysis poller")

            # Wait for analysis completion
            print(f"Waiting for analysis completion")
            analysis_result = await analysis_poller.result()
            print(f"  Analysis completed")

            # Get test file directory for saving output
            test_file_dir = os.path.dirname(os.path.abspath(__file__))
            output_filename = save_analysis_result_to_file(
                analysis_result, "test_content_analyzers_begin_analyze_url", test_file_dir, analyzer_id
            )

            # Now assert the field results
            assert_simple_content_analyzer_result(analysis_result, "Analysis result")

        finally:
            # Always clean up the created analyzer, even if the test fails
            await delete_analyzer_and_assert(client, analyzer_id, created_analyzer)

    @ContentUnderstandingPreparer()
    @recorded_by_proxy_async
    async def test_content_analyzers_begin_analyze_binary(self, contentunderstanding_endpoint: str) -> None:
        """
        Test Summary:
        - Create simple analyzer for binary analysis
        - Read sample invoice PDF file
        - Begin binary analysis operation with analyzer
        - Wait for analysis completion
        - Save analysis result to output file
        - Verify fields node exists in first result
        - Verify total_amount field exists and equals 110
        - Clean up created analyzer
        """
        client: ContentUnderstandingClient = self.create_async_client(endpoint=contentunderstanding_endpoint)
        analyzer_id = generate_analyzer_id(client, "analyze_binary", is_async=True)
        created_analyzer = False

        # Create a simple analyzer for binary analysis
        content_analyzer = new_simple_content_analyzer_object(
            analyzer_id=analyzer_id,
            description=f"test analyzer for binary analysis: {analyzer_id}",
            tags={"test_type": "binary_analysis"},
        )

        try:
            # Create analyzer using the refactored function
            poller = await create_analyzer_and_assert_async(client, analyzer_id, content_analyzer)
            created_analyzer = True

            # Read the sample invoice PDF file using absolute path based on this test file's location
            test_file_dir = os.path.dirname(os.path.abspath(__file__))
            pdf_path = os.path.join(test_file_dir, "test_data", "sample_invoice.pdf")
            with open(pdf_path, "rb") as pdf_file:
                pdf_content = pdf_file.read()

            print(f"Starting binary analysis with analyzer {analyzer_id}")

            # Begin binary analysis operation
            analysis_poller = await client.begin_analyze_binary(analyzer_id=analyzer_id, binary_input=pdf_content)
            assert_poller_properties(analysis_poller, "Analysis poller")

            # Wait for analysis completion
            print(f"Waiting for analysis completion")
            analysis_result = await analysis_poller.result()
            print(f"  Analysis completed")

            output_filename = save_analysis_result_to_file(
                analysis_result, "test_content_analyzers_begin_analyze_binary", test_file_dir, analyzer_id
            )

            # Now assert the field results
            assert_simple_content_analyzer_result(analysis_result, "Analysis result")

        finally:
            # Always clean up the created analyzer, even if the test fails
            await delete_analyzer_and_assert(client, analyzer_id, created_analyzer)

    @ContentUnderstandingPreparer()
    @recorded_by_proxy_async
    async def test_content_analyzers_get_result_file(self, contentunderstanding_endpoint: str) -> None:
        """
        Test Summary:
        - Create marketing video analyzer based on the marketing video template
        - Read FlightSimulator.mp4 file
        - Begin video analysis operation with analyzer
        - Wait for analysis completion
        - Use get_result_file to retrieve image files generated from video analysis
        - Verify image file content is returned and save to test_output
        - Clean up created analyzer
        """
        if not is_live_and_not_recording():
            pytest.skip(
                "This test requires live mode to run, as it involves large video files that are too big for test proxy to record"
            )
            return  # Skip this test in playback mode as it requires large video files is too big for test proxy to record
        client: ContentUnderstandingClient = self.create_async_client(endpoint=contentunderstanding_endpoint)
        analyzer_id = generate_analyzer_id(client, "get_result_file", is_async=True)
        created_analyzer = False

        # Create a marketing video analyzer based on the template
        video_analyzer = new_marketing_video_analyzer_object(
            analyzer_id=analyzer_id,
            description=f"marketing video analyzer for get result file test: {analyzer_id}",
            tags={"test_type": "get_result_file_video"},
        )

        try:
            # Create analyzer using the refactored function
            poller = await create_analyzer_and_assert_async(client, analyzer_id, video_analyzer)
            created_analyzer = True

            # Use the FlightSimulator.mp4 video file from remote location
            video_file_url = "https://github.com/Azure-Samples/azure-ai-content-understanding-assets/raw/refs/heads/main/videos/sdk_samples/FlightSimulator.mp4"
            print(f"Using video file from URL: {video_file_url}")

            # Get test file directory for saving output
            test_file_dir = os.path.dirname(os.path.abspath(__file__))

            print(f"Starting video analysis to get operation ID")

            # Begin video analysis operation using URL
            analysis_poller = await client.begin_analyze(
                analyzer_id=analyzer_id, inputs=[AnalyzeInput(url=video_file_url)]
            )

            # Wait for analysis completion first
            print(f"Waiting for analysis completion")
            analysis_result = await analysis_poller.result()
            print(f"Analysis completed")

            # Save the analysis result to file
            output_filename = save_analysis_result_to_file(
                analysis_result, "test_content_analyzers_get_result_file", test_file_dir, analyzer_id
            )

            # Extract operation ID for get_result_file test using custom poller's details property
            from azure.ai.contentunderstanding.aio.operations._patch import AnalyzeAsyncLROPoller

            assert isinstance(analysis_poller, AnalyzeAsyncLROPoller), "Should return custom AnalyzeAsyncLROPoller"

            details = analysis_poller.details
            assert "operation_id" in details, "Details should contain operation_id"
            analysis_operation_id = details["operation_id"]
            assert analysis_operation_id is not None, "Operation ID should not be None"
            assert len(analysis_operation_id) > 0, "Operation ID should not be empty"
            print(f"Analysis operation ID: {analysis_operation_id}")

            # Use the analysis result we already have from the poller to see what files are available
            result = analysis_result
            assert result is not None, "Analysis result should not be None"
            print(f"Analysis result contains {len(result.contents)} contents")

            # Use the refactored function to download keyframes by calling client.get_result_file
            await download_keyframes_and_assert_async(client, analysis_operation_id, result, test_file_dir, analyzer_id)

        finally:
            # Always clean up the created analyzer, even if the test fails
            await delete_analyzer_and_assert(client, analyzer_id, created_analyzer)

    # @ContentUnderstandingPreparer()
    # @recorded_by_proxy_async
    # @pytest.mark.skip(reason="GA API addition - to be implemented")


#     @ContentUnderstandingPreparer()
#     @recorded_by_proxy_async
#     @pytest.mark.skip(reason="GA API addition - to be implemented")
#     async def test_content_analyzers_begin_analyze(self, contentunderstanding_endpoint):
#         client = self.create_async_client(endpoint=contentunderstanding_endpoint)
#         response = await (
#             await client.begin_analyze(
#                 analyzer_id="str",
#                 body={
#                     "inputs": [
#                         {
#                             "data": bytes("bytes", encoding="utf-8"),
#                             "mimeType": "str",
#                             "name": "str",
#                             "range": "str",
#                             "url": "str",
#                         }
#                     ],
#                     "modelDeployments": {"str": "str"},
#                 },
#             )
#         ).result()  # call '.result()' to poll until service return final result

# please add some check logic here by yourself
# ...


# @ContentUnderstandingPreparer()
# @recorded_by_proxy_async
# @pytest.mark.skip(reason="GA API addition - to be implemented")


#     @ContentUnderstandingPreparer()
#     @recorded_by_proxy_async
#     @pytest.mark.skip(reason="GA API addition - to be implemented")
#     async def test_content_analyzers_begin_copy(self, contentunderstanding_endpoint):
#         client = self.create_async_client(endpoint=contentunderstanding_endpoint)
#         response = await (
#             await client.begin_copy(
#                 analyzer_id="str",
#                 body={"sourceAnalyzerId": "str", "sourceAzureResourceId": "str", "sourceRegion": "str"},
#                 source_analyzer_id="str",
#             )
#         ).result()  # call '.result()' to poll until service return final result

# please add some check logic here by yourself
# ...


# @ContentUnderstandingPreparer()
# @recorded_by_proxy_async
# @pytest.mark.skip(reason="GA API addition - to be implemented")


#     @ContentUnderstandingPreparer()
#     @recorded_by_proxy_async
#     @pytest.mark.skip(reason="GA API addition - to be implemented")
#     async def test_content_analyzers_begin_create_or_replace(self, contentunderstanding_endpoint):
#         client = self.create_async_client(endpoint=contentunderstanding_endpoint)
#         response = await (
#             await client.begin_create_or_replace(
#                 analyzer_id="str",
#                 resource={
#                     "analyzerId": "str",
#                     "createdAt": "2020-02-20 00:00:00",
#                     "lastModifiedAt": "2020-02-20 00:00:00",
#                     "status": "str",
#                     "baseAnalyzerId": "str",
#                     "config": {
#                         "annotationFormat": "str",
#                         "chartFormat": "str",
#                         "contentCategories": {"str": {"analyzer": ..., "analyzerId": "str", "description": "str"}},
#                         "disableFaceBlurring": bool,
#                         "enableAnnotation": bool,
#                         "enableFigureAnalysis": bool,
#                         "enableFigureDescription": bool,
#                         "enableFormula": bool,
#                         "enableLayout": bool,
#                         "enableOcr": bool,
#                         "enableSegment": bool,
#                         "estimateFieldSourceAndConfidence": bool,
#                         "locales": ["str"],
#                         "omitContent": bool,
#                         "returnDetails": bool,
#                         "segmentPerPage": bool,
#                         "tableFormat": "str",
#                     },
#                     "description": "str",
#                     "dynamicFieldSchema": bool,
#                     "fieldSchema": {
#                         "fields": {
#                             "str": {
#                                 "$ref": "str",
#                                 "description": "str",
#                                 "enum": ["str"],
#                                 "enumDescriptions": {"str": "str"},
#                                 "estimateSourceAndConfidence": bool,
#                                 "examples": ["str"],
#                                 "items": ...,
#                                 "method": "str",
#                                 "properties": {"str": ...},
#                                 "type": "str",
#                             }
#                         },
#                         "definitions": {
#                             "str": {
#                                 "$ref": "str",
#                                 "description": "str",
#                                 "enum": ["str"],
#                                 "enumDescriptions": {"str": "str"},
#                                 "estimateSourceAndConfidence": bool,
#                                 "examples": ["str"],
#                                 "items": ...,
#                                 "method": "str",
#                                 "properties": {"str": ...},
#                                 "type": "str",
#                             }
#                         },
#                         "description": "str",
#                         "name": "str",
#                     },
#                     "knowledgeSources": ["knowledge_source"],
#                     "models": {"str": "str"},
#                     "processingLocation": "str",
#                     "supportedModels": {"completion": {"str": "str"}, "embedding": {"str": "str"}},
#                     "tags": {"str": "str"},
#                     "warnings": [...],
#                 },
#             )
#         ).result()  # call '.result()' to poll until service return final result

# please add some check logic here by yourself
# ...


# @ContentUnderstandingPreparer()
# @recorded_by_proxy_async
# @pytest.mark.skip(reason="GA API addition - to be implemented")


#     @ContentUnderstandingPreparer()
#     @recorded_by_proxy_async
#     @pytest.mark.skip(reason="GA API addition - to be implemented")
#     async def test_content_analyzers_delete_result(self, contentunderstanding_endpoint):
#         client = self.create_async_client(endpoint=contentunderstanding_endpoint)
#         response = await client.delete_result(
#             operation_id="str",
#         )

# please add some check logic here by yourself
# ...


# @ContentUnderstandingPreparer()
# @recorded_by_proxy_async
# @pytest.mark.skip(reason="GA API addition - to be implemented")


#     @ContentUnderstandingPreparer()
#     @recorded_by_proxy_async
#     @pytest.mark.skip(reason="GA API addition - to be implemented")
#     async def test_content_analyzers_get_defaults(self, contentunderstanding_endpoint):
#         client = self.create_async_client(endpoint=contentunderstanding_endpoint)
#         response = await client.get_defaults()

# please add some check logic here by yourself
# ...


# @ContentUnderstandingPreparer()
# @recorded_by_proxy_async
# @pytest.mark.skip(reason="GA API addition - to be implemented")


#     @ContentUnderstandingPreparer()
#     @recorded_by_proxy_async
#     @pytest.mark.skip(reason="GA API addition - to be implemented")
#     async def test_content_analyzers_get_operation_status(self, contentunderstanding_endpoint):
#         client = self.create_async_client(endpoint=contentunderstanding_endpoint)
#         response = await client.get_operation_status(
#             analyzer_id="str",
#             operation_id="str",
#         )

# please add some check logic here by yourself
# ...


# @ContentUnderstandingPreparer()
# @recorded_by_proxy_async
# @pytest.mark.skip(reason="GA API addition - to be implemented")


#     @ContentUnderstandingPreparer()
#     @recorded_by_proxy_async
#     @pytest.mark.skip(reason="GA API addition - to be implemented")
#     async def test_content_analyzers_grant_copy_authorization(self, contentunderstanding_endpoint):
#         client = self.create_async_client(endpoint=contentunderstanding_endpoint)
#         response = await client.grant_copy_authorization(
#             analyzer_id="str",
#             body={"targetAzureResourceId": "str", "targetRegion": "str"},
#             target_azure_resource_id="str",
#         )

# please add some check logic here by yourself
# ...


# @ContentUnderstandingPreparer()
# @recorded_by_proxy_async
# @pytest.mark.skip(reason="GA API addition - to be implemented")


#     @ContentUnderstandingPreparer()
#     @recorded_by_proxy_async
#     @pytest.mark.skip(reason="GA API addition - to be implemented")
#     async def test_content_analyzers_update_defaults(self, contentunderstanding_endpoint):
#         client = self.create_async_client(endpoint=contentunderstanding_endpoint)
#         response = await client.update_defaults(
#             body={"modelDeployments": {}},
#         )
#         please add some check logic here by yourself
#
