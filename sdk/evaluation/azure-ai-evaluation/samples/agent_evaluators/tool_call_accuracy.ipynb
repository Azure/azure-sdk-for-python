{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool Call Accuracy Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "This sample demonstrates how to use Intent Resolution Evaluator\n",
    "Before running the sample:\n",
    "```bash\n",
    "pip install azure-ai-projects azure-identity azure-ai-evaluation\n",
    "```\n",
    "Set these environment variables with your own values:\n",
    "1) **PROJECT_CONNECTION_STRING** - The project connection string, as found in the overview page of your Azure AI Foundry project.\n",
    "2) **MODEL_DEPLOYMENT_NAME** - The deployment name of the AI model, as found under the \"Name\" column in the \"Models + endpoints\" tab in your Azure AI Foundry project.\n",
    "3) **AZURE_OPENAI_ENDPOINT** - Azure Open AI Endpoint to be used for evaluation.\n",
    "4) **AZURE_OPENAI_API_KEY** - Azure Open AI Key to be used for evaluation.\n",
    "5) **AZURE_OPENAI_API_VERSION** - Azure Open AI Api version to be used for evaluation.\n",
    "6) **AZURE_SUBSCRIPTION_ID** - Azure Subscription Id of Azure AI Project\n",
    "7) **PROJECT_NAME** - Azure AI Project Name\n",
    "8) **RESOURCE_GROUP_NAME** - Azure AI Project Resource Group Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tool Call Accuracy evaluator assesses how accurately an AI uses tools by examining:\n",
    "- Relevance to the conversation\n",
    "- Parameter correctness according to tool definitions\n",
    "- Parameter value extraction from the conversation\n",
    "- Potential usefulness of the tool call\n",
    "\n",
    "The evaluator uses a binary scoring system (0 or 1):\n",
    "    - Score 0: The tool call is irrelevant or contains information not in the conversation/definition\n",
    "    - Score 1: The tool call is relevant with properly extracted parameters from the conversation\n",
    "\n",
    "This evaluation focuses on measuring whether tool calls meaningfully contribute to addressing query while properly following tool definitions and using information present in the conversation history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tool Call Accuracy requires following input:\n",
    "- Query - This can be a single query or a list of messages(conversation history with agent). Latter helps to determine if Agent used the information in history to make right tool calls.\n",
    "- Tool Calls - Tool Call(s) made by Agent to answer the query. Optional - if response has tool calls, if not provided evaluator will look for tool calls in response.\n",
    "- Response - (Optional)Response from Agent (or any GenAI App). This can be a single text response or a list or messages generated as part of Agent Response. If tool calls are not provide Tool Call Accuracy Evaluator will look at response for tool calls.\n",
    "- Tool Definitions - Tool(s) definition used by Agent to answer the query. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Tool Call Accuracy Evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class ToolCallAccuracyEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.ai.evaluation import ToolCallAccuracyEvaluator , AzureOpenAIModelConfiguration\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"MODEL_DEPLOYMENT_NAME\"],\n",
    ")\n",
    "\n",
    "\n",
    "tool_call_accuracy = ToolCallAccuracyEvaluator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Single Tool Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How is the weather in Seattle ?\"\n",
    "tool_calls=[\n",
    "                {\n",
    "                    \"type\": \"tool_call\",\n",
    "                    \"tool_call_id\": \"call_eYtq7fMyHxDWIgeG2s26h0lJ\",\n",
    "                    \"name\": \"fetch_weather\",\n",
    "                    \"arguments\": {\n",
    "                        \"location\": \"New York\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "tool_definitions=[\n",
    "                {\n",
    "                    \"name\": \"fetch_weather\",\n",
    "                    \"type\": \"function\",\n",
    "                    \"description\": \"Fetches the weather information for the specified location.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"location\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The location to fetch weather for.\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "EvaluationException",
     "evalue": "(UserError) Tool definition not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEvaluationException\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[43mtool_call_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_calls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_call\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_definitions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_definitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m pprint(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Repos\\azure\\azure-sdk-for-python\\sdk\\evaluation\\azure-ai-evaluation\\azure\\ai\\evaluation\\_evaluators\\_tool_call_accuracy\\_tool_call_accuracy.py:366\u001b[39m, in \u001b[36mToolCallAccuracyEvaluator.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(  \u001b[38;5;66;03m# pylint: disable=docstring-missing-param\u001b[39;00m\n\u001b[32m    343\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    344\u001b[39m     *args,\n\u001b[32m    345\u001b[39m     **kwargs,\n\u001b[32m    346\u001b[39m ):\n\u001b[32m    347\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    348\u001b[39m \u001b[33;03m    Evaluate tool call accuracy. Accepts a query, tool definitions, and tool calls for evaluation.\u001b[39;00m\n\u001b[32m    349\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    364\u001b[39m \u001b[33;03m    :rtype: Dict[str, Union[str, float]]\u001b[39;00m\n\u001b[32m    365\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Repos\\azure\\azure-sdk-for-python\\sdk\\evaluation\\azure-ai-evaluation\\azure\\ai\\evaluation\\_evaluators\\_common\\_base_eval.py:136\u001b[39m, in \u001b[36mEvaluatorBase.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(  \u001b[38;5;66;03m# pylint: disable=docstring-missing-param\u001b[39;00m\n\u001b[32m    122\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    123\u001b[39m     *args,\n\u001b[32m    124\u001b[39m     **kwargs,\n\u001b[32m    125\u001b[39m ) -> Union[DoEvalResult[T_EvalValue], AggregateResult[T_EvalValue]]:\n\u001b[32m    126\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Evaluate a given input. This method serves as a wrapper and is meant to be overridden by child classes for\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[33;03m    one main reason - to overwrite the method headers and docstring to include additional inputs as needed.\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[33;03m    The actual behavior of this function shouldn't change beyond adding more inputs to the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    134\u001b[39m \u001b[33;03m    :rtype: Union[DoEvalResult[T_EvalValue], AggregateResult[T_EvalValue]]\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masync_run_allowing_running_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_async_evaluator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anksing\\.conda\\envs\\azure-ai-evaluation-dev\\Lib\\site-packages\\promptflow\\_utils\\async_utils.py:94\u001b[39m, in \u001b[36masync_run_allowing_running_loop\u001b[39m\u001b[34m(async_func, *args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _has_running_loop():\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutorWithContext() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43masync_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m asyncio.run(_invoke_async_with_sigint_handler(async_func, *args, **kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anksing\\.conda\\envs\\azure-ai-evaluation-dev\\Lib\\concurrent\\futures\\_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anksing\\.conda\\envs\\azure-ai-evaluation-dev\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anksing\\.conda\\envs\\azure-ai-evaluation-dev\\Lib\\concurrent\\futures\\thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anksing\\.conda\\envs\\azure-ai-evaluation-dev\\Lib\\site-packages\\promptflow\\_utils\\async_utils.py:94\u001b[39m, in \u001b[36masync_run_allowing_running_loop.<locals>.<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _has_running_loop():\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutorWithContext() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m executor.submit(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43masync_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m).result()\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m asyncio.run(_invoke_async_with_sigint_handler(async_func, *args, **kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anksing\\.conda\\envs\\azure-ai-evaluation-dev\\Lib\\asyncio\\runners.py:195\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug, loop_factory)\u001b[39m\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    192\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug, loop_factory=loop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anksing\\.conda\\envs\\azure-ai-evaluation-dev\\Lib\\asyncio\\runners.py:118\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(self, coro, context)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._interrupt_count = \u001b[32m0\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.CancelledError:\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._interrupt_count > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anksing\\.conda\\envs\\azure-ai-evaluation-dev\\Lib\\asyncio\\base_events.py:691\u001b[39m, in \u001b[36mBaseEventLoop.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    688\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m future.done():\n\u001b[32m    689\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m691\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Repos\\azure\\azure-sdk-for-python\\sdk\\evaluation\\azure-ai-evaluation\\azure\\ai\\evaluation\\_evaluators\\_common\\_base_eval.py:528\u001b[39m, in \u001b[36mAsyncEvaluatorBase.__call__\u001b[39m\u001b[34m(self, query, response, context, conversation, ground_truth, tool_calls, tool_definitions, messages, retrieval_ground_truth, retrieved_documents, **kwargs)\u001b[39m\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retrieved_documents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    526\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mretrieved_documents\u001b[39m\u001b[33m\"\u001b[39m] = retrieved_documents\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._real_call(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Repos\\azure\\azure-sdk-for-python\\sdk\\evaluation\\azure-ai-evaluation\\azure\\ai\\evaluation\\_evaluators\\_tool_call_accuracy\\_tool_call_accuracy.py:249\u001b[39m, in \u001b[36mToolCallAccuracyEvaluator._real_call\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"The asynchronous call where real end-to-end evaluation logic is performed.\u001b[39;00m\n\u001b[32m    242\u001b[39m \n\u001b[32m    243\u001b[39m \u001b[33;03m:keyword kwargs: The inputs to evaluate.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    246\u001b[39m \u001b[33;03m:rtype: Union[DoEvalResult[T_EvalValue], AggregateResult[T_EvalValue]]\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# Convert inputs into list of evaluable inputs.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m eval_input_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_kwargs_to_eval_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(eval_input_list) == \u001b[32m0\u001b[39m:\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;28mself\u001b[39m._AGGREGATE_RESULT_KEY: \u001b[38;5;28mself\u001b[39m._NOT_APPLICABLE_RESULT,\n\u001b[32m    252\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._AGGREGATE_RESULT_KEY\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_result\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._NOT_APPLICABLE_RESULT,\n\u001b[32m    253\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._AGGREGATE_RESULT_KEY\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_threshold\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.threshold,\n\u001b[32m   (...)\u001b[39m\u001b[32m    256\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mper_tool_call_details\u001b[39m\u001b[33m\"\u001b[39m: []\n\u001b[32m    257\u001b[39m             }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Repos\\azure\\azure-sdk-for-python\\sdk\\evaluation\\azure-ai-evaluation\\azure\\ai\\evaluation\\_evaluators\\_tool_call_accuracy\\_tool_call_accuracy.py:202\u001b[39m, in \u001b[36mToolCallAccuracyEvaluator._convert_kwargs_to_eval_input\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    200\u001b[39m         eval_inputs.append({\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: query, \u001b[33m\"\u001b[39m\u001b[33mtool_call\u001b[39m\u001b[33m\"\u001b[39m: tool_call, \u001b[33m\"\u001b[39m\u001b[33mtool_definition\u001b[39m\u001b[33m\"\u001b[39m: tool_definition})\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m EvaluationException(\n\u001b[32m    203\u001b[39m             message=\u001b[33m\"\u001b[39m\u001b[33mTool definition not found\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    204\u001b[39m             blame=ErrorBlame.USER_ERROR,\n\u001b[32m    205\u001b[39m             category=ErrorCategory.INVALID_VALUE,\n\u001b[32m    206\u001b[39m             target=ErrorTarget.TOOL_CALL_ACCURACY_EVALUATOR,\n\u001b[32m    207\u001b[39m         )\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m eval_inputs\n",
      "\u001b[31mEvaluationException\u001b[39m: (UserError) Tool definition not found"
     ]
    }
   ],
   "source": [
    "response = tool_call_accuracy(query=query, tool_calls=tool_call, tool_definition=tool_definitions)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Tool Calls used by Agent to respond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How is the weather in Seattle ?\"\n",
    "tool_calls = [{\n",
    "                    \"type\": \"tool_call\",\n",
    "                    \"tool_call_id\": \"call_CUdbkBfvVBla2YP3p24uhElJ\",\n",
    "                    \"name\": \"fetch_weather\",\n",
    "                    \"arguments\": {\n",
    "                        \"location\": \"Seattle\"\n",
    "                    }\n",
    "            },\n",
    "            {\n",
    "                    \"type\": \"tool_call\",\n",
    "                    \"tool_call_id\": \"call_CUdbkBfvVBla2YP3p24uhElJ\",\n",
    "                    \"name\": \"fetch_weather\",\n",
    "                    \"arguments\": {\n",
    "                        \"location\": \"London\"\n",
    "                    }\n",
    "            }]\n",
    "\n",
    "tool_definition = {\n",
    "                    \"id\": \"fetch_weather\",\n",
    "                    \"name\": \"fetch_weather\",\n",
    "                    \"description\": \"Fetches the weather information for the specified location.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"location\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The location to fetch weather for.\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tool_call_accuracy(query=query, tool_calls=tool_calls, tool_definitions=tool_definition)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tool Calls passed as part of `Response` (common for agent case)\n",
    "- Tool Call Accuracy Evaluator extracts tool calls from response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you send me an email with weather information for Seattle?\"\n",
    "response = [\n",
    "        {\n",
    "            \"createdAt\": \"2025-03-26T17:27:35Z\",\n",
    "            \"run_id\": \"run_zblZyGCNyx6aOYTadmaqM4QN\",\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"tool_call\",\n",
    "                    \"tool_call_id\": \"call_CUdbkBfvVBla2YP3p24uhElJ\",\n",
    "                    \"name\": \"fetch_weather\",\n",
    "                    \"arguments\": {\n",
    "                        \"location\": \"Seattle\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"createdAt\": \"2025-03-26T17:27:37Z\",\n",
    "            \"run_id\": \"run_zblZyGCNyx6aOYTadmaqM4QN\",\n",
    "            \"tool_call_id\": \"call_CUdbkBfvVBla2YP3p24uhElJ\",\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"tool_result\",\n",
    "                    \"tool_result\": {\n",
    "                        \"weather\": \"Rainy, 14\\u00b0C\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"createdAt\": \"2025-03-26T17:27:38Z\",\n",
    "            \"run_id\": \"run_zblZyGCNyx6aOYTadmaqM4QN\",\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"tool_call\",\n",
    "                    \"tool_call_id\": \"call_iq9RuPxqzykebvACgX8pqRW2\",\n",
    "                    \"name\": \"send_email\",\n",
    "                    \"arguments\": {\n",
    "                        \"recipient\": \"your_email@example.com\",\n",
    "                        \"subject\": \"Weather Information for Seattle\",\n",
    "                        \"body\": \"The current weather in Seattle is rainy with a temperature of 14\\u00b0C.\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"createdAt\": \"2025-03-26T17:27:41Z\",\n",
    "            \"run_id\": \"run_zblZyGCNyx6aOYTadmaqM4QN\",\n",
    "            \"tool_call_id\": \"call_iq9RuPxqzykebvACgX8pqRW2\",\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"tool_result\",\n",
    "                    \"tool_result\": {\n",
    "                        \"message\": \"Email successfully sent to your_email@example.com.\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"createdAt\": \"2025-03-26T17:27:42Z\",\n",
    "            \"run_id\": \"run_zblZyGCNyx6aOYTadmaqM4QN\",\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"I have successfully sent you an email with the weather information for Seattle. The current weather is rainy with a temperature of 14\\u00b0C.\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "tool_definitions = [\n",
    "    {\n",
    "\t\t\"name\": \"fetch_weather\",\n",
    "\t\t\"description\": \"Fetches the weather information for the specified location.\",\n",
    "\t\t\"parameters\": {\n",
    "\t\t\t\"type\": \"object\",\n",
    "\t\t\t\"properties\": {\n",
    "\t\t\t\t\"location\": {\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"description\": \"The location to fetch weather for.\"\n",
    "\t\t\t\t}\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\t},\n",
    "    {\n",
    "\t\t\"name\": \"send_email\",\n",
    "\t\t\"description\": \"Sends an email with the specified subject and body to the recipient.\",\n",
    "\t\t\"parameters\": {\n",
    "\t\t\t\"type\": \"object\",\n",
    "\t\t\t\"properties\": {\n",
    "\t\t\t\t\"recipient\": {\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"description\": \"Email address of the recipient.\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t\"subject\": {\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"description\": \"Subject of the email.\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t\"body\": {\n",
    "\t\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\t\"description\": \"Body content of the email.\"\n",
    "\t\t\t\t}\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\t}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tool_call_accuracy(query=query, response=response, tool_definitions=tool_definitions)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure-ai-evaluation-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
