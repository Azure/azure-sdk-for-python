{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing AOAI Integration Features in the AIP SDK\n",
    "\n",
    "This notebook will run users through the following features in the AI SDK:\n",
    "- Using creating grader classes by:\n",
    "    - Directly supplying an OAI grader config\n",
    "    - Using grader-specific classes.\n",
    "- Submitting graders to the `evaluate` method to start an evaluation\n",
    "- Submitting graders to the remote evaluation service\n",
    "\n",
    "## Pre-setup\n",
    "Check that you have the following 2 other files available. These should have been included in the zip that contains this notebook\n",
    "- eval_eval_input.jsonl : a simple test dataset\n",
    "- azure_ai_evaluation-1.6.0-py3-none-any.whl : The pre-release variant of the AI SDK that contains the changes of interest.\n",
    "\n",
    "Install the AI and projects SDKs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azure.ai.projects\n",
    "!pip install azure_ai_evaluation-1.6.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup credentials needed for later logic. These need to be filled in with whatever values you use, or with the values supplied outside of this value during the bugbash:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO fill in with an API, or use your own credentials\n",
    "model_config = {\n",
    "    \"azure_endpoint\": \"https://jamahaja-gpt4o-westus2.openai.azure.com/\",\n",
    "    # \"api_key\": \"...\",\n",
    "    \"api_version\": \"2025-04-01-preview\",\n",
    "    \"azure_deployment\": \"gpt-4o\"\n",
    "}\n",
    "project = {\n",
    "    \"subscription_id\": \"2d385bf4-0756-4a76-aa95-28bf9ed3b625\",\n",
    "    \"resource_group_name\": \"rg-quso-ai-canary\",\n",
    "    \"project_name\": \"qusong-canary\"\n",
    "}\n",
    "fname=\"test_eval_input.jsonl\"\n",
    "\n",
    "if model_config[\"api_key\"] == \"...\":\n",
    "    raise ValueError(\"Please set your API key in the code snippet.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create grader objects\n",
    "\n",
    "The AIP SDK wraps OpenAI Grader configurations alongside the necessary credentials to ensure that any grader can be evaluated without additional context. 5 objects are created below, a normal evaluator for comparison, three specific grader classes, and a general grader. The details of the two grader types are as follows:\n",
    "- Grader specific classes. The first three graders each have unique, required inputs that simplify their use. These inputs are then plugged directly into the corresponding OAI grader configuration object. The graders accounted for are listed in the OAI API [here](https://github.com/openai/openai-python/blob/main/src/openai/types/eval_create_params.py#L151).\n",
    "- The general `AzureOpenAIGrader` class. Rather than accepting the exact inputs needed to create a specific grader configuration, this class simply accepts a single dictionary and performs no validation upon it. This inputted object is assumed to be an OAI-API-ready configuration. This class is expected to be used by OAI veterans and users who want to test bleeding-edge features that have yet to be accounted for by the other specific grader classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.eval_string_check_grader import EvalStringCheckGrader\n",
    "from azure.ai.evaluation import (\n",
    "    AzureOpenAILabelGrader,\n",
    "    AzureOpenAIStringCheckGrader,\n",
    "    AzureOpenAITextSimilarityGrader,\n",
    "    AzureOpenAIGrader,\n",
    "    F1ScoreEvaluator,\n",
    ")\n",
    "\n",
    "# create a normal evaluator for comparison\n",
    "f1_eval = F1ScoreEvaluator()\n",
    "\n",
    "## ---- Initialize specific graders ----\n",
    "\n",
    "# Corresponds to https://github.com/openai/openai-python/blob/ed53107e10e6c86754866b48f8bd862659134ca8/src/openai/types/eval_text_similarity_grader.py#L11\n",
    "sim_grader = AzureOpenAITextSimilarityGrader(\n",
    "    model_config=model_config,\n",
    "    evaluation_metric=\"fuzzy_match\",\n",
    "    input=\"{{item.query}}\",\n",
    "    name=\"similarity\",\n",
    "    pass_threshold=1,\n",
    "    reference=\"{{item.query}}\",\n",
    ")\n",
    "\n",
    "# Corresponds to https://github.com/openai/openai-python/blob/ed53107e10e6c86754866b48f8bd862659134ca8/src/openai/types/eval_string_check_grader_param.py#L10\n",
    "string_grader = AzureOpenAIStringCheckGrader(\n",
    "    model_config=model_config,\n",
    "    input=\"{{item.query}}\",\n",
    "    name=\"starts with what is\",\n",
    "    operation=\"like\",\n",
    "    reference=\"What is\",\n",
    ")\n",
    "\n",
    "# Corresponds to https://github.com/openai/openai-python/blob/ed53107e10e6c86754866b48f8bd862659134ca8/src/openai/types/eval_create_params.py#L132\n",
    "label_grader = AzureOpenAILabelGrader(\n",
    "    model_config=model_config,\n",
    "    input=[{\"content\": \"{{item.query}}\", \"role\": \"user\"}],\n",
    "    labels=[\"too short\", \"just right\", \"too long\"],\n",
    "    passing_labels=[\"just right\"],\n",
    "    model=\"gpt-4o\",\n",
    "    name=\"label\",\n",
    ")\n",
    "\n",
    "# ---- General Grader Initialization ----\n",
    "\n",
    "# Define an string check grader config directly using the OAI SDK\n",
    "oai_string_check_grader = EvalStringCheckGrader(\n",
    "    input=\"{{item.query}}\",\n",
    "    name=\"contains hello\",\n",
    "    operation=\"like\",\n",
    "    reference=\"hello\",\n",
    "    type=\"string_check\"\n",
    ")\n",
    "# Plug that into the general grader\n",
    "general_grader = AzureOpenAIGrader(\n",
    "    model_config=model_config,\n",
    "    grader_config=oai_string_check_grader\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Local\" evaluation\n",
    "\n",
    "Using the `evaluate` method, we can evaluate the test dataset against the graders. The word 'local' is quoted because the code then calls the OAI API to perform the evaluations. The resulting logs will note this. Click on the studio url to view the resulting evaluation online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "evaluation = evaluate(\n",
    "    data=fname,\n",
    "    evaluators={\n",
    "        \"label\": label_grader,\n",
    "        \"general\": general_grader,\n",
    "        \"string\": string_grader,\n",
    "        \"similarity\": sim_grader,\n",
    "        \"f1\": f1_eval,\n",
    "    },\n",
    "    azure_ai_project=project\n",
    ")\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remote evaluation\n",
    "\n",
    "Using the Projects SDK, we can have the remote evaluation service evaluate graders for us. The existing `EvaluatorConfiguration` object is already capable of handling graders as inputs.\n",
    "\n",
    "Start by filling in some asset ids you will need for remote evaluation. The first two have already been filled in with functional defaults, but the third value will need to be pulled from the front page of the Foundry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id =\"azureml://locations/eastus2euap/workspaces/e362e695-8af6-42b9-8fcf-dd271e7d0d53/data/generated_response/versions/1\"\n",
    "environment_id = \"azureml://registries/jamahaja-evals-registry/environments/etwinter-aoai/versions/12\"\n",
    "project_connection_string = \"TODO FILL ME IN PLEASE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the evaluator configs that will be supplied to the remove evaluation service,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects.models import EvaluatorConfiguration\n",
    "from azure.ai.evaluation import (\n",
    "    LabelGrader,\n",
    "    StringCheckGrader,\n",
    "    TextSimilarityGrader,\n",
    "    AoaiGrader,\n",
    ")\n",
    "\n",
    "\n",
    "f1_eval_config = EvaluatorConfiguration(\n",
    "    id=\"azureml://registries/azureml/models/F1Score-Evaluator/versions/4\",\n",
    "    init_params={},\n",
    ")\n",
    "\n",
    "\n",
    "sim_grader_config = EvaluatorConfiguration(\n",
    "    id=TextSimilarityGrader.id,\n",
    "    init_params={\n",
    "        \"model_config\": model_config,\n",
    "        \"evaluation_metric\": \"fuzzy_match\",\n",
    "        \"input\": \"{{item.query}}\",\n",
    "        \"name\": \"similarity\",\n",
    "        \"pass_threshold\": 1,\n",
    "        \"reference\": \"{{item.query}}\",\n",
    "    },\n",
    ")\n",
    "\n",
    "string_grader_config = EvaluatorConfiguration(\n",
    "    id=StringCheckGrader.id,\n",
    "    init_params={\n",
    "        \"model_config\": model_config,\n",
    "        \"input\": \"{{item.query}}\",\n",
    "        \"name\": \"contains hello\",\n",
    "        \"operation\": \"like\",\n",
    "        \"reference\": \"hello\",\n",
    "    },\n",
    ")\n",
    "\n",
    "label_grader_config = EvaluatorConfiguration(\n",
    "    id=LabelGrader.id,\n",
    "    init_params={\n",
    "        \"model_config\": model_config,\n",
    "        \"input\": [{\"content\": \"{{item.query}}\", \"role\": \"user\"}],\n",
    "        \"labels\": [\"too short\", \"just right\", \"too long\"],\n",
    "        \"passing_labels\": [\"just right\"],\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"name\": \"label\",\n",
    "    },\n",
    ")\n",
    "\n",
    "general_grader_config = EvaluatorConfiguration(\n",
    "    id=AoaiGrader.id,\n",
    "    init_params={\n",
    "        \"model_config\": model_config,\n",
    "        \"grader_config\": {\n",
    "            \"input\": \"{{item.query}}\",\n",
    "            \"name\": \"contains hello\",\n",
    "            \"operation\": \"like\",\n",
    "            \"reference\": \"hello\",\n",
    "            \"type\": \"string_check\",\n",
    "        },\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, submit the remote evaluation run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import Evaluation, Dataset\n",
    "from azure.identity import DefaultAzureCredential\n",
    "# Note you might want to change the run name to avoid confusion with others\n",
    "run_name = \"Test Remote AOAI Evaluation\"\n",
    "evaluation = Evaluation(\n",
    "    display_name=run_name,\n",
    "    description=\"Evaluation started by test_remote_aoai_evaluation e2e test.\",\n",
    "    evaluators = {\n",
    "        \"f1\": f1_eval_config,\n",
    "        \"label\": label_grader_config,\n",
    "        \"general\": general_grader_config,\n",
    "        \"string\": string_grader_config,\n",
    "        \"similarity\": sim_grader_config,\n",
    "    },\n",
    "    data=Dataset(id=dataset_id),\n",
    "    properties={ \"Environment\":environment_id}\n",
    ")\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    conn_str=project_connection_string,\n",
    ")\n",
    "created_evaluation = project_client.evaluations.create(evaluation)\n",
    "print(f\"review remote evaluation results at {created_evaluation.properties['AiStudioEvaluationUri']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More examples\n",
    "\n",
    "The following code blocks show case more configurations and edge cases.\n",
    "\n",
    "### Adding Target mappings\n",
    "\n",
    "In this example, we re-define the string grader to require an input not found in the original dataset called \"new_input\". This value is instead produced by a target function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "string_grader = StringCheckGrader(\n",
    "    model_config=model_config,\n",
    "    input=\"{{item.new_input}}\",\n",
    "    name=\"contains hello\",\n",
    "    operation=\"like\",\n",
    "    reference=\"hello\",\n",
    ")\n",
    "\n",
    "def target_fn(query: str) -> str:\n",
    "    return {\"new_input\": query.replace(\"a\", \"e\")}\n",
    "\n",
    "evaluation = evaluate(\n",
    "    data=fname,\n",
    "    evaluators={\n",
    "        \"label\": label_grader,\n",
    "        \"general\": general_grader,\n",
    "        \"string\": string_grader,\n",
    "        \"similarity\": sim_grader,\n",
    "    },\n",
    "    azure_ai_project=project,\n",
    "    target=target_fn,\n",
    "    _use_run_submitter_client=True\n",
    ")\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grader-specific column mappings\n",
    "\n",
    "This example runs an evaluation in which some graders have unique target mappings, which cause their 'item.query' value to be derived from a different column in the dataset, instead of matching the original dataset's query column. \n",
    "\n",
    "This causes each grader to be evaluated by a separate OAI eval run, since there would otherwise be a risk of conflicting column mappings between graders. It's a post-build TODO item to determine when this caution is needed more carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "string_grader = StringCheckGrader(\n",
    "    model_config=model_config,\n",
    "    input=\"{{item.newer_input}}\",\n",
    "    name=\"contains hello\",\n",
    "    operation=\"like\",\n",
    "    reference=\"hello\",\n",
    ")\n",
    "\n",
    "def target_fn(query: str) -> str:\n",
    "    return {\"new_input\": query.replace(\"a\", \"e\")}\n",
    "\n",
    "evaluation = evaluate(\n",
    "    data=fname,\n",
    "    evaluators={\n",
    "        \"label\": label_grader,\n",
    "        \"general\": general_grader,\n",
    "        \"string\": string_grader,\n",
    "        \"similarity\": sim_grader,\n",
    "    },\n",
    "    azure_ai_project=project,\n",
    "    target=target_fn,\n",
    "    evaluation_config={\n",
    "        \"label\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.ground_truth}\",\n",
    "            }\n",
    "        },\n",
    "        \"general\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.response}\",\n",
    "            }\n",
    "        },\n",
    "        \"string\": {\n",
    "            \"column_mapping\": {\n",
    "                \"newer_input\": \"${target.new_input}\",\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    _use_run_submitter_client=True\n",
    ")\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
