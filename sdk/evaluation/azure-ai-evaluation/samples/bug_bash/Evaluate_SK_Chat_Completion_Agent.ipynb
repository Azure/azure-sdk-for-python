{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf5280e2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Evaluate Semantic Kernel AI (ChatCompletion) Agents in Azure AI Foundry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0330c099",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This sample demonstrates how to evaluate Semantic Kernel AI ChatCompletionAgents in Azure AI Foundry. It provides a step-by-step guide to set up the environment, create an agent, and evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b364c694",
   "metadata": {},
   "source": [
    "## Time\n",
    "You can expect to complete this sample in approximately 20 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919c6017",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "### Packages\n",
    "- `semantic-kernel` installed (`pip install semantic-kernel`)\n",
    "- `azure-ai-evaluation` SDK installed *(pip install latest version to include the SK converter)*\n",
    "- An Azure OpenAI resource with a deployment configured\n",
    "\n",
    "### Environment Variables\n",
    "- For AzureChatService: *(related to SK Agent generating conversation)*\n",
    "  - `api_key` The API key to access your Azure OpenAI resource.\n",
    "  - `chat_deployment_name` The name of the chat model deployment (e.g., gpt-4-32k) used by your agent.\n",
    "  - `endpoint` The full endpoint URL of your Azure OpenAI resource (e.g., https://your-resource.openai.azure.com).\n",
    "- For evaluating agents:\n",
    "  - `AZURE_OPENAI_ENDPOINT` The base endpoint for Azure OpenAI (e.g., https://<resource>.openai.azure.com).\n",
    "  - `AZURE_OPENAI_API_KEY` The API key for your Azure OpenAI resource.\n",
    "  - `AZURE_OPENAI_API_VERSION` The API version to use (e.g., 2024-05-01-preview).\n",
    "  - `MODEL_DEPLOYMENT_NAME` The model deployment name used by the agent or evaluation flow (e.g., gpt-4).\n",
    "- For Azure AI Foundry (Bonus):\n",
    "  - `AZURE_SUBSCRIPTION_ID` Your Azure subscription ID.\n",
    "  - `PROJECT_NAME` The name of your Azure AI Foundry project.\n",
    "  - `RESOURCE_GROUP_NAME` The name of the resource group containing your Azure AI project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1d6576",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Create a AzureChatCompletion service - [reference](https://learn.microsoft.com/en-us/semantic-kernel/concepts/ai-services/chat-completion/?tabs=csharp-AzureOpenAI%2Cpython-AzureOpenAI%2Cjava-AzureOpenAI&pivots=programming-language-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c50e12e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e17d87aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-01-preview\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getenv(\"AZURE_OPENAI_API_VERSION\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dc6ce40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "# You can do the following if you have set the necessary environment variables or created a .env file\n",
    "chat_completion_service = AzureChatCompletion(service_id=\"my-service-id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef319288",
   "metadata": {},
   "source": [
    "### Create a ChatCompletionAgent - [reference](https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/agent-types/chat-completion-agent?pivots=programming-language-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76781359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sk_tools import AgentToolsPlugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6abead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "\n",
    "# Create the agent by directly providing the chat completion service\n",
    "agent = ChatCompletionAgent(\n",
    "    service=chat_completion_service,\n",
    "    name=\"Assistant\",\n",
    "    instructions=\"You are a helpful assistant\",\n",
    "    plugins=[AgentToolsPlugin()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc6b03ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples\n",
    "bugbash_examples = {\n",
    "    \"fetch_current_datetime\": [\n",
    "        \"What is the current date and time?\",\n",
    "        \"Give me the current time in format '%A, %d %B %Y %I:%M %p'.\"\n",
    "    ],\n",
    "    \"fetch_weather\": [\n",
    "        \"What's the weather like in Tokyo?\",\n",
    "        \"Can you provide the weather information for London?\",\n",
    "        \"Check the weather for Cairo.\"\n",
    "    ],\n",
    "    \"send_email\": [\n",
    "        \"Send an email to jane.doe@example.com with the subject 'Lunch Plans' and body 'Shall we grab lunch at 12:30 today?'\",\n",
    "        \"Email to manager@company.com with subject 'Weekly Update' and body 'The sprint is on track, and all tasks are green.'\"\n",
    "    ],\n",
    "    \"send_email_using_recipient_name\": [\n",
    "        \"Send an email to Ahmed with subject 'Bugbash Follow-up' and body 'Thanks for your participation!'\",\n",
    "        \"Email Sarah with subject 'Meeting Notes' and body 'I've attached the minutes of today's meeting.'\"\n",
    "    ],\n",
    "    \"calculate_sum\": [\n",
    "        \"What's the sum of 103 and 47?\",\n",
    "        \"Add 999 and 1.\"\n",
    "    ],\n",
    "    \"convert_temperature\": [\n",
    "        \"Convert 30 degrees Celsius to Fahrenheit.\",\n",
    "        \"What's 0°C in Fahrenheit?\"\n",
    "    ],\n",
    "    \"toggle_flag\": [\n",
    "        \"Toggle the flag True.\",\n",
    "        \"If a value is False, what will it be if toggled?\"\n",
    "    ],\n",
    "    \"merge_dicts\": [\n",
    "        \"Merge these two dictionaries: {'team': 'AI'} and {'members': 5}.\",\n",
    "        \"Combine {'x': 10, 'y': 20} with {'y': 30, 'z': 40}.\"\n",
    "    ],\n",
    "    \"get_user_info\": [\n",
    "        \"Retrieve user information for user ID 2.\",\n",
    "        \"What are the details for user ID 99?\"\n",
    "    ],\n",
    "    \"longest_word_in_sentences\": [\n",
    "        \"Find the longest word in each of these sentences: ['The sky was clear and blue', 'Programming is fun'].\",\n",
    "        \"Give me the longest word per sentence: ['Azure is scalable', 'Pythonic code is clean and readable'].\"\n",
    "    ],\n",
    "    \"process_records\": [\n",
    "        \"Process the following records: [{'math': 95, 'science': 85}, {'art': 70, 'music': 80}].\",\n",
    "        \"Sum the values in each record: [{'a': 5, 'b': 10, 'c': 20}, {'x': 1, 'y': 2}]\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "bugbash_topics = list(bugbash_examples.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c215996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected topic: fetch_weather\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "topic = random.choice(bugbash_topics)\n",
    "print(f\"Selected topic: {topic}\")\n",
    "examples = bugbash_examples[topic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7b9ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## User: Hello\n",
      "## Assistant: Hello! How can I assist you today?\n",
      "\n",
      "## User: What's the weather like in Tokyo?\n",
      "## Assistant: The weather in Tokyo is rainy, with a temperature of 22°C.\n",
      "\n",
      "## User: Can you provide the weather information for London?\n",
      "## Assistant: The weather in London is cloudy, with a temperature of 18°C.\n",
      "\n",
      "## User: Check the weather for Cairo.\n",
      "## Assistant: I'm sorry, but the weather data is not available for Cairo at the moment.\n",
      "\n",
      "## User: Thank you\n",
      "## Assistant: You're welcome! If you have any more questions or need further assistance, feel free to ask.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "thread = None\n",
    "\n",
    "user_inputs = [\"Hello\"] + examples + [\"Thank you\"]\n",
    "\n",
    "for user_input in user_inputs:\n",
    "    response = await agent.get_response(messages=user_input, thread=thread)\n",
    "    print(f\"## User: {user_input}\")\n",
    "    print(f\"## {response.name}: {response}\\n\")\n",
    "    thread = response.thread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2586d3e5",
   "metadata": {},
   "source": [
    "### Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcd6ac41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available turn indices: [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import SKAgentConverter\n",
    "\n",
    "# Get the avaiable turn indices for the thread,\n",
    "# useful for selecting a specific turn for evaluation\n",
    "turn_indices = await SKAgentConverter._get_thread_turn_indices(thread=thread)\n",
    "print(f\"Available turn indices: {turn_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1d4ae12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class SKAgentConverter: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "converter = SKAgentConverter()\n",
    "\n",
    "# Get a single agent run data\n",
    "evaluation_data_single_run = await converter.convert(\n",
    "    thread=thread,\n",
    "    turn_index=1, # Specify the turn index you want to evaluate\n",
    "    agent=agent # Pass it to include the instructions and plugins in the evaluation data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7813b5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_name = \"evaluation_data.jsonl\"\n",
    "# Save the agent thread data to a JSONL file (all turns)\n",
    "evaluation_data = await converter.prepare_evaluation_data(threads=[thread], filename=file_name, agent=agent)\n",
    "# print(json.dumps(evaluation_data, indent=4))\n",
    "len(evaluation_data) # number of turns in the thread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf87cab",
   "metadata": {},
   "source": [
    "### Setting up evaluator\n",
    "\n",
    "We will select the following evaluators to assess the different aspects relevant for agent quality: \n",
    "\n",
    "- [Intent resolution](https://aka.ms/intentresolution-sample): measures the extent of which an agent identifies the correct intent from a user query. Scale: integer 1-5. Higher is better.\n",
    "- [Tool call accuracy](https://aka.ms/toolcallaccuracy-sample): evaluates the agent’s ability to select the appropriate tools, and process correct parameters from previous steps. Scale: float 0-1. Higher is better.\n",
    "- [Task adherence](https://aka.ms/taskadherence-sample): measures the extent of which an agent’s final response adheres to the task based on its system message and a user query. Scale: integer 1-5. Higher is better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6ee09df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class IntentResolutionEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class ToolCallAccuracyEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class TaskAdherenceEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "from azure.ai.evaluation import (\n",
    "    ToolCallAccuracyEvaluator,\n",
    "    AzureOpenAIModelConfiguration,\n",
    "    IntentResolutionEvaluator,\n",
    "    TaskAdherenceEvaluator,\n",
    ")\n",
    "\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"MODEL_DEPLOYMENT_NAME\"],\n",
    ")\n",
    "\n",
    "intent_resolution = IntentResolutionEvaluator(model_config=model_config)\n",
    "\n",
    "tool_call_accuracy = ToolCallAccuracyEvaluator(model_config=model_config)\n",
    "\n",
    "task_adherence = TaskAdherenceEvaluator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80bd50ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"tool_call_accuracy\": 1.0,\n",
      "    \"tool_call_accuracy_result\": \"pass\",\n",
      "    \"tool_call_accuracy_threshold\": 0.8,\n",
      "    \"per_tool_call_details\": [\n",
      "        {\n",
      "            \"tool_call_accurate\": true,\n",
      "            \"tool_call_accurate_reason\": \"The TOOL CALL is directly relevant to the user's inquiry about the weather in Tokyo, uses appropriate parameters that match the TOOL DEFINITION, and the parameter values are correctly inferred from the CONVERSATION. Thus, it is likely to be very useful in advancing the conversation.\",\n",
      "            \"tool_call_id\": \"call_UNFfTMKX4PdWYzDDp0PLYhh5\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test a single evaluation run\n",
    "evaluator = ToolCallAccuracyEvaluator(model_config=model_config)\n",
    "\n",
    "# evaluation_data_single_run.keys() # query, response, tool_definitions\n",
    "res = evaluator(**evaluation_data_single_run)\n",
    "print(json.dumps(res, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bab561",
   "metadata": {},
   "source": [
    "#### Bonus - run on perviously saved file for all turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0530c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "response = evaluate(\n",
    "    data=file_name,\n",
    "    evaluators={\n",
    "        \"tool_call_accuracy\": tool_call_accuracy,\n",
    "        \"intent_resolution\": intent_resolution,\n",
    "        \"task_adherence\": task_adherence,\n",
    "    },\n",
    "    azure_ai_project={\n",
    "        \"subscription_id\": os.environ[\"AZURE_SUBSCRIPTION_ID\"],\n",
    "        \"project_name\": os.environ[\"PROJECT_NAME\"],\n",
    "        \"resource_group_name\": os.environ[\"RESOURCE_GROUP_NAME\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "pprint(f'AI Foundary URL: {response.get(\"studio_url\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac38d924",
   "metadata": {},
   "source": [
    "## Inspect results on Azure AI Foundry\n",
    "\n",
    "Go to AI Foundry URL for rich Azure AI Foundry data visualization to inspect the evaluation scores and reasoning to quickly identify bugs and issues of your agent to fix and improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225ae69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively, you can use the following to get the evaluation results in memory\n",
    "\n",
    "# average scores across all runs\n",
    "pprint(response[\"metrics\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
