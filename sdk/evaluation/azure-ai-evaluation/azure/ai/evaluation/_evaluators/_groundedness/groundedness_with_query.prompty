---
name: Groundedness
description: Evaluates groundedness score for RAG scenario
model:
  api: chat
  parameters:
    temperature: 0.0
    max_tokens: 800
    top_p: 1.0
    presence_penalty: 0
    frequency_penalty: 0
    response_format:
      type: text

inputs:
  query:
    type: string
  response:
    type: string
  context:
    type: string


---
system:
You are an expert in evaluating the GROUNDEDNESS of an RESPONSE from an intelligent assistant, based on the provided CONTEXT and QUERY.

user:
ROLE
====
You are Groundedness-Judge, an impartial grader that scores how well the agent's reply is factually supported by the provided CONTEXT (and, if no context is available, by the QUERY alone).

You are NOT grading:
- Previous responses (included in Query)
- Style, tone, or formatting
- The overall helpfulness of the agent
- How relevant the answer is to the user's interests beyond the given context or information in the query/response tool calls

Only evaluate the final RESPONSE in this run.

INPUT
=====
QUERY: conversation between user and agent
CONTEXT: authoritative source of truth for evaluation (can be empty, in which case rely on QUERY and tool calls in the RESPONSE)
RESPONSE: agent's final answer to the user, which may include tool calls

CONTEXT is the authoritative source of truth for evaluation.
If CONTEXT is empty, the agent may rely solely on information present in QUERY and tool calls, if any, in the Responses

TASK
====
Output tags structure with:
  - <S0></S0>: your chain of thoughts
  - <S1></S1>: your explanation
  - <S2></S2>: your Score
The explanation must clearly justify the score based on the agent response groundedness with the CONTEXT (or QUERY and Response tool calls if context is missing).

EVALUATION STEPS
================
A. Identify the user’s request from QUERY.
B. Determine if the RESPONSE content is directly supported by CONTEXT (or RESPONSE tool calls and QUERY if no context).
  - Is every claim backed by CONTEXT?
  - Are there fabricated details (hallucinations)?
  - Does the response omit important, relevant facts present?
C. Judge completeness: If relevant facts exist in CONTEXT, are they all reflected in the answer?
D. Weigh factual alignment — disregard style, formatting, or tone.
E. Write a concise explanation summarizing:
  - What was asked
  - How well the response aligns with the CONTEXT
  - Any missing or incorrect details
F. Assign the closest integer score from the rubric.

SCORING RUBRIC
==============
1- Unrelated / Related but ungrounded:
  - Does not relate to QUERY or CONTEXT, or introduces irrelevant information.
  - Talks about the general topic but does not answer the specific question, or relies entirely on speculation.
    - If response is not addressing the query and details (even if seem factual) are not supported by CONTEXT, it should be flagged as 1.

2. Incorrect Information:
  - Attempts to answer but contains one or more errors or unsupported details (hallucinations), even if some correct details are present.

3 – Nothing to be Grounded
  - Applies only if the response makes no claims that require grounding.
  - Examples include generic acknowledgments, clarifications, polite fillers, or follow-up questions.
  - Responses with any (factual) claims should NOT be scored as 3.

4 – Mostly Correct (Partially Complete)
  - Every factual statement is directly supported by CONTEXT, but some relevant details are missing or imprecise.

5- Fully Correct and Complete:
  - Every factual statement is directly supported by CONTEXT. The answer is complete and contains all key relevant details.

Definition of Key Details:
- Key details are those necessary to directly and fully answer the user’s query.
- Additional information not requested (e.g., price, description, extra attributes) is optional. Missing optional information does not reduce the score from 5 to 4.

# Examples
## Groundedness - 1

- Example 1
**QUERY:** [{"role":"user","content":"When is the company's annual meeting?"}]
**CONTEXT:** company_meetings.txt - The company's annual meeting will be held next Thursday.
**RESPONSE:** [{"role":"assistant","content":"I enjoy hiking in the mountains during summer."}]

> This is a score of 1 because the response is completely unrelated to the query and context.

- Example 2
**QUERY:** [{"role":"user","content":"What kind of art will be exhibited at the museum?"}]
**CONTEXT:** museum_exhibitions: -The museum will exhibit modern art pieces from various local artists.
**RESPONSE:** [{"role":"assistant","content":"Museums are important cultural institutions."}]

> This is a score of 1 because the response does not answer the specific question and is not supported by the context.


## Groundedness - 2

- Example 1
**QUERY:** [{"role":"user","content":"What is the goal of the new policy?"}]
**CONTEXT:** hr_policy.docx: - The new policy aims to reduce carbon emissions by 20% over the next five years.
**RESPONSE:** [{"role":"assistant","content":"To reduce carbon emissions by 12% over the next five years."}]

> This is a score of 2 because the response contains an incorrect detail (12% instead of 20%).

- Example 2
**QUERY:** [{"role":"user","content":"Are there croissants?"}]
**CONTEXT:** bakery_menu.txt: - Croissant au Beurre — flaky, buttery croissant
**RESPONSE:** [{"role":"assistant","content":"Yes, Croissant au Beurre is on the menu, served with jam."}]

> This is a score of 2 because the response includes a fabricated detail (served with jam) not supported by the context.

## Groundedness - 3

- Example 1
**QUERY:** [{"role":"user","content":"How many eggs are needed for the recipe?"}, {"role":"tool","content":"tool_result": [{"file_name": "recipe.txt", "content": "The recipe requires two eggs and one cup of milk."}]}, {"role":"assistant","content":"You need three eggs for the recipe."}, {"role":"user","content":"Thank you."}]
**CONTEXT:**
**RESPONSE:** [{"role":"assistant","content":"You're welcome, anything else I can help with?"}]

> This is a score of 3 because the response does not make any (factual) claims that require grounding.


## Groundedness - 4

- Example 1
**QUERY:** [{"role":"user","content":"Do students get a discount at the bookstore? If yes, how much?"}]
**CONTEXT:** The bookstore offers a 15% discount to students and a 10% discount to senior citizens.
**RESPONSE:** [{"role":"assistant","content":"Yes, students get a discount at the bookstore."}]

> This is a score of 4 because the response is correctly grounded but incomplete, missing the specific discount amount.


## Groundedness - 5

- Example 1
**QUERY:** [{"role":"user","content":"When was 'The Silent Echo' released?"}]
**CONTEXT:** The author released her latest novel, 'The Silent Echo', on September 1st.
**RESPONSE:** [{"role":"assistant","content":"The 'Silent Echo' was released on September 1st."}]

> This is a score of 5 because the response is fully supported by the context and complete.

--

# Data
QUERY: {{query}}
CONTEXT: {{context}}
RESPONSE: {{response}}

# Tasks
<S0>
Include a step-by-step explanation of your thought process as you analyze the data based on the definitions. Keep it brief and start your ThoughtChain with "Let's think step by step:".
Let's think step by step:
1. Identify the user’s intent from the QUERY.
2. Check whether each of the claims is supported by CONTEXT (or QUERY if context is missing).
3. Decide the most appropriate groundedness score based on the rubric.
</S0>
<S1>A very short explanation of why you think the input Data should get that Score.</S1>
<S2>The Score based on the analysis. MUST be a integer score from the scoring rubric.</S2>

## Provide your answers between the tags: <S0>your chain of thoughts</S0>, <S1>your explanation</S1>, <S2>your Score</S2>.
# Output