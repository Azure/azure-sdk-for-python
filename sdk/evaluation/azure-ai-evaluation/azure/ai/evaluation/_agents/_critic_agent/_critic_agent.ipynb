{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14af9ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Could not import SKAgentConverter. Please install the dependency with `pip install semantic-kernel`.\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "import time\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from critic_functions import user_functions\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv(\"C:\\\\Users\\\\ghyadav\\\\work\\\\ghyadav_azure_sdk\\\\azure-sdk-for-python\\\\sdk\\\\evaluation\\\\azure-ai-evaluation\\\\critic_agent\\\\ghyadav.env\")  # Load environment variables from .env file\n",
    "load_dotenv(\"./eval_ws.env\")  # Load environment variables from .env file\n",
    "\n",
    "from azure.ai.projects import __version__ as projects_version\n",
    "from packaging.version import Version\n",
    "# some dependencies have been updated with breaking changes -- indicates whether to use updated models and APIs or not\n",
    "updated_agents = Version(projects_version) > Version(\"1.0.0b10\") or projects_version.startswith(\"1.0.0a\")\n",
    "\n",
    "if updated_agents:\n",
    "    from azure.ai.agents.models import FunctionTool, ToolSet\n",
    "    project_client = AIProjectClient(\n",
    "        endpoint=os.environ[\"PROJECT_ENDPOINT\"],\n",
    "        credential=DefaultAzureCredential(),\n",
    "        # api_version=\"latest\"\n",
    "    )\n",
    "else:\n",
    "    from azure.ai.projects.models import FunctionTool, ToolSet\n",
    "    project_client = AIProjectClient.from_connection_string(\n",
    "        credential=DefaultAzureCredential(),\n",
    "        conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"],\n",
    "    )\n",
    "\n",
    "AGENT_NAME = \"Critic Agent\"\n",
    "\n",
    "# Adding Tools to be used by Agent \n",
    "functions = FunctionTool(user_functions)\n",
    "\n",
    "toolset = ToolSet()\n",
    "toolset.add(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "846cbbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://ghyadav-critic-resource.services.ai.azure.com/api/projects/ghyadav-critic'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"PROJECT_ENDPOINT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94589105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a250ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Critic Evaluation Agent, ID: asst_03O5D6Q9XSdfvDa8OiYbmxOo\n"
     ]
    }
   ],
   "source": [
    "# Create a Critic Evaluation Agent\n",
    "CRITIC_AGENT_NAME = \"Critic Evaluation Assistant Test\"\n",
    "\n",
    "critic_agent = project_client.agents.create_agent(\n",
    "    model=os.environ[\"MODEL_DEPLOYMENT_NAME\"],\n",
    "    name=CRITIC_AGENT_NAME,\n",
    "    instructions=\"\"\"You are a Critic Evaluation Assistant specialized in evaluating AI agents. \n",
    "    You have access to the following main evaluation functions:\n",
    "    \n",
    "    1. 'evaluate' - For targeted evaluation with specific parameters:\n",
    "       - Can evaluate by agent_id (for comprehensive agent evaluation) \n",
    "       - Can evaluate by thread_id (for single conversation evaluation)\n",
    "       - Allows specifying which evaluators to use\n",
    "       \n",
    "    2. 'auto_evaluate' - For automatic evaluation using all available evaluators:\n",
    "       - Automatically runs all default evaluators (IntentResolution, ToolCallAccuracy, TaskAdherence)\n",
    "       - Can evaluate by agent_id or thread_id\n",
    "       - Best for quick, comprehensive assessments\n",
    "   3. 'get_user_info' - For retrieving user information:\n",
    "         - Can retrieve user information by user_id\n",
    "         - Returns user details like name, email, and preferences\n",
    "\n",
    "    When asked to evaluate an agent or conversation, determine which function is most appropriate \n",
    "    and gather the required parameters (Azure AI project endpoint, model configuration, etc.).\n",
    "    Always provide clear, structured results and explain the evaluation findings.\"\"\",\n",
    "    tools=functions.definitions,\n",
    ")\n",
    "\n",
    "print(f\"Created Critic Evaluation Agent, ID: {critic_agent.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6934d859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created evaluation request message, ID: msg_NfVJ2FYwY1PoZmTSN5oF76oQ\n",
      "Evaluation request submitted to critic agent...\n"
     ]
    }
   ],
   "source": [
    "# Ask the critic agent to auto-evaluate our conversation thread\n",
    "thread_id = \"thread_YQpM4sgUQte7ZSwTQlBYiMH8\"\n",
    "evaluation_request = f\"\"\"Please auto-evaluate the conversation thread with ID '{thread_id}' using the following configuration:\n",
    "\n",
    "Azure AI Project: {{\n",
    "    \"azure_endpoint\": \"{os.environ.get('PROJECT_ENDPOINT', 'YOUR_ENDPOINT')}\"\n",
    "}}\n",
    "\n",
    "Model Configuration: {{\n",
    "    \"azure_endpoint\": \"{os.environ['AZURE_OPENAI_ENDPOINT']}\",\n",
    "    \"api_key\": \"{os.environ['AZURE_OPENAI_API_KEY']}\",\n",
    "    \"api_version\": \"{os.environ['AZURE_OPENAI_API_VERSION']}\",\n",
    "    \"azure_deployment\": \"{os.environ['MODEL_DEPLOYMENT_NAME']}\"\n",
    "}}\n",
    "\n",
    "This should automatically run all appropriate evaluators for this conversation.\"\"\"\n",
    "critic_thread = project_client.agents.threads.create()\n",
    "# Create a new message for evaluation\n",
    "if updated_agents:\n",
    "    eval_message = project_client.agents.messages.create(\n",
    "        thread_id=critic_thread.id,\n",
    "        role=\"user\",\n",
    "        content=evaluation_request,\n",
    "    )        \n",
    "else:\n",
    "    eval_message = project_client.agents.create_message(\n",
    "        thread_id=critic_thread.id,\n",
    "        role=\"user\",\n",
    "        content=evaluation_request,\n",
    "    )\n",
    "    \n",
    "print(f\"Created evaluation request message, ID: {eval_message.id}\")\n",
    "print(\"Evaluation request submitted to critic agent...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746b121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the critic agent\n",
    "from azure.ai.agents.models import (\n",
    "    RequiredFunctionToolCall,\n",
    "    SubmitToolOutputsAction,\n",
    "    ToolOutput,\n",
    ")\n",
    "# Cancel any previous runs if they exist\n",
    "# try:\n",
    "#     existing_runs = project_client.agents.runs.list(thread_id=critic_thread.id)\n",
    "#     for run in existing_runs:\n",
    "#         if run.status not in [\"completed\", \"failed\"]:\n",
    "#             print(f\"Cancelling existing run: {run.id} with status {run.status}\")\n",
    "#             project_client.agents.runs.cancel(thread_id=critic_thread.id, run_id=run.id)\n",
    "# except Exception as e:\n",
    "#     print(f\"Error checking existing runs: {e}\")\n",
    "critic_run = project_client.agents.runs.create(thread_id=critic_thread.id, agent_id=critic_agent.id)\n",
    "\n",
    "while critic_run.status in [\"queued\", \"in_progress\", \"requires_action\"]:\n",
    "    time.sleep(1)\n",
    "    critic_run = project_client.agents.runs.get(thread_id=critic_thread.id, run_id=critic_run.id)\n",
    "\n",
    "    if critic_run.status == \"requires_action\" and isinstance(critic_run.required_action, SubmitToolOutputsAction):\n",
    "        tool_calls = critic_run.required_action.submit_tool_outputs.tool_calls\n",
    "        if not tool_calls:\n",
    "            print(\"No tool calls provided - cancelling run\")\n",
    "            project_client.agents.runs.cancel(thread_id=critic_thread.id, run_id=critic_run.id)\n",
    "            break\n",
    "\n",
    "        tool_outputs = []\n",
    "        for tool_call in tool_calls:\n",
    "            if isinstance(tool_call, RequiredFunctionToolCall):\n",
    "                try:\n",
    "                    print(f\"Executing critic tool call: {tool_call}\")\n",
    "                    output = functions.execute(tool_call)\n",
    "                    tool_outputs.append(\n",
    "                        ToolOutput(\n",
    "                            tool_call_id=tool_call.id,\n",
    "                            output=output,\n",
    "                        )\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error executing tool_call {tool_call.id}: {e}\")\n",
    "\n",
    "        print(f\"Critic tool outputs: {tool_outputs}\")\n",
    "        if tool_outputs:\n",
    "            project_client.agents.runs.submit_tool_outputs(thread_id=critic_thread.id, run_id=critic_run.id, tool_outputs=tool_outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adb6dde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread_cEQbLICUpA71yR9iTzPAmadH\n"
     ]
    }
   ],
   "source": [
    "print(critic_thread.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0293cc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Critic Agent Conversation ===\")\n",
    "for message in project_client.agents.messages.list(critic_thread.id, order=\"asc\"):\n",
    "    print(f\"Role: {message.role}\")\n",
    "    print(f\"Content: {message.content[0].text.value}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d941415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aoai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
