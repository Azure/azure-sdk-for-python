# ---------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# ---------------------------------------------------------

import logging
from math import sin

from openai import AzureOpenAI, OpenAI
import openai
import pandas as pd
from typing import Any, Callable, Dict, List, Optional, Set, Tuple, TypedDict, TypeVar, Union, Type
from time import sleep

from ._batch_run import CodeClient, ProxyClient

#import aoai_mapping
from azure.ai.evaluation._exceptions import ErrorBlame, ErrorCategory, ErrorTarget, EvaluationException
from azure.ai.evaluation._aoai.aoai_grader import AoaiGrader
from azure.ai.evaluation._constants import AOAI_ID_MAPPING

TClient = TypeVar("TClient", ProxyClient, CodeClient)
LOGGER = logging.getLogger(__name__)



def _begin_aoai_evaluation(client: Union[OpenAI, AzureOpenAI], graders: Dict[str, AoaiGrader], data: pd.DataFrame, run_name: str) -> Tuple[str, str, Dict[str, str]]:
    """
    Use the AOAI SDK to start an evaluation of the inputted dataset against the supplied graders.
    AOAI evaluation runs must be queried for completion, so this returns a poller to accomplish that task
    at a later time.

    :param client: The AOAI client to use for the evaluation.
    :type client: Union[OpenAI, AzureOpenAI]
    :param graders: The graders to use for the evaluation. Should be a dictionary of string to AOAIGrader.
    :type graders: Dict[str, AoaiGrader]
    :param data: The data to evaluate, preprocessed by the `_validate_and_load_data` method.
    :type data: pd.DataFrame
    :param run_name: The name of the evaluation run.
    :type run_name: str
    :return: A tuple containing the eval group ID and eval run ID of the resultant eval run, as well as a dictionary
        that maps the user-supplied evaluators to the names of the graders as generated by the OAI service.
    :rtype: Tuple[str, str, Dict[str, str]]
    """

    # Format data for eval group creation
    grader_name_list  = []
    grader_list = []
    for name, grader in graders.items():
        grader_name_list.append(name)
        grader_list.append(grader.get_grader_config())
    data_source_config = _generate_default_data_source_config(data)

    # Create eval group
    eval_group_info = client.evals.create(
        data_source_config=data_source_config,
        testing_criteria=grader_list,
        metadata={"is_foundry_eval": "true"}
    )
    # Use eval group info to map grader IDs back to user-assigned names.
    eval_name_map = {}
    num_criteria = len(eval_group_info.testing_criteria)
    if num_criteria != len(grader_name_list):
        raise EvaluationException(
            message=f"Number of testing criteria ({num_criteria})" +
                f" returned by OAI eval group does not match oai graders({len(grader_name_list)}).",
            blame=ErrorBlame.USER_ERROR,
            category=ErrorCategory.INVALID_VALUE,
            target=ErrorTarget.AOAI_GRADER,
        )
    for name, criteria in zip(grader_name_list, eval_group_info.testing_criteria):
        eval_name_map[criteria.id] = name

    # Create eval run 
    eval_run_id = _begin_eval_run(client, eval_group_info.id, run_name, data)

    return eval_group_info.id, eval_run_id, eval_name_map

def _get_evaluation_run_results(client: Union[OpenAI, AzureOpenAI], eval_group_id: str, eval_run_id: str, grader_name_map: Dict[str, str]) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """
    Get the results of an OAI evaluation run, formatted in a way that is easy for the rest of the evaluation
    pipeline to consume.
    
    :param client: The AOAI client to use for the evaluation.
    :type client: Union[OpenAI, AzureOpenAI]
    :param eval_group_id: The ID of the evaluation group that contains the evaluation run of interest.
    :type eval_group_id: str
    :param eval_run_id: The evaluation run ID to get the results of.
    :type eval_run_id: str
    :param grader_name_map: A dictionary that maps the names of graders as generated by the OAI service during
        eval group creation to the user-supplied name of each grader as entered in the inputted `evaluators`
        dictionary.
    :type grader_name_map: Dict[str, str]
    :return: A tuple containing the results of the evaluation run as a dataframe, and a dictionary of metrics
        calculated from the evaluation run.
    :rtype: Tuple[pd.DataFrame, Dict[str, Any]]
    :raises EvaluationException: If the evaluation run fails or is not completed before timing out.
    """

    # Wait for evaluation run to complete
    run_results = _get_run_results(client, eval_group_id, eval_run_id)
    if run_results.status != "completed":
        raise EvaluationException(
            message=f"AOAI evaluation run {eval_group_id}/{eval_run_id} failed with status {run_results.status}.",
            blame=ErrorBlame.UNKNOWN,
            category=ErrorCategory.FAILED_EXECUTION,
            target=ErrorTarget.AOAI_GRADER,
        )
    # Convert run results into a dictionary of metrics
    run_metrics = {}
    for criteria_result in run_results.per_testing_criteria_results:
        grader_name = grader_name_map[criteria_result.testing_criteria]
        passed = criteria_result.passed
        failed = criteria_result.failed
        ratio = passed / (passed + failed)
        formatted_column_name = f"{grader_name}.pass_rate"
        run_metrics[formatted_column_name] = ratio

    
    # Get full results and convert them into a dataframe.
    # Notes on raw full data output from OAI eval runs:
    # Each row in the full results list in itself a list.
    # Each entry corresponds to one grader's results from the criteria list that was inputted to the eval group.
    # Each entry is a dictionary, with a name, sample, passed boolean, and score number.
    # The name is used to figure out which grader the entry refers to, the sample is ignored.
    # The passed and score values are then added to the results dictionary, prepended with the grader's name
    # as entered by the user in the inputted dictionary.
    # Other values, if they exist, are also added to the results dictionary.
    raw_list_results = client.evals.runs.output_items.list(eval_id=eval_group_id, run_id=eval_run_id)
    listed_results = {}
    for row_result in raw_list_results.data:
        for single_grader_row_result in row_result.results:
            grader_name = grader_name_map[single_grader_row_result["name"]]
            for name, value in single_grader_row_result.items():
                if name in ["name", "sample"]:
                    continue
                formatted_column_name = f"outputs.{grader_name}.{name}"
                if (formatted_column_name not in listed_results):
                    listed_results[formatted_column_name] = []
                listed_results[f"outputs.{grader_name}.{name}"].append(value)
    output_df = pd.DataFrame(listed_results)

    return output_df, run_metrics

def _split_evaluators_and_grader_configs(evaluators: Dict[str, Union[Callable, AoaiGrader]]) -> Tuple[Dict[str, Callable], Dict[str, AoaiGrader]]:
    """
    Given a dictionary of strings to Evaluators and AOAI grader configs. Identity which is which, and return two
    dictionaries that each contain one subset, the first containing the evaluators and the second containing
    the AOAI grader configs.

    :param evaluators: Evaluators to be used for evaluation. It should be a dictionary with key as alias for evaluator
        and value as the evaluator function or AOAI grader. 
    :type evaluators: Dict[str, Union[Callable, ]]
    :return: Tuple of two dictionaries, the first containing evaluators and the second containing AOAI grader configs.
    :rtype: Tuple[Dict[str, Callable], Dict[str, AoaiGrader]]
    """
    true_evaluators = {}
    aoai_graders = {}
    for key, value in evaluators.items():
        if isinstance(value, AoaiGrader):
            aoai_graders[key] = value
        else:
            true_evaluators[key] = value
    return true_evaluators, aoai_graders

# TODO - if we decide on a per-grader-config class implementation, this function will need
# the following updates:
# - Add back the model_id input, use that to call _get_grader_class
# - further check that grader_config aligns with grader implied by model_id
def _convert_remote_eval_params_to_grader(grader_id: str, init_params: Dict[str, Any]) -> AoaiGrader:
    """
    Given a model ID that refers to a specific AOAI grader wrapper class, return an instance of that class
    using this initialization parameters provided.

    :param grader_id: The model ID that refers to a specific AOAI grader wrapper class.
    :type grader_id: str
    :param init_params: The initialization parameters to be used for the AOAI grader wrapper class.
        Requires that it contain a model_config and grader_config as top-level keys.
    :type init_params: Dict[str, Any]
    """

    model_config = init_params.get("model_config", None)
    if model_config is None:
        raise EvaluationException(
            message="Grader converter needs a valid 'model_config' key in init_params.",
            blame=ErrorBlame.USER_ERROR,
            category=ErrorCategory.INVALID_VALUE,
            target=ErrorTarget.AOAI_GRADER,
        )

    grader_config = init_params.get("grader_config", None)
    if grader_config is None:
        raise EvaluationException(
            message="Grader converter needs a valid 'grader_config' key in init_params.",
            blame=ErrorBlame.USER_ERROR,
            category=ErrorCategory.INVALID_VALUE,
            target=ErrorTarget.AOAI_GRADER,
        )

    grader_class =  AoaiGrader #_get_grader_class(model_id)
    return grader_class(model_config=model_config, grader_config=grader_config)

# NOTE: Currently unused in due to single grader class implementation
# Delete if we don't switch to a class-per-grader-config implementation
def _get_grader_class(model_id: str) -> Type[AoaiGrader]:
    """
    Given a model ID, return the class of the corresponding grader wrapper.
    """
    stripped_id = ""# TODO extract minimal string to ID things that ID graders
    if (stripped_id not in AOAI_ID_MAPPING):
        # TODO better error using package exception class
        raise ValueError(f"Model ID {model_id} not recognized as an AOAI grader")
    return AOAI_ID_MAPPING[stripped_id]


def _generate_default_data_source_config(input_data_df: pd.DataFrame) -> Dict[str, Any]:
    """Produce a data source config that naively maps all columns from the supplied data source into
    the OAI API.
    
    :param input_data_df: The input data to be evaluated, as produced by the `_validate_and_load_data`
    helper function.
    :type input_data_df: pd.DataFrame
    :return: A dictionary that can act as data source config for OAI evaluation group creation.
    :rtype: Dict[str, Any]
    """

    properties = {}
    required = []

    for column in input_data_df.columns:
        properties[column] = {
            "type": "string",
        }
        required.append(column)
    data_source_config = {
        "type": "custom",
        "item_schema": {
            "type": "object",
            "properties": properties,
            "required": required,
        }
    }
    return data_source_config

def _begin_eval_run(client: Union[OpenAI, AzureOpenAI], eval_group_id: str, run_name: str, input_data_df: pd.DataFrame) -> str:
    """
    Given an eval group id and a dataset file path, use the AOAI API to 
    start an evaluation run with the given name and description.
    Returns a poller that can be used to monitor the run. 

    :param client: The AOAI client to use for the evaluation.
    :type client: Union[OpenAI, AzureOpenAI]
    :param eval_group_id: The ID of the evaluation group to use for the evaluation run.
    :type eval_group_id: str
    :param run_name: The name of the evaluation run.
    :type run_name: str
    :param input_data_df: The input data to be evaluated, as produced by the `_validate_and_load_data`
        helper function.
    :type input_data_df: pd.DataFrame
    :return: The ID of the evaluation run.
    :rtype: str
    """

    # TODO use SDK when available to start an eval run.
    # eval_run_id = AOAI_SDK.start_eval_run(eval_group_id, run_name, run_description, eval_data)

    # TODO if needed, convert SDK return value into a poller, although hopefully the SDK is good enough to do that on its own

    # Convert input dataframe into our best guess of OpenAI's mystery input format.
    content = []
    for row in input_data_df.iterrows():
        row_dict = {}
        for key,value in row[1].items():
            # Convert to string to OAI doesn't whine about types.
            row_dict[key] = str(value)
        content.append({"item": row_dict})

    data_source = {
        "type": "jsonl",
        "source": {
            "type": "file_content",
            "content": content,
        }
    }

    eval_run = client.evals.runs.create(
        eval_id=eval_group_id,
        data_source=data_source,
        name=run_name,
        metadata={"sample_generation": "off","file_format": "jsonl", "is_foundry_eval": "true"}
        # TODO decide if we want to add our own timeout value?
    )
    return eval_run.id

def _get_run_results(client: Union[OpenAI, AzureOpenAI], eval_group_id: str, eval_run_id: str) -> Any:
    """
    Perform exponential backoff polling to get the results of an AOAI evaluation run.
    Raises an EvaluationException if max attempts are reached without receiving a concluding status.

    :param client: The AOAI client to use for the evaluation.
    :type client: Union[OpenAI, AzureOpenAI]
    :param eval_group_id: The ID of the evaluation group that contains the evaluation run of interest.
    :type eval_group_id: str
    :param eval_run_id: The evaluation run ID to get the results of.
    :type eval_run_id: str
    """


    iters = 0
    max_iters = 10 # TODO assign as a constant somewhere? Make configurable?
    wait_interval = 10 # Seconds. TODO assign as a constant somewhere? Make configurable?
    # Max wait is 
    while(iters < max_iters):
        iters += 1
        wait_interval *= 2
        sleep(wait_interval)
        response = client.evals.runs.retrieve(eval_id=eval_group_id, run_id=eval_run_id)
        if response.status not in  ["queued", "running"]:
            return response
        if iters > max_iters:
            raise EvaluationException(
                message=f"Timed out waiting for AOAI evaluation to complete after {max_iters} rounds of polling. Final status was {response.status}",
                blame=ErrorBlame.USER_ERROR,
                category=ErrorCategory.FAILED_EXECUTION,
                target=ErrorTarget.AOAI_GRADER,
            )
        
    