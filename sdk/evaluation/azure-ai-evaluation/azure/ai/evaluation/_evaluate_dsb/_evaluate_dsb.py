# ---------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# ---------------------------------------------------------

from enum import Enum
import os
import inspect
from typing import Callable, Dict, List, Optional, Union
from azure.ai.evaluation._evaluators import _content_safety, _protected_material,  _groundedness
from azure.ai.evaluation._evaluate import _evaluate
from azure.ai.evaluation._exceptions import ErrorBlame, ErrorCategory, ErrorTarget, EvaluationException
from azure.ai.evaluation._model_configurations import AzureAIProject, EvaluationResult
from azure.ai.evaluation.simulator import Simulator, AdversarialSimulator, AdversarialScenario
from azure.ai.evaluation.simulator._utils import JsonLineList
from azure.ai.evaluation._model_configurations import AzureOpenAIModelConfiguration
from azure.core.credentials import TokenCredential
import json
from pathlib import Path

class DSBEvaluator(Enum):
    """Evaluator types

    """

    CONTENT_SAFETY = "content_safety"
    GROUNDEDNESS = "groundedness"
    PROTECTED_MATERIAL = "protected_material"

async def _simulate_dsb(
        azure_ai_project: AzureAIProject,
        target: Callable, 
        credential: TokenCredential,
        max_conversation_turns: int = 1,
        max_simulation_results: int = 3, 
        adversarial_scenario: Optional[AdversarialScenario] = None,
        model_config: Optional[dict] = None,
        source_text: Optional[str] = None,
) -> str:
    ## Define callback
    async def callback(
        messages: List[Dict],
        stream: bool = False,
        session_state: Optional[str] = None,
        context: Optional[Dict] = None
    ) -> dict:
        messages_list = messages["messages"] # type: ignore
        latest_message = messages_list[-1]
        application_input = latest_message["content"]
        context = latest_message.get("context", None)
        latest_context = None
        try:
            if check_target_returns_context(target):
                response, latest_context = target(query=application_input)
            else:
                response = target(query=application_input)
        except Exception as e:
            response = f"Something went wrong {e!s}"

        ## We format the response to follow the openAI chat protocol format
        formatted_response = {
            "content": response,
            "role": "assistant",
            "context": latest_context if latest_context else context,
        }
        ## NOTE: In the future, instead of appending to messages we should just return `formatted_response`
        messages["messages"].append(formatted_response) # type: ignore
        return {"messages": messages_list, "stream": stream, "session_state": session_state, "context": latest_context if latest_context else context}
    
    ## Run simulator
    simulator_outputs = None
    data_path = "simulator_outputs.jsonl"

    ## If adversarial_scenario is not provided, run Simulator
    if adversarial_scenario is None and model_config:
        simulator = Simulator(model_config=AzureOpenAIModelConfiguration(**model_config))
        simulator_outputs = await simulator(
            max_conversation_turns=max_conversation_turns,
            max_simulation_results=max_simulation_results,
            target=callback,
            text=source_text if source_text else "",
        )

    ## Ryun AdversarialSimulator
    elif adversarial_scenario:
        simulator = AdversarialSimulator(azure_ai_project=azure_ai_project, credential=credential)
        simulator_outputs = await simulator(
            scenario=adversarial_scenario,
            max_conversation_turns=max_conversation_turns,
            max_simulation_results=max_simulation_results,
            target=callback,
            text=source_text,
        )

    ## If no outputs are generated, raise an exception
    if not simulator_outputs:
        msg = "No outputs generated by the simulator"
        raise EvaluationException(
            message=msg,
            internal_message=msg,
            target=ErrorTarget.ADVERSARIAL_SIMULATOR,
            category=ErrorCategory.UNKNOWN,
            blame=ErrorBlame.USER_ERROR,
        )

    ## Write outputs to file according to scenario
    with Path(data_path).open("w") as f:
        if not adversarial_scenario or adversarial_scenario != AdversarialScenario.ADVERSARIAL_CONVERSATION:
            if source_text or check_target_returns_context(target):
                eval_input_data_json_lines = ""
                for output in simulator_outputs:
                    query = None
                    response = None
                    context = source_text
                    ground_truth = source_text
                    for message in output["messages"]:
                        if message["role"] == "user":
                            query = message["content"]
                        if message["role"] == "assistant":
                            response = message["content"]
                    if query and response:
                        eval_input_data_json_lines += (
                            json.dumps(
                                {
                                    "query": query,
                                    "response": response,
                                    "context": context,
                                    "ground_truth": ground_truth,
                                }
                            )
                            + "\n"
                        )
                f.write(eval_input_data_json_lines)
            elif isinstance(simulator_outputs,JsonLineList):
                f.writelines(simulator_outputs.to_eval_qr_json_lines())
            else:
                f.writelines(output.to_eval_qr_json_lines() for output in simulator_outputs)
        else:
            f.writelines(
                [json.dumps({"conversation": {"messages": conversation["messages"]}}) + "\n" for conversation in simulator_outputs]
            )
    return data_path

def get_evaluators(
        evaluators: List[DSBEvaluator], 
        azure_ai_project: AzureAIProject,
        credential: TokenCredential, 
        model_config: Optional[dict] = None
) -> Dict[str, Callable]:
    evaluators_dict = {}
    for evaluator in evaluators:
        if evaluator == DSBEvaluator.CONTENT_SAFETY:
            evaluators_dict["content_safety"] = _content_safety.ContentSafetyEvaluator(
                azure_ai_project=azure_ai_project, credential=credential
            )
        elif evaluator ==  DSBEvaluator.GROUNDEDNESS:
            evaluators_dict["groundedness"] = _groundedness.GroundednessEvaluator(
                model_config=model_config,
            )
        elif evaluator == DSBEvaluator.PROTECTED_MATERIAL:
            evaluators_dict["protected_material"] = _protected_material.ProtectedMaterialEvaluator(
                azure_ai_project=azure_ai_project, credential=credential
            )
        else:
            msg = f"Invalid evaluator: {evaluator}. Supported evaluators are: {DSBEvaluator.__members__.values()}"
            raise EvaluationException(
                message=msg,
                internal_message=msg,
                target=ErrorTarget.UNKNOWN, ## NOTE: We should add a target for this potentially
                category=ErrorCategory.INVALID_VALUE,
                blame=ErrorBlame.USER_ERROR,
            )
    return evaluators_dict

def check_target_returns_context(target: Callable) -> bool:
    sig = inspect.signature(target)
    ret_type = sig.return_annotation
    if ret_type == inspect.Signature.empty:
        return False
    if ret_type is tuple: 
        return True
    return False

def validate_args(
        evaluators: List[DSBEvaluator],
        target: Callable,
        source_text: Optional[str] = None,
        data_path: Optional[Union[str, os.PathLike]] = None,
        model_config: Optional[dict] = None,
):
    if DSBEvaluator.GROUNDEDNESS in evaluators and not model_config:
        msg = "`model_config`parameter is required for GroundednessEvaluator"
        raise EvaluationException(
            message=msg,
            internal_message=msg,
            target=ErrorTarget.GROUNDEDNESS_EVALUATOR,
            category=ErrorCategory.MISSING_FIELD,
            blame=ErrorBlame.USER_ERROR,
        )

    if DSBEvaluator.GROUNDEDNESS in evaluators and not (check_target_returns_context(target) or source_text):
        msg = "GroundednessEvaluator requires either source_text or a target function that returns context"
        raise EvaluationException(
            message=msg,
            internal_message=msg,
            target=ErrorTarget.GROUNDEDNESS_EVALUATOR,
            category=ErrorCategory.MISSING_FIELD,
            blame=ErrorBlame.USER_ERROR,
        )
    
    if not AdversarialScenario and not model_config and not data_path:
        msg = "model_config or AdversarialScenario is required for Simulation"
        raise EvaluationException(
            message=msg,
            internal_message=msg,
            target=ErrorTarget.ADVERSARIAL_SIMULATOR,
            category=ErrorCategory.MISSING_FIELD,
            blame=ErrorBlame.USER_ERROR,
        )

async def evaluate_dsb(
        azure_ai_project: AzureAIProject,
        credential: TokenCredential,
        evaluators: List[DSBEvaluator],
        target: Callable,
        adversarial_scenario: Optional[AdversarialScenario] = None,
        max_conversation_turns: int = 1,
        max_simulation_results: int = 3,
        source_text: Optional[str] = None,
        data_path: Optional[Union[str, os.PathLike]] = None,
        model_config: Optional[dict] = None,
        output_path: Optional[Union[str, os.PathLike]] = None,
) -> EvaluationResult:
    
    ## Validate arguments
    validate_args(
        evaluators=evaluators, 
        target=target, 
        source_text=source_text,
        data_path=data_path, 
        model_config=model_config, 
    )
    
    ## If `data_path` is not provided, run simulator
    if data_path is None:
        data_path = await _simulate_dsb(
            azure_ai_project=azure_ai_project,
            target=target,
            adversarial_scenario=adversarial_scenario,
            credential=credential,
            max_conversation_turns=max_conversation_turns,
            max_simulation_results=max_simulation_results,
            source_text=source_text,
        )

    ## Get evaluators
    evaluators_dict = get_evaluators(evaluators, azure_ai_project, credential, model_config)

    ## Run evaluation
    evaluate_outputs = _evaluate.evaluate(
        data=data_path,
        evaluators=evaluators_dict,
        azure_ai_project=azure_ai_project,
        output_path=output_path,
    )
    return evaluate_outputs