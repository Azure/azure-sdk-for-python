# coding=utf-8
# --------------------------------------------------------------------------
# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.2.1, generator: {generator})
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------
import functools
from typing import Any, Callable, Dict, Generic, Optional, TypeVar, Union
import warnings

from azure.core.exceptions import (
    ClientAuthenticationError,
    HttpResponseError,
    ResourceExistsError,
    ResourceNotFoundError,
    map_error,
)
from azure.core.pipeline import PipelineResponse
from azure.core.pipeline.transport import AsyncHttpResponse
from azure.core.polling import AsyncLROPoller, AsyncNoPolling, AsyncPollingMethod
from azure.core.polling.async_base_polling import AsyncLROBasePolling
from microsoft_cognitive_language_service.core.rest import HttpRequest

from ... import models as _models
from ...rest import question_answering_projects as rest_question_answering_projects

T = TypeVar("T")
ClsType = Optional[Callable[[PipelineResponse[HttpRequest, AsyncHttpResponse], T, Dict[str, Any]], Any]]


class QuestionAnsweringProjectsOperations:
    """QuestionAnsweringProjectsOperations async operations.

    You should not instantiate this class directly. Instead, you should create a Client instance that
    instantiates it for you and attaches it as an attribute.

    :ivar models: Alias to model classes used in this operation group.
    :type models: ~microsoft_cognitive_language_service.models
    :param client: Client for service requests.
    :param config: Configuration of service client.
    :param serializer: An object model serializer.
    :param deserializer: An object model deserializer.
    """

    models = _models

    def __init__(self, client, config, serializer, deserializer) -> None:
        self._client = client
        self._serialize = serializer
        self._deserialize = deserializer
        self._config = config

    async def list_projects(self, **kwargs: Any) -> "_models.ProjectsMetadata":
        """Gets all projects for a user.

        Gets all projects for a user.

        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: ProjectsMetadata, or the result of cls(response)
        :rtype: ~microsoft_cognitive_language_service.models.ProjectsMetadata
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop("cls", None)  # type: ClsType["_models.ProjectsMetadata"]
        error_map = {401: ClientAuthenticationError, 404: ResourceNotFoundError, 409: ResourceExistsError}
        error_map.update(kwargs.pop("error_map", {}))

        request = rest_question_answering_projects.build_list_projects_request(
            template_url=self.list_projects.metadata["url"], **kwargs
        )._internal_request
        path_format_arguments = {
            "Endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, response)
            raise HttpResponseError(response=response, model=error)

        deserialized = self._deserialize("ProjectsMetadata", pipeline_response)

        if cls:
            return cls(pipeline_response, deserialized, {})

        return deserialized

    list_projects.metadata = {"url": "/query-knowledgebases/projects"}  # type: ignore

    async def get_project_details(self, project_name: str, **kwargs: Any) -> "_models.ProjectMetadata":
        """Get the requested project metadata.

        Get the requested project metadata.

        :param project_name: The name of the project to use.
        :type project_name: str
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: ProjectMetadata, or the result of cls(response)
        :rtype: ~microsoft_cognitive_language_service.models.ProjectMetadata
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop("cls", None)  # type: ClsType["_models.ProjectMetadata"]
        error_map = {401: ClientAuthenticationError, 404: ResourceNotFoundError, 409: ResourceExistsError}
        error_map.update(kwargs.pop("error_map", {}))

        request = rest_question_answering_projects.build_get_project_details_request(
            project_name=project_name, template_url=self.get_project_details.metadata["url"], **kwargs
        )._internal_request
        path_format_arguments = {
            "Endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, response)
            raise HttpResponseError(response=response, model=error)

        deserialized = self._deserialize("ProjectMetadata", pipeline_response)

        if cls:
            return cls(pipeline_response, deserialized, {})

        return deserialized

    get_project_details.metadata = {"url": "/query-knowledgebases/projects/{projectName}"}  # type: ignore

    async def create_project(
        self, project_name: str, body: "_models.CreateProjectParameters", **kwargs: Any
    ) -> "_models.ProjectMetadata":
        """Create or update a project.

        Create or update a project.

        :param project_name: The name of the project to use.
        :type project_name: str
        :param body: Parameters needed to create the project.
        :type body: ~microsoft_cognitive_language_service.models.CreateProjectParameters
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: ProjectMetadata, or the result of cls(response)
        :rtype: ~microsoft_cognitive_language_service.models.ProjectMetadata
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop("cls", None)  # type: ClsType["_models.ProjectMetadata"]
        error_map = {401: ClientAuthenticationError, 404: ResourceNotFoundError, 409: ResourceExistsError}
        error_map.update(kwargs.pop("error_map", {}))

        content_type = kwargs.pop("content_type", "application/json")  # type: Optional[str]

        json = self._serialize.body(body, "object")

        request = rest_question_answering_projects.build_create_project_request(
            project_name=project_name,
            json=json,
            content_type=content_type,
            template_url=self.create_project.metadata["url"],
            **kwargs
        )._internal_request
        path_format_arguments = {
            "Endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200, 201]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, response)
            raise HttpResponseError(response=response, model=error)

        if response.status_code == 200:
            deserialized = self._deserialize("ProjectMetadata", pipeline_response)

        if response.status_code == 201:
            deserialized = self._deserialize("ProjectMetadata", pipeline_response)

        if cls:
            return cls(pipeline_response, deserialized, {})

        return deserialized

    create_project.metadata = {"url": "/query-knowledgebases/projects/{projectName}"}  # type: ignore

    async def delete_project(self, project_name: str, **kwargs: Any) -> None:
        """Delete the project.

        Delete the project.

        :param project_name: The name of the project to use.
        :type project_name: str
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: None, or the result of cls(response)
        :rtype: None
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop("cls", None)  # type: ClsType[None]
        error_map = {401: ClientAuthenticationError, 404: ResourceNotFoundError, 409: ResourceExistsError}
        error_map.update(kwargs.pop("error_map", {}))

        request = rest_question_answering_projects.build_delete_project_request(
            project_name=project_name, template_url=self.delete_project.metadata["url"], **kwargs
        )._internal_request
        path_format_arguments = {
            "Endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [204]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, response)
            raise HttpResponseError(response=response, model=error)

        if cls:
            return cls(pipeline_response, None, {})

    delete_project.metadata = {"url": "/query-knowledgebases/projects/{projectName}"}  # type: ignore

    async def _export_initial(
        self,
        project_name: str,
        body: Optional["_models.ExportJobParameters"] = None,
        *,
        format: Optional[Union[str, "_models.Format"]] = "json",
        **kwargs: Any
    ) -> None:
        cls = kwargs.pop("cls", None)  # type: ClsType[None]
        error_map = {401: ClientAuthenticationError, 404: ResourceNotFoundError, 409: ResourceExistsError}
        error_map.update(kwargs.pop("error_map", {}))

        content_type = kwargs.pop("content_type", "application/json")  # type: Optional[str]

        if body is not None:
            json = self._serialize.body(body, "object")
        else:
            body = None

        request = rest_question_answering_projects.build_export_request_initial(
            project_name=project_name,
            format=format,
            json=json,
            content_type=content_type,
            template_url=self._export_initial.metadata["url"],
            **kwargs
        )._internal_request
        path_format_arguments = {
            "Endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [202]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        response_headers = {}
        response_headers["Operation-Location"] = self._deserialize("str", response.headers.get("Operation-Location"))

        if cls:
            return cls(pipeline_response, None, response_headers)

    _export_initial.metadata = {"url": "/query-knowledgebases/projects/{projectName}/:export"}  # type: ignore

    async def begin_export(
        self,
        project_name: str,
        body: Optional["_models.ExportJobParameters"] = None,
        *,
        format: Optional[Union[str, "_models.Format"]] = "json",
        **kwargs: Any
    ) -> AsyncLROPoller[None]:
        """Export project metadata and assets.

        Export project metadata and assets.

        :param project_name: The name of the project to use.
        :type project_name: str
        :keyword format: Knowledgebase Import or Export format.
        :paramtype format: str or ~microsoft_cognitive_language_service.models.Format
        :param body: Parameters required for export project job.
        :type body: ~microsoft_cognitive_language_service.models.ExportJobParameters
        :keyword callable cls: A custom type or function that will be passed the direct response
        :keyword str continuation_token: A continuation token to restart a poller from a saved state.
        :keyword polling: By default, your polling method will be AsyncLROBasePolling. Pass in False
         for this operation to not poll, or pass in your own initialized polling object for a personal
         polling strategy.
        :paramtype polling: bool or ~azure.core.polling.AsyncPollingMethod
        :keyword int polling_interval: Default waiting time between two polls for LRO operations if no
         Retry-After header is present.
        :return: An instance of AsyncLROPoller that returns either None or the result of cls(response)
        :rtype: ~azure.core.polling.AsyncLROPoller[None]
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        polling = kwargs.pop("polling", True)  # type: Union[bool, AsyncPollingMethod]
        cls = kwargs.pop("cls", None)  # type: ClsType[None]
        lro_delay = kwargs.pop("polling_interval", self._config.polling_interval)
        cont_token = kwargs.pop("continuation_token", None)  # type: Optional[str]
        if cont_token is None:
            raw_result = await self._export_initial(
                project_name=project_name, format=format, body=body, cls=lambda x, y, z: x, **kwargs
            )

        kwargs.pop("error_map", None)
        kwargs.pop("content_type", None)

        content_type = kwargs.pop("content_type", "application/json")  # type: Optional[str]

        def get_long_running_output(pipeline_response):
            response = pipeline_response.http_response
            if cls:
                return cls(pipeline_response, None, {})

        path_format_arguments = {
            "Endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }

        if polling is True:
            polling_method = AsyncLROBasePolling(lro_delay, path_format_arguments=path_format_arguments, **kwargs)
        elif polling is False:
            polling_method = AsyncNoPolling()
        else:
            polling_method = polling
        if cont_token:
            return AsyncLROPoller.from_continuation_token(
                polling_method=polling_method,
                continuation_token=cont_token,
                client=self._client,
                deserialization_callback=get_long_running_output,
            )
        else:
            return AsyncLROPoller(self._client, raw_result, get_long_running_output, polling_method)

    begin_export.metadata = {"url": "/query-knowledgebases/projects/{projectName}/:export"}  # type: ignore

    async def get_export_status(self, project_name: str, job_id: str, **kwargs: Any) -> "_models.ExportJobState":
        """Gets the status of an Export job, once job completes, returns the project metadata, and assets.

        Gets the status of an Export job, once job completes, returns the project metadata, and assets.

        :param project_name: The name of the project to use.
        :type project_name: str
        :param job_id: Job ID.
        :type job_id: str
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: ExportJobState, or the result of cls(response)
        :rtype: ~microsoft_cognitive_language_service.models.ExportJobState
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop("cls", None)  # type: ClsType["_models.ExportJobState"]
        error_map = {401: ClientAuthenticationError, 404: ResourceNotFoundError, 409: ResourceExistsError}
        error_map.update(kwargs.pop("error_map", {}))

        request = rest_question_answering_projects.build_get_export_status_request(
            project_name=project_name, job_id=job_id, template_url=self.get_export_status.metadata["url"], **kwargs
        )._internal_request
        path_format_arguments = {
            "Endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, response)
            raise HttpResponseError(response=response, model=error)

        deserialized = self._deserialize("ExportJobState", pipeline_response)

        if cls:
            return cls(pipeline_response, deserialized, {})

        return deserialized

    get_export_status.metadata = {"url": "/query-knowledgebases/projects/{projectName}/export/jobs/{jobId}"}  # type: ignore

    async def _import_method_initial(
        self,
        project_name: str,
        body: Optional["_models.ImportJobParameters"] = None,
        *,
        format: Optional[Union[str, "_models.Format"]] = "json",
        **kwargs: Any
    ) -> None:
        cls = kwargs.pop("cls", None)  # type: ClsType[None]
        error_map = {401: ClientAuthenticationError, 404: ResourceNotFoundError, 409: ResourceExistsError}
        error_map.update(kwargs.pop("error_map", {}))

        content_type = kwargs.pop("content_type", "application/json")  # type: Optional[str]

        if body is not None:
            json = self._serialize.body(body, "object")
        else:
            body = None

        request = rest_question_answering_projects.build_import_method_request_initial(
            project_name=project_name,
            format=format,
            json=json,
            content_type=content_type,
            template_url=self._import_method_initial.metadata["url"],
            **kwargs
        )._internal_request
        path_format_arguments = {
            "Endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [202]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        response_headers = {}
        response_headers["Operation-Location"] = self._deserialize("str", response.headers.get("Operation-Location"))

        if cls:
            return cls(pipeline_response, None, response_headers)

    _import_method_initial.metadata = {"url": "/query-knowledgebases/projects/{projectName}/:import"}  # type: ignore

    async def begin_import_method(
        self,
        project_name: str,
        body: Optional["_models.ImportJobParameters"] = None,
        *,
        format: Optional[Union[str, "_models.Format"]] = "json",
        **kwargs: Any
    ) -> AsyncLROPoller[None]:
        """Import project assets.

        Import project assets.

        :param project_name: The name of the project to use.
        :type project_name: str
        :keyword format: Knowledgebase Import or Export format.
        :paramtype format: str or ~microsoft_cognitive_language_service.models.Format
        :param body: Project assets the needs to be imported.
        :type body: ~microsoft_cognitive_language_service.models.ImportJobParameters
        :keyword callable cls: A custom type or function that will be passed the direct response
        :keyword str continuation_token: A continuation token to restart a poller from a saved state.
        :keyword polling: By default, your polling method will be AsyncLROBasePolling. Pass in False
         for this operation to not poll, or pass in your own initialized polling object for a personal
         polling strategy.
        :paramtype polling: bool or ~azure.core.polling.AsyncPollingMethod
        :keyword int polling_interval: Default waiting time between two polls for LRO operations if no
         Retry-After header is present.
        :return: An instance of AsyncLROPoller that returns either None or the result of cls(response)
        :rtype: ~azure.core.polling.AsyncLROPoller[None]
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        polling = kwargs.pop("polling", True)  # type: Union[bool, AsyncPollingMethod]
        cls = kwargs.pop("cls", None)  # type: ClsType[None]
        lro_delay = kwargs.pop("polling_interval", self._config.polling_interval)
        cont_token = kwargs.pop("continuation_token", None)  # type: Optional[str]
        if cont_token is None:
            raw_result = await self._import_method_initial(
                project_name=project_name, format=format, body=body, cls=lambda x, y, z: x, **kwargs
            )

        kwargs.pop("error_map", None)
        kwargs.pop("content_type", None)

        content_type = kwargs.pop("content_type", "application/json")  # type: Optional[str]

        def get_long_running_output(pipeline_response):
            response = pipeline_response.http_response
            if cls:
                return cls(pipeline_response, None, {})

        path_format_arguments = {
            "Endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }

        if polling is True:
            polling_method = AsyncLROBasePolling(lro_delay, path_format_arguments=path_format_arguments, **kwargs)
        elif polling is False:
            polling_method = AsyncNoPolling()
        else:
            polling_method = polling
        if cont_token:
            return AsyncLROPoller.from_continuation_token(
                polling_method=polling_method,
                continuation_token=cont_token,
                client=self._client,
                deserialization_callback=get_long_running_output,
            )
        else:
            return AsyncLROPoller(self._client, raw_result, get_long_running_output, polling_method)

    begin_import_method.metadata = {"url": "/query-knowledgebases/projects/{projectName}/:import"}  # type: ignore

    async def get_import_status(self, project_name: str, job_id: str, **kwargs: Any) -> "_models.JobState":
        """Gets the status of an Import job.

        Gets the status of an Import job.

        :param project_name: The name of the project to use.
        :type project_name: str
        :param job_id: Job ID.
        :type job_id: str
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: JobState, or the result of cls(response)
        :rtype: ~microsoft_cognitive_language_service.models.JobState
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop("cls", None)  # type: ClsType["_models.JobState"]
        error_map = {401: ClientAuthenticationError, 404: ResourceNotFoundError, 409: ResourceExistsError}
        error_map.update(kwargs.pop("error_map", {}))

        request = rest_question_answering_projects.build_get_import_status_request(
            project_name=project_name, job_id=job_id, template_url=self.get_import_status.metadata["url"], **kwargs
        )._internal_request
        path_format_arguments = {
            "Endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, response)
            raise HttpResponseError(response=response, model=error)

        deserialized = self._deserialize("JobState", pipeline_response)

        if cls:
            return cls(pipeline_response, deserialized, {})

        return deserialized

    get_import_status.metadata = {"url": "/query-knowledgebases/projects/{projectName}/import/jobs/{jobId}"}  # type: ignore

    async def _deploy_project_initial(self, project_name: str, deployment_name: str, **kwargs: Any) -> None:
        cls = kwargs.pop("cls", None)  # type: ClsType[None]
        error_map = {401: ClientAuthenticationError, 404: ResourceNotFoundError, 409: ResourceExistsError}
        error_map.update(kwargs.pop("error_map", {}))

        request = rest_question_answering_projects.build_deploy_project_request_initial(
            project_name=project_name,
            deployment_name=deployment_name,
            template_url=self._deploy_project_initial.metadata["url"],
            **kwargs
        )._internal_request
        path_format_arguments = {
            "Endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [202]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        response_headers = {}
        response_headers["Operation-Location"] = self._deserialize("str", response.headers.get("Operation-Location"))

        if cls:
            return cls(pipeline_response, None, response_headers)

    _deploy_project_initial.metadata = {"url": "/query-knowledgebases/projects/{projectName}/deployments/{deploymentName}"}  # type: ignore

    async def begin_deploy_project(
        self, project_name: str, deployment_name: str, **kwargs: Any
    ) -> AsyncLROPoller[None]:
        """Deploy project to production.

        Deploy project to production.

        :param project_name: The name of the project to use.
        :type project_name: str
        :param deployment_name: The name of the specific deployment of the project to use.
        :type deployment_name: str
        :keyword callable cls: A custom type or function that will be passed the direct response
        :keyword str continuation_token: A continuation token to restart a poller from a saved state.
        :keyword polling: By default, your polling method will be AsyncLROBasePolling. Pass in False
         for this operation to not poll, or pass in your own initialized polling object for a personal
         polling strategy.
        :paramtype polling: bool or ~azure.core.polling.AsyncPollingMethod
        :keyword int polling_interval: Default waiting time between two polls for LRO operations if no
         Retry-After header is present.
        :return: An instance of AsyncLROPoller that returns either None or the result of cls(response)
        :rtype: ~azure.core.polling.AsyncLROPoller[None]
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        polling = kwargs.pop("polling", True)  # type: Union[bool, AsyncPollingMethod]
        cls = kwargs.pop("cls", None)  # type: ClsType[None]
        lro_delay = kwargs.pop("polling_interval", self._config.polling_interval)
        cont_token = kwargs.pop("continuation_token", None)  # type: Optional[str]
        if cont_token is None:
            raw_result = await self._deploy_project_initial(
                project_name=project_name, deployment_name=deployment_name, cls=lambda x, y, z: x, **kwargs
            )

        kwargs.pop("error_map", None)
        kwargs.pop("content_type", None)

        def get_long_running_output(pipeline_response):
            response = pipeline_response.http_response
            if cls:
                return cls(pipeline_response, None, {})

        path_format_arguments = {
            "Endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }

        if polling is True:
            polling_method = AsyncLROBasePolling(lro_delay, path_format_arguments=path_format_arguments, **kwargs)
        elif polling is False:
            polling_method = AsyncNoPolling()
        else:
            polling_method = polling
        if cont_token:
            return AsyncLROPoller.from_continuation_token(
                polling_method=polling_method,
                continuation_token=cont_token,
                client=self._client,
                deserialization_callback=get_long_running_output,
            )
        else:
            return AsyncLROPoller(self._client, raw_result, get_long_running_output, polling_method)

    begin_deploy_project.metadata = {"url": "/query-knowledgebases/projects/{projectName}/deployments/{deploymentName}"}  # type: ignore

    async def get_deploy_status(
        self, project_name: str, deployment_name: str, job_id: str, **kwargs: Any
    ) -> "_models.JobState":
        """Gets the status of a Deploy job.

        Gets the status of a Deploy job.

        :param project_name: The name of the project to use.
        :type project_name: str
        :param deployment_name: The name of the specific deployment of the project to use.
        :type deployment_name: str
        :param job_id: Job ID.
        :type job_id: str
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: JobState, or the result of cls(response)
        :rtype: ~microsoft_cognitive_language_service.models.JobState
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop("cls", None)  # type: ClsType["_models.JobState"]
        error_map = {401: ClientAuthenticationError, 404: ResourceNotFoundError, 409: ResourceExistsError}
        error_map.update(kwargs.pop("error_map", {}))

        request = rest_question_answering_projects.build_get_deploy_status_request(
            project_name=project_name,
            deployment_name=deployment_name,
            job_id=job_id,
            template_url=self.get_deploy_status.metadata["url"],
            **kwargs
        )._internal_request
        path_format_arguments = {
            "Endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = self._deserialize.failsafe_deserialize(_models.ErrorResponse, response)
            raise HttpResponseError(response=response, model=error)

        deserialized = self._deserialize("JobState", pipeline_response)

        if cls:
            return cls(pipeline_response, deserialized, {})

        return deserialized

    get_deploy_status.metadata = {"url": "/query-knowledgebases/projects/{projectName}/deployments/{deploymentName}/jobs/{jobId}"}  # type: ignore
